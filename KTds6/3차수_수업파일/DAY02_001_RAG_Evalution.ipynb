{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  RAG 성능 평가 \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정 및 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) Langsmith tracing 설정`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "# Langsmith tracing 여부를 확인 (true: langsmith 추척 활성화, false: langsmith 추척 비활성화)\n",
    "import os\n",
    "print(os.getenv('LANGSMITH_TRACING'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RAG 시스템 성능 평가\n",
    "\n",
    "- **RAG 기술의 핵심**: 외부 지식 검색과 LLM 결합으로 응답 품질 향상\n",
    "\n",
    "- **평가 기준**: LLM-as-judge 방식으로 사실성, 관련성, 충실도, 유용성 평가\n",
    "\n",
    "- **체계적인 A/B 테스트**: 각 컴포넌트별 성능 비교 및 영향도 분석으로 최적 구성 도출\n",
    "\n",
    "- **평가 방법론**: 오프라인(참조답변 기반), 온라인(실시간), 페어와이즈(비교) 평가 구분\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/rag_evaluation.png\" alt=\"rag\" align=\"center\" border=\"0\"  width=\"1000\" height=auto>\n",
    "</center>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **평가 대상(Evaluation Target)**\n",
    "\n",
    "- **검색(Retrieval)** 단계: \n",
    "    1. 관련 문서와 쿼리 간의 연관성(Relevance)을 통해 검색된 문서가 쿼리의 정보 요구를 얼마나 잘 충족하는지 평가\n",
    "    1. 관련 문서와 후보 문서 간의 정확성(Accuracy)을 통해 시스템이 적절한 문서를 식별하는 능력을 측정\n",
    "\n",
    "- **생성(Generation)** 단계:\n",
    "    1. 응답과 쿼리의 연관성(Relavance)\n",
    "    1. 응답과 관련 문서 간의 충실도(Faithfulness)\n",
    "    1. 응답과 샘플 응답 간의 정확성(Correctness)\n",
    "\n",
    "- 추가 고려사항:\n",
    "    - **핵심 성능 지표**: Latency(응답 속도), Diversity(검색 다양성), Noise Robustness(잡음 내구성)\n",
    "    - **안전성 평가**: Negative Rejection(불충분 정보 거부), Counterfactual Robustness(오정보 식별)\n",
    "    - **사용자 경험**: Readability(가독성), Toxicity(유해성), Perplexity(복잡성) 등 추가 고려\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/rag_evaluation_target.png\"  alt=\"rag\" align=\"center\" border=\"0\"  width=\"800\" height=auto>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "[출처] https://arxiv.org/abs/2405.07437"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **평가 데이터셋 구축(Evaluation Dataset)**\n",
    "\n",
    "- **데이터셋 구성 방식**: LLM 기반 새로운 데이터셋 생성 (Synthetic Data)\n",
    "\n",
    "- **맞춤형 데이터셋** 구축으로 RAG 시스템의 실용성 평가 강화\n",
    "\n",
    "\n",
    "- [실습] : **Ragas** (https://docs.ragas.io/en/stable/) 활용\n",
    "\n",
    "    - **RAG 시스템 평가**를 위한 오픈소스 프레임워크\n",
    "    - **주요 지표**: Faithfulness, Answer Relevancy, Context Relevancy 등 평가\n",
    "    - **실용성**: 자동화된 평가 파이프라인 구축 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) LangChain 문서 준비`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean data:\n",
      "[Document(metadata={'source': 'data\\\\리비안_KR.md'}, page_content='Rivian Automotive, Inc.는 2009년에 설립된 미국의 전기 자동차 제조업체, 자동차 기술 및 야외 레크리에이션 회사입니다.\\n\\n**주요 정보:**\\n\\n- **회사 유형:** 상장\\n- **거래소:** NASDAQ: RIVN\\n- **설립:** 2009년 6월, 플로리다 주 록ledge\\n- **설립자:** R. J. 스캐린지\\n- **본사:** 미국 캘리포니아 주 어바인\\n- **서비스 지역:** 북미\\n- **주요 인물:** R. J. 스캐린지 (CEO)\\n- **제품:** 전기 자동차, 배터리\\n- **생산량 (2023):** 57,232대\\n- **서비스:** 전기 자동차 충전, 자동차 보험\\n- **수익 (2023):** 44억 3천만 미국 달러\\n- **순이익 (2023):** -54억 미국 달러\\n- **총 자산 (2023):** 168억 미국 달러\\n- **총 자본 (2023):** 91억 4천만 미국 달러\\n- **직원 수 (2023년 12월):** 16,790명\\n- **웹사이트:** rivian.com\\n\\n**개요**\\n\\nRivian은 \"스케이트보드\" 플랫폼(R1T 및 R1S 모델)을 기반으로 한 전기 스포츠 유틸리티 차량(SUV), 픽업 트럭 및 전기 배달 밴(Rivian EDV)을 생산합니다. R1T 배송은 2021년 말에 시작되었습니다. 회사는 2022년에 미국에서 충전 네트워크를 시작하여 2024년에 다른 차량에도 개방했습니다. 생산 공장은 일리노이 주 노멀에 있으며, 다른 시설은 미국, 캐나다, 영국 및 세르비아의 여러 주에 있습니다.\\n\\n**역사**\\n\\n**초창기 (2009–15):**\\n\\n- 2009년 R. J. 스캐린지가 Mainstream Motors로 설립.\\n- 2011년 Rivian Automotive로 사명 변경.\\n- 처음에는 스포츠카 프로토타입(R1)에 집중했지만 전기 및 자율 주행 차량으로 전환.\\n\\n**생산 준비 (2016–20):**\\n\\n- 2017년 일리노이 주 노멀에 있는 이전 Mitsubishi Motors 제조 공장을 1,600만 달러에 인수.\\n- 2017년 12월, 첫 두 제품인 R1T (픽업 트럭)와 R1S (SUV)를 공개.\\n- 생산은 2020년에 시작될 예정.\\n\\n**첫 모델 배송; IPO; 감원 및 확장 (2021–24):**\\n\\n- R1T 배송은 2021년 9월에 시작되어 Rivian은 완전 전기 픽업을 소비자 시장에 출시한 최초의 자동차 제조업체가 됨.\\n- 2021년 11월, Rivian은 IPO를 통해 135억 달러를 조달하여 상장 회사가 됨.\\n- Ford와 Rivian은 전기 자동차 공동 개발 계획을 취소.\\n- 2022년 3월, Rivian은 부품 비용으로 인해 R1T 및 R1S의 가격 인상을 발표했으며, 이후 소급 적용에 대해 사과.\\n- 2022년 7월, Rivian은 6%의 인력 감축을 발표.\\n- 2022년 9월, 유럽에서 전기 밴을 생산하기 위해 Mercedes-Benz Group과 합작 투자를 발표했지만 나중에 취소됨.\\n- 2022년 10월, 느슨한 토크 볼트로 인해 13,000대의 차량을 자발적으로 리콜.\\n- 2023년 6월, Tesla의 북미 충전 시스템(NACS) 채택을 발표.\\n- 2023년 6월, \"A Better Route Planner\"(ABRP) EV 경로 계획 앱을 개발한 Iternio 인수.\\n- 2023년 10월, Amazon과의 독점 계약을 종료하여 Rivian이 다른 상업 고객에게 공급할 수 있도록 허용.\\n- 2024년 2월, 급여 직원 10% 감축 발표.\\n- 2024년 3월, 더 작은 가격대의 R2 SUV 및 R3 공개.\\n\\n**Volkswagen과의 파트너십 (2024)**\\n\\n- 2024년 6월, Volkswagen Group은 전기 아키텍처 및 소프트웨어 기술 개발을 목표로 Rivian에 최대 50억 달러를 투자할 의향을 발표.\\n\\n**차량**\\n\\n- **R1T:** 4개의 전기 모터가 장착된 픽업 트럭. 배터리 크기는 105 kWh에서 180 kWh까지 다양함.\\n- **R1S:** 첫 번째 Rivian 플랫폼의 스포츠 유틸리티 차량(SUV) 버전.\\n- **Electric Delivery Van (EDV):** 상업용 전기 밴으로, 주로 Amazon용으로 설계되어 사용됨.\\n- **R2:** 더 작고 저렴한 SUV로, 새로운 플랫폼에서 2026년 초에 출시될 예정.\\n- **R3:** 출시 예정인 전기 소형 SUV.\\n\\n**EV 충전**\\n\\nRivian은 미국과 캐나다 전역에 공공 충전소 네트워크를 개발하고 있습니다. Rivian은 2021년에 Waypoint 충전기 설치를 시작했지만 2023년 4월에 Rivian Adventure Network가 Rivian 이외의 차량에도 개방될 것이라고 발표되었습니다. Rivian은 또한 2025년 모델 연도부터 북미에서 차량에 북미 충전 시스템(NACS)을 채택할 예정입니다.\\n\\n**시설**\\n\\n- **Irvine, California:** 차량 엔지니어링 및 설계에 중점을 둔 본사.\\n- **Normal, Illinois:** 차량 부품을 생산하고 조립을 수행하는 제조 공장.\\n- **Plymouth, Michigan:** 차량 엔지니어링, 프로토타입 제작, 공급망 및 회계에 중점을 둡니다.\\n- **Palo Alto, California:** 소프트웨어 개발 및 엔지니어링에 중점을 둡니다.\\n- Carson, California 및 Woking, England에 추가 사무실이 있습니다.\\n- 애틀랜타 동쪽에 있는 새로운 50억 달러 규모의 배터리 및 조립 공장은 보류 중입니다.\\n\\n**재정**\\n\\nRivian의 재무 성과는 상당한 수익 성장과 상당한 순손실로 특징지어집니다.\\n\\n| 연도 | 수익 (백만 USD) | 순이익 (백만 USD) | 총 자산 (백만 USD) |\\n| ---- | --------------- | ----------------- | ------------------ |\\n| 2020 | 0               | -1,018            | 4,602              |\\n| 2021 | 55              | -4,688            | 22,294             |\\n| 2022 | 1,658           | -6,752            | 17,876             |\\n| 2023 | 4,434           | -5,432            | 16,778             |\\n\\n**최대 주주**\\n\\n2023년 12월 현재 최대 주주는 Amazon, T. Rowe Price International, The Vanguard Group, BlackRock 및 Fidelity Investments였습니다.\\n\\n**협력**\\n\\nRivian은 Alex Honnold, the Honnold Foundation, Casa Pueblo, Ewan McGregor, Charley Boorman, Yakima 및 MAXTRAX와 파트너십을 맺었습니다.\\n\\n**소송**\\n\\n- 2020년 7월, Tesla는 Rivian이 독점 정보를 훔치고 직원을 빼갔다고 주장하며 소송을 제기했습니다.\\n- 2021년 3월, Illinois Automobile Dealers Association은 Rivian과 Lucid Motors가 소비자에게 직접 판매했다는 이유로 소송을 제기했습니다.\\n- 2021년 11월, 전 VP Laura Schwab은 차별 혐의로 소송을 제기하고 차량 가격 책정 및 안전 표준에 대한 우려를 제기했습니다.\\n'),\n",
      " Document(metadata={'source': 'data\\\\테슬라_KR.md'}, page_content='Tesla, Inc.는 미국의 다국적 자동차 및 청정 에너지 회사입니다. 이 회사는 전기 자동차(BEV), 고정형 배터리 에너지 저장 장치, 태양 전지판, 태양광 지붕널 및 관련 제품/서비스를 설계, 제조 및 판매합니다. 2003년 7월 Martin Eberhard와 Marc Tarpenning이 Tesla Motors로 설립했으며, Nikola Tesla를 기리기 위해 명명되었습니다. Elon Musk는 2004년 Tesla의 초기 자금 조달을 주도하여 2008년에 회장 겸 CEO가 되었습니다.\\n\\nTesla의 차량 생산은 2008년 Roadster로 시작하여 Model S (2012), Model X (2015), Model 3 (2017), Model Y (2020), Semi (2022) 및 Cybertruck (2023)으로 이어졌습니다. 이 회사는 세계에서 가장 가치 있는 회사 중 하나이며 2020년 7월 이후 가장 가치 있는 자동차 제조업체입니다. Tesla는 2021~2022년, 그리고 2024년에 다시 한 번 한때 1조 달러의 가치를 달성했습니다. 2023년에는 배터리 전기 자동차 시장에서 가장 큰 점유율(19.9%)을 차지했으며 Forbes Global 2000에서 69위에 랭크되었습니다.\\n\\nTesla는 내부 고발자 보복, 근로자 권리 침해, 안전 결함, 홍보 부족, Musk의 논란의 여지가 있는 발언과 관련된 소송, 정부 조사 및 비판에 직면했습니다.\\n\\n## 역사\\n\\n### 창립 (2003–2004)\\n\\nTesla Motors, Inc.는 2003년 7월 1일에 Martin Eberhard와 Marc Tarpenning에 의해 설립되었으며, 각각 CEO와 CFO를 역임했습니다. Ian Wright는 얼마 지나지 않아 합류했습니다. 2004년 2월, Elon Musk는 750만 달러의 시리즈 A 자금 조달을 주도하여 회장 겸 최대 주주가 되었습니다. J. B. Straubel은 2004년 5월 CTO로 합류했습니다. 다섯 명 모두 공동 설립자로 인정받고 있습니다.\\n\\n### Roadster (2005–2009)\\n\\nElon Musk는 주류 차량으로 확장하기 전에 프리미엄 스포츠카로 시작하는 전략에 초점을 맞춰 적극적인 역할을 수행했습니다. 후속 자금 조달에는 Valor Equity Partners (2006)와 Sergey Brin, Larry Page, Jeff Skoll과 같은 기업가의 투자가 포함되었습니다.\\n\\n2007년 8월, Eberhard는 CEO에서 물러나라는 요청을 받았고, Tarpenning은 2008년 1월에 이어졌습니다. Michael Marks는 Ze\\'ev Drori가 인수하기 전에 임시 CEO를 역임했으며, Musk는 2008년 10월에 인수했습니다. Eberhard는 2009년 6월 Musk를 상대로 소송을 제기했지만 나중에 기각되었습니다.\\n\\nRoadster 생산은 2008년에 시작되었습니다. 2009년 1월까지 Tesla는 1억 8,700만 달러를 모금하고 147대의 자동차를 인도했습니다. 2009년 6월, Tesla는 미국 에너지부로부터 4억 6,500만 달러의 대출을 받았으며, 2013년 5월에 이자와 함께 상환했습니다.\\n\\n### IPO, Model S 및 Model X (2010–2015)\\n\\n2010년 5월, Tesla는 캘리포니아 주 프리몬트의 NUMMI 공장을 Toyota로부터 인수했습니다. 이 회사는 2010년 6월 NASDAQ에 상장하여 2억 2,600만 달러를 조달했습니다.\\n\\nTesla는 2012년 6월 Model S 고급 세단을 출시했습니다. Model S는 여러 자동차 상을 받았으며 노르웨이(2013년 9월)와 전 세계(2015년 및 2016년)에서 가장 많이 팔린 전기 자동차가 되었습니다. Tesla는 2013년 7월 NASDAQ-100에 합류했습니다.\\n\\n2014년, Tesla는 운전자 지원 시스템인 Autopilot을 발표했습니다. Tesla는 2015년 4월 Powerwall 및 Powerpack과 함께 에너지 저장 시장에 진출하여 일주일 만에 8억 달러의 주문을 받았습니다. Model X SUV는 2015년 9월에 출시되었습니다.\\n\\n### SolarCity 및 Model 3 (2016–2018)\\n\\nTesla는 2016년 11월 SolarCity를 26억 달러에 인수하여 Tesla Energy를 설립했습니다. 2017년 2월, Tesla Motors는 사명을 Tesla, Inc.로 변경했습니다.\\n\\nModel 3 세단은 2016년 4월에 공개되어 일주일 만에 325,000건 이상의 예약이 접수되었습니다. \"생산 지옥\"으로 묘사된 생산 문제로 인해 지연이 발생했습니다. 2018년 말까지 Model 3는 세계에서 가장 많이 팔린 전기 자동차(2018-2021)가 되었습니다. 2018년 8월, Musk는 Tesla를 비상장으로 전환할 계획을 발표했지만 실현되지 않았고 논란과 SEC의 증권 사기 혐의로 이어졌습니다.\\n\\n### 글로벌 확장 및 Model Y (2019–현재)\\n\\n2019년 7월부터 2020년 6월까지 Tesla는 4분기 연속 흑자를 보고했으며 S&P 500에 추가되었습니다. 2020년에는 주가가 크게 상승했으며 시가 총액은 다음 9개의 대형 자동차 제조업체를 합친 것보다 많았습니다.\\n\\nModel Y 중형 크로스오버 SUV는 2019년 3월에 소개되었으며 배송은 2020년 3월에 시작되었습니다. Tesla는 Gigafactory Shanghai (2019), Gigafactory Berlin (2020) 및 Gigafactory Texas (2020)로 생산 능력을 확장했습니다.\\n\\n2023년 3월, Tesla는 잠재적인 관세 영향으로 인해 지연된 Gigafactory Mexico에 대한 계획을 발표했습니다. COVID-19 팬데믹 초기에 Tesla는 2020년 3월에 프리몬트 공장을 폐쇄하고 지방 당국과의 분쟁 후 2020년 5월 11일에 재개장했습니다. Tesla는 2021년 12월에 법적 본사를 Gigafactory Texas로 이전했지만 Megafactory (2022)와 Palo Alto의 글로벌 엔지니어링 본사(2023)로 캘리포니아에서의 입지를 확장했습니다.\\n\\n2021년 초, Tesla는 Bitcoin에 15억 달러를 투자하고 환경 문제로 인해 중단하기 전에 잠시 결제 수단으로 허용했습니다. 2022년 7월까지 Tesla는 Bitcoin 보유량의 약 75%를 매각했습니다. 2023년 5월과 2024년 2월 사이에 북미 EV 제조업체는 Tesla의 북미 충전 표준으로 전환할 계획을 발표했습니다.\\n\\n2023년 11월, Tesla는 Cybertruck 배송을 시작했습니다. 2024년 4월, 회사는 직원 10% 감축을 발표하고 6월에 법인 설립지를 델라웨어에서 텍사스로 이전했습니다. 2024년 10월, Tesla는 미래의 차량 호출 서비스인 Tesla Network를 위해 Cybercab 및 Robovan의 컨셉 버전을 공개했습니다.\\n\\n2024년 12월, 델라웨어 법원은 부적절한 이사회 승인을 이유로 Elon Musk의 560억 달러 급여 패키지를 거부했습니다.\\n\\n## 자동차 제품 및 서비스\\n\\n2024년 11월 현재 Tesla는 Model S, Model X, Model 3, Model Y, Semi 및 Cybertruck의 6가지 차량 모델을 제공합니다. 다음은 Tesla 모델 목록입니다.\\n\\n| Tesla 모델 |          |       |                  |\\n| :--------- | :------- | :---- | :--------------- |\\n| 이름       | 제조년도 | 좌석  | 참고             |\\n| Roadster   | 2008     | 2     | 2012년에 단종    |\\n| Model S    | 2012     | 5/7   |                  |\\n| Model X    | 2015     | 5/6/7 |                  |\\n| Model 3    | 2017     | 5     |                  |\\n| Model Y    | 2020     | 5/7   |                  |\\n| Semi       | 2022     | 2     |                  |\\n| Cybertruck | 2023     | 5     |                  |\\n| Roadster 2 |          | 2/4   | 2025년 출시 예정 |\\n| Cybercab   |          | 2     | 2026년 출시 예정 |\\n| Robovan    |          | 20    | 명시된 기간 없음 |\\n\\n### 사용 가능한 제품\\n\\n- **Model S:** 리프트백 차체 스타일과 듀얼 모터, 전륜 구동 레이아웃을 갖춘 풀사이즈 고급차. Model S 개발은 2007년 이전에 시작되었으며 배송은 2012년 6월에 시작되었습니다.\\n- **Model X:** 듀얼 모터 또는 트리 모터, 전륜 구동 레이아웃을 갖춘 5인승, 6인승 및 7인승 구성으로 제공되는 중형 고급 크로스오버 SUV. 뒷좌석 승객 문은 관절형 \"팔콘 윙\" 디자인으로 수직으로 열립니다. Model X 프로토타입은 2012년 2월에 처음 공개되었으며 배송은 2015년 9월에 시작되었습니다.\\n- **Model 3:** 패스트백 차체 스타일과 듀얼 모터, 전륜 구동 레이아웃 또는 후륜 모터, 후륜 구동 레이아웃을 갖춘 중형차. 이 차량은 고급 Model S 세단보다 저렴하도록 설계되었습니다. Model 3 프로토타입은 2016년에 처음 공개되었으며 일주일 만에 325,000건 이상의 유료 예약이 접수되었습니다.\\n- **Model Y:** 싱글 모터, 후륜 구동 또는 듀얼 모터, 전륜 구동 레이아웃을 갖춘 5인승 및 7인승 구성으로 제공되는 중형 크로스오버 SUV. 이 차량은 고급 Model X SUV보다 저렴하도록 설계되었습니다. Model Y 프로토타입은 2019년 3월에 처음 공개되었으며 배송은 2020년 3월에 시작되었습니다.\\n- **Tesla Semi:** Tesla Semi는 Tesla, Inc.의 클래스 8 세미 트럭으로, 트리 모터, 후륜 구동 레이아웃을 갖추고 있습니다. Tesla는 Semi가 일반적인 디젤 세미 트럭보다 약 3배 더 강력하고 주행 거리가 500마일(800km)이라고 주장합니다. 초기 배송은 2022년 12월 1일에 PepsiCo에 이루어졌습니다.\\n- **Cybertruck:** 2019년 11월에 처음 발표된 풀사이즈 픽업 트럭. 후륜 구동, 듀얼 모터 전륜 구동, 트리 모터 전륜 구동의 세 가지 모델이 제공됩니다.\\n\\n### 발표된 제품\\n\\n- **Roadster (2세대):** 2017년에 공개되었으며 620마일(1,000km)의 주행 거리와 고성능 사양을 갖춘 것으로 알려져 있습니다. 2025년에 생산 예정입니다.\\n- **Tesla 차세대 차량:** 2025년 상반기에 배송될 것으로 예상되는 발표된 배터리 전기 플랫폼.\\n- **Cybercab:** 출시 예정인 2인승 배터리 전기 자율 주행차.\\n- **Robovan:** 미래 개발을 위해 계획된 전기 자율 밴.\\n\\n### 단종\\n\\n- **Tesla Roadster:** 2008년부터 2012년까지 생산된 2인승 스포츠카.\\n\\n### 연결 서비스\\n\\nTesla 자동차에는 내비게이션을 위한 \"표준 연결\"이 함께 제공됩니다. \"프리미엄 연결\"은 구독료로 실시간 교통 정보, 위성 지도, 인터넷 검색 및 미디어 스트리밍을 추가합니다.\\n\\n### 차량 서비스\\n\\nTesla는 원격 진단 및 수리를 우선시하여 모바일 기술자를 파견하거나 필요한 경우 고객을 서비스 센터로 안내합니다. 이 회사는 전 세계에 소매점, 갤러리, 서비스, 배송 및 차체 수리점을 운영하고 있습니다. Tesla는 차량 서비스에서 이익을 얻지 않는 것을 목표로 합니다. 2019년 초에 연간 유지 보수 요구 사항이 제거되었고 대신 브레이크액, 에어컨, 타이어 및 에어 필터를 주기적으로 서비스할 것을 권장합니다.\\n\\n### 충전 서비스\\n\\n- **Supercharger 네트워크:** Tesla의 고전압 DC 급속 충전 네트워크로, 2012년에 도입되었습니다.\\n- **Destination 충전 위치 네트워크:** 호텔, 레스토랑 및 쇼핑 센터에 있는 더 느린 충전기.\\n\\n### 보험 서비스\\n\\nTesla는 Tesla Insurance Services, Inc.를 통해 미국에서 차량 보험을 제공하고 차량 데이터를 사용하여 개인 맞춤 가격을 제공합니다.\\n\\n## 에너지 제품\\n\\nTesla Energy는 태양 에너지 생성 시스템과 배터리 에너지 저장 제품을 개발, 구축, 판매 및 설치합니다. 제품에는 태양 전지판, Solar Roof, Solar Inverter, Powerwall 및 Megapack이 포함됩니다. 이 회사는 대규모 고객을 위한 자동화된 전력 거래, 수요 예측 및 제품 제어를 위한 온라인 플랫폼을 운영하고 있습니다.\\n\\n## 비즈니스 전략\\n\\nTesla의 전략은 배터리 비용을 줄이기 위해 고가, 소량 차량으로 시작한 다음 더 저렴하고 대량 차량을 제공하는 것입니다. Tesla는 자동차의 하드웨어를 지속적으로 업데이트하고 웹사이트와 회사 소유 매장을 통해 직접 차량을 판매합니다. Tesla는 수직적으로 통합되어 많은 구성 요소를 자체 개발합니다. Tesla는 일반적으로 지속 가능한 에너지 채택을 촉진하기 위해 경쟁 업체가 자사 기술을 라이선스하도록 허용합니다.\\n\\n## 기술\\n\\n### 배터리\\n\\nTesla는 CATL, LG Energy Solution 및 Panasonic에서 공급받은 원통형 및 각형 배터리 셀을 사용하고 있으며 자체 배터리를 생산할 수 있는 능력을 구축하고 있습니다.\\n\\n### 소프트웨어\\n\\nTesla는 무선 업데이트를 사용하여 강력한 온보드 컴퓨터를 통해 기능을 추가하거나 문제를 해결합니다.\\n\\n### 모터\\n\\nTesla는 유도 모터와 동기 릴럭턴스 모터(SynRM) 특성을 가진 내부 영구 자석(IPM) 모터를 만듭니다.\\n\\n### 북미 충전 표준\\n\\n북미 충전 표준(NACS)은 Tesla에서 개발한 전기 자동차 충전 커넥터 시스템입니다.\\n\\n### Autopilot 및 완전 자율 주행\\n\\nTesla Autopilot은 Tesla에서 개발한 고급 운전자 지원 시스템(ADAS)으로, 부분적인 차량 자동화를 의미합니다.\\n\\n### 로봇 공학\\n\\nTesla는 대형 주조 기계(Giga Press)를 사용하여 대형 단일 피스 하체를 만듭니다. 이 회사는 2022년부터 Optimus라는 휴머노이드 로봇을 개발해 왔습니다.\\n\\n## 시설\\n\\nTesla는 소매점, 갤러리 및 서비스 센터를 포함하여 전 세계에 대규모 공장과 소규모 시설을 운영하고 있습니다.\\n\\n**Tesla에서 운영하는 주요 시설**\\n\\n| 개장 | 이름                | 도시                    | 국가 | 직원 수 | 제품                               |\\n| :--- | :------------------ | :---------------------- | :--- | :------ | :--------------------------------- |\\n| 2010 | Tesla 프리몬트 공장 | 캘리포니아 주 프리몬트  | 미국 | 22,000  | Model S, Model X, Model 3, Model Y |\\n| 2016 | Gigafactory 네바다  | 네바다 주 스토리 카운티 | 미국 | 7,000   | 배터리, Powerwall, Semi            |\\n| 2017 | Gigafactory 뉴욕    | 뉴욕 주 버팔로          | 미국 | 1,500   | Solar Roof, Supercharger           |\\n| 2019 | Gigafactory 상하이  | 상하이                  | 중국 | 20,000  | Model 3, Model Y, Supercharger     |\\n| 2022 | Gigafactory 베를린  | 그륀하이데              | 독일 | 10,000  | Model Y                            |\\n| 2022 | Gigafactory 텍사스  | 텍사스 주 오스틴        | 미국 | 12,000  | Model Y, Cybertruck                |\\n\\n## 파트너\\n\\nTesla는 Panasonic과 파트너십을 맺고 있으며 리튬 공급에 대한 장기 계약을 맺고 있습니다. 이전 파트너로는 Daimler와 Toyota가 있습니다.\\n\\n## 소송 및 논란\\n\\nTesla는 성희롱, 노동 분쟁, 사기 혐의, 대리점 분쟁, 지적 재산권, 환경 위반, 재산 피해, 인종 차별, COVID-19 팬데믹 대응 및 수리 권리와 관련된 소송 및 논란에 직면했습니다.\\n\\n## 비판\\n\\nTesla는 데이터 개인 정보 보호, 공매도자, 지연, 차량 제품 문제, 화재, Autopilot 충돌, 소프트웨어 해킹, 가상 제동 및 주행 거리 성능과 관련된 비판에 직면했습니다.\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# 데이터 로드\n",
    "def load_text_files(txt_files):\n",
    "    data = []\n",
    "\n",
    "    for text_file in txt_files:\n",
    "        loader = TextLoader(text_file, encoding='utf-8')\n",
    "        data += loader.load()\n",
    "\n",
    "    return data\n",
    "\n",
    "korean_txt_files = glob(os.path.join('data', '*_KR.md')) \n",
    "korean_data = load_text_files(korean_txt_files)\n",
    "\n",
    "print('Korean data:')\n",
    "pprint(korean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 문서 수: 39\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'data\\\\리비안_KR.md'}\n",
      "('Rivian Automotive, Inc.는 2009년에 설립된 미국의 전기 자동차 제조업체, 자동차 기술 및 야외 레크리에이션 '\n",
      " '회사입니다.\\n'\n",
      " '\\n'\n",
      " '**주요 정보:**')\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'data\\\\리비안_KR.md'}\n",
      "('- **회사 유형:** 상장\\n'\n",
      " '- **거래소:** NASDAQ: RIVN\\n'\n",
      " '- **설립:** 2009년 6월, 플로리다 주 록ledge\\n'\n",
      " '- **설립자:** R. J. 스캐린지\\n'\n",
      " '- **본사:** 미국 캘리포니아 주 어바인\\n'\n",
      " '- **서비스 지역:** 북미\\n'\n",
      " '- **주요 인물:** R. J. 스캐린지 (CEO)\\n'\n",
      " '- **제품:** 전기 자동차, 배터리\\n'\n",
      " '- **생산량 (2023):** 57,232대\\n'\n",
      " '- **서비스:** 전기 자동차 충전, 자동차 보험\\n'\n",
      " '- **수익 (2023):** 44억 3천만 미국 달러\\n'\n",
      " '- **순이익 (2023):** -54억 미국 달러\\n'\n",
      " '- **총 자산 (2023):** 168억 미국 달러')\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 문장을 구분하여 분할 - 정규표현식 사용 (문장 구분자: 마침표, 느낌표, 물음표 다음에 공백이 오는 경우)\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",    # TikToken 인코더 이름\n",
    "    separators=['\\n\\n', '\\n', r'(?<=[.!?])\\s+'],   # 구분자\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=0,\n",
    "    is_separator_regex=True,      # 구분자가 정규식인지 여부\n",
    "    keep_separator=True,          # 구분자 유지 여부\n",
    ")\n",
    "\n",
    "korean_docs = text_splitter.split_documents(korean_data)\n",
    "\n",
    "print(\"한국어 문서 수:\", len(korean_docs))\n",
    "print(\"-\"*100)\n",
    "print(korean_docs[0].metadata)\n",
    "pprint(korean_docs[0].page_content)\n",
    "print(\"-\"*100)\n",
    "print(korean_docs[1].metadata)\n",
    "pprint(korean_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장된 Document 개수: 39\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI Embeddings 모델을 로드\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Chroma 벡터 저장소 생성하기\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=korean_docs,\n",
    "    embedding=embedding_model,    \n",
    "    collection_name=\"db_korean_cosine\", \n",
    "    persist_directory=\"./chroma_db\",\n",
    "    collection_metadata = {'hnsw:space': 'cosine'}, # l2, ip, cosine 중에서 선택 \n",
    ")\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"저장된 Document 개수: {len(vector_store.get()['ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) LLM 설정`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# LLM과 임베딩 모델 초기화\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\", temperature=0.2))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) Test Data 생성`\n",
    "\n",
    "- uv add rapidfuzz 설치 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.persona import Persona\n",
    "\n",
    "# 페르소나 정의 (다양한 관점에서 질문 생성)\n",
    "personas = [\n",
    "    Persona(\n",
    "        name=\"graduate_researcher\",  # 박사과정 연구원: 심도 있는 분석적 질문\n",
    "        role_description=\"미국 전기차 시장을 연구하는 한국인 박사과정 연구원으로, 전기차 정책과 시장 동향에 대해 깊이 있는 분석을 하고 있습니다. 한국어만을 사용합니다.\",\n",
    "    ),\n",
    "    Persona(\n",
    "        name=\"masters_student\",    # 석사과정 학생: 개념 이해를 위한 질문\n",
    "        role_description=\"전기차 산업을 공부하는 한국인 석사과정 학생으로, 미국 전기차 시장의 기초적인 개념과 트렌드를 이해하려 노력하고 있습니다. 한국어만을 사용합니다.\",\n",
    "    ),\n",
    "    Persona(\n",
    "        name=\"industry_analyst\",   # 산업 분석가: 실무 중심적 질문\n",
    "        role_description=\"한국 자동차 회사에서 미국 전기차 시장을 분석하는 주니어 연구원으로, 실무적인 시장 데이터와 경쟁사 동향에 관심이 많습니다. 한국어만을 사용합니다.\",\n",
    "    ),\n",
    "    Persona(\n",
    "        name=\"undergraduate_student\",  # 학부생: 기초적인 학습 질문\n",
    "        role_description=\"자동차 공학을 전공하는 한국인 학부생으로, 미국 전기차 기술과 시장에 대해 기본적인 지식을 습득하고자 합니다. 한국어만을 사용합니다.\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying SummaryExtractor:  21%|██        | 7/34 [01:29<07:27, 16.56s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 454. Please try again in 908ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  26%|██▋       | 9/34 [01:41<04:52, 11.68s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  29%|██▉       | 10/34 [01:48<04:08, 10.34s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 439. Please try again in 878ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  32%|███▏      | 11/34 [01:59<04:03, 10.58s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 407. Please try again in 814ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  35%|███▌      | 12/34 [02:05<03:26,  9.39s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 403. Please try again in 806ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  38%|███▊      | 13/34 [02:06<02:20,  6.69s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 365. Please try again in 730ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  41%|████      | 14/34 [02:13<02:18,  6.93s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  47%|████▋     | 16/34 [02:26<01:48,  6.03s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29781, Requested 457. Please try again in 476ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  59%|█████▉    | 20/34 [02:36<00:57,  4.09s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  65%|██████▍   | 22/34 [02:41<00:35,  2.96s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29911, Requested 389. Please try again in 600ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  71%|███████   | 24/34 [02:52<00:45,  4.56s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  74%|███████▎  | 25/34 [02:53<00:30,  3.37s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 448. Please try again in 896ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  76%|███████▋  | 26/34 [02:57<00:29,  3.73s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  79%|███████▉  | 27/34 [03:10<00:45,  6.51s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 467. Please try again in 934ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  82%|████████▏ | 28/34 [04:02<02:00, 20.05s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 430. Please try again in 860ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  88%|████████▊ | 30/34 [04:09<00:45, 11.40s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 491. Please try again in 982ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  91%|█████████ | 31/34 [04:16<00:30, 10.02s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29673, Requested 460. Please try again in 266ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  94%|█████████▍| 32/34 [04:24<00:18,  9.34s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying SummaryExtractor:  97%|█████████▋| 33/34 [04:24<00:06,  6.77s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29960, Requested 431. Please try again in 782ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying CustomNodeFilter:   0%|          | 0/39 [00:00<?, ?it/s]         Node 5ceaf7f3-8dd1-459f-9273-ca8ae982aa14 does not have a summary. Skipping filtering.\n",
      "Node 20ef2c1a-d026-4c30-b423-55ca40eaf806 does not have a summary. Skipping filtering.\n",
      "Node b60b493b-0c50-4228-b8d5-491da1cc9dc7 does not have a summary. Skipping filtering.\n",
      "Node 31eaa65a-07e7-40e4-a045-401d65167861 does not have a summary. Skipping filtering.\n",
      "Node c40f2b1f-df43-4140-b4f9-6efafb7b232d does not have a summary. Skipping filtering.\n",
      "Node 8a15fb07-e55f-4a59-af5e-06f7f9812f53 does not have a summary. Skipping filtering.\n",
      "Node c64f89f2-d1dd-464d-9531-8f1dbe9414ee does not have a summary. Skipping filtering.\n",
      "Node 1c4550eb-d1de-40ba-898a-fca2e85d4167 does not have a summary. Skipping filtering.\n",
      "Node 145d28a2-fd6e-4448-8a74-77b5775d874c does not have a summary. Skipping filtering.\n",
      "Node af0f0a76-b215-4503-817a-5a1816a62572 does not have a summary. Skipping filtering.\n",
      "Node 92d781d5-28bf-45b2-b6f1-6843a09720e4 does not have a summary. Skipping filtering.\n",
      "Node 82021a48-437c-413c-b092-7df793a536e4 does not have a summary. Skipping filtering.\n",
      "Node a2b98d3b-3471-44ac-9c44-143954e99bfe does not have a summary. Skipping filtering.\n",
      "Node dd567815-ca81-4659-a235-33c5de8f476a does not have a summary. Skipping filtering.\n",
      "Node 19795cf9-1ea0-4be2-823e-1b0a338293d9 does not have a summary. Skipping filtering.\n",
      "Node 37adbfc7-ba2f-4620-b7ec-70dd1ac3032d does not have a summary. Skipping filtering.\n",
      "Node 512c53e1-e002-4953-b0be-c9975efa47d6 does not have a summary. Skipping filtering.\n",
      "Node 221b838e-3d3c-4ded-9aa9-3ea066612af3 does not have a summary. Skipping filtering.\n",
      "Node 99acc4fe-0204-4843-b03a-1328804dff94 does not have a summary. Skipping filtering.\n",
      "Node 4bedc886-aa6b-4463-9f97-fe3ed97a5462 does not have a summary. Skipping filtering.\n",
      "Node 4b924f51-8269-48e2-8fb1-1bd00b76d355 does not have a summary. Skipping filtering.\n",
      "Node 03fd9e09-ea1f-4cfd-82b1-75013dc4f24b does not have a summary. Skipping filtering.\n",
      "Node 0b68da3d-54d1-4cf0-a122-fa5e5cdb8ef7 does not have a summary. Skipping filtering.\n",
      "Node fe16864a-5e39-4feb-b992-8d833c5e7052 does not have a summary. Skipping filtering.\n",
      "unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29695, Requested 695. Please try again in 780ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying CustomNodeFilter:  67%|██████▋   | 26/39 [01:46<00:53,  4.15s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29916, Requested 555. Please try again in 941ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying CustomNodeFilter:  69%|██████▉   | 27/39 [01:47<00:47,  3.97s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29597, Requested 687. Please try again in 568ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying CustomNodeFilter:  72%|███████▏  | 28/39 [01:49<00:41,  3.73s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29567, Requested 631. Please try again in 396ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying CustomNodeFilter:  77%|███████▋  | 30/39 [02:02<00:40,  4.47s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 577. Please try again in 1.154s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying CustomNodeFilter:  79%|███████▉  | 31/39 [02:05<00:34,  4.28s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29672, Requested 641. Please try again in 626ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying CustomNodeFilter:  82%|████████▏ | 32/39 [02:09<00:28,  4.11s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29723, Requested 551. Please try again in 548ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying CustomNodeFilter:  85%|████████▍ | 33/39 [02:16<00:29,  4.87s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 645. Please try again in 1.29s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying CustomNodeFilter:  87%|████████▋ | 34/39 [02:25<00:28,  5.74s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29905, Requested 723. Please try again in 1.256s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying CustomNodeFilter:  90%|████████▉ | 35/39 [02:31<00:23,  5.92s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 682. Please try again in 1.364s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying CustomNodeFilter:  92%|█████████▏| 36/39 [02:34<00:15,  5.14s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29468, Requested 600. Please try again in 136ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying CustomNodeFilter:  95%|█████████▍| 37/39 [02:44<00:12,  6.42s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 586. Please try again in 1.172s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying CustomNodeFilter:  97%|█████████▋| 38/39 [02:44<00:04,  4.70s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29984, Requested 595. Please try again in 1.158s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   1%|          | 1/112 [00:07<13:05,  7.07s/it]unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  32%|███▏      | 36/112 [00:18<00:35,  2.12it/s]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  33%|███▎      | 37/112 [01:41<06:05,  4.88s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 409. Please try again in 818ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  34%|███▍      | 38/112 [01:48<06:10,  5.01s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29902, Requested 413. Please try again in 630ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  36%|███▌      | 40/112 [01:51<05:11,  4.32s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29695, Requested 447. Please try again in 284ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  37%|███▋      | 41/112 [01:54<04:56,  4.17s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 411. Please try again in 822ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  38%|███▊      | 42/112 [02:02<05:30,  4.72s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 401. Please try again in 802ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  38%|███▊      | 43/112 [02:06<05:16,  4.59s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  39%|███▉      | 44/112 [02:09<04:57,  4.38s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29998, Requested 434. Please try again in 864ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  42%|████▏     | 47/112 [02:33<06:48,  6.28s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  43%|████▎     | 48/112 [02:34<05:09,  4.83s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  44%|████▍     | 49/112 [02:36<04:13,  4.02s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  45%|████▍     | 50/112 [02:36<03:12,  3.10s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 464. Please try again in 928ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  46%|████▌     | 51/112 [02:54<07:19,  7.21s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 454. Please try again in 908ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  47%|████▋     | 53/112 [03:14<08:06,  8.24s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 442. Please try again in 884ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  48%|████▊     | 54/112 [03:16<06:11,  6.41s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  50%|█████     | 56/112 [03:25<05:29,  5.88s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29937, Requested 455. Please try again in 784ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  52%|█████▏    | 58/112 [04:12<12:25, 13.81s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 422. Please try again in 844ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  53%|█████▎    | 59/112 [04:13<08:56, 10.12s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 402. Please try again in 804ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  54%|█████▎    | 60/112 [04:15<06:38,  7.65s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 398. Please try again in 796ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  54%|█████▍    | 61/112 [04:26<07:10,  8.44s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29693, Requested 371. Please try again in 128ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  55%|█████▌    | 62/112 [04:32<06:36,  7.92s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 452. Please try again in 904ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  56%|█████▋    | 63/112 [04:35<05:06,  6.25s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  57%|█████▋    | 64/112 [04:42<05:15,  6.58s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29864, Requested 485. Please try again in 698ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  58%|█████▊    | 65/112 [04:48<04:57,  6.33s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 335. Please try again in 670ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  59%|█████▉    | 66/112 [04:49<03:36,  4.71s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  60%|█████▉    | 67/112 [04:54<03:38,  4.85s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 413. Please try again in 826ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  61%|██████    | 68/112 [04:56<02:55,  3.99s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29895, Requested 426. Please try again in 642ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  62%|██████▏   | 69/112 [05:06<04:08,  5.79s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  63%|██████▎   | 71/112 [05:12<03:00,  4.40s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 475. Please try again in 950ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  64%|██████▍   | 72/112 [05:27<05:00,  7.51s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  68%|██████▊   | 76/112 [05:52<04:27,  7.43s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 416. Please try again in 832ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  69%|██████▉   | 77/112 [06:09<05:58, 10.24s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29910, Requested 440. Please try again in 700ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  70%|██████▉   | 78/112 [06:18<05:38,  9.96s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 418. Please try again in 836ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  71%|███████   | 79/112 [06:30<05:46, 10.50s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  71%|███████▏  | 80/112 [06:33<04:24,  8.27s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 436. Please try again in 872ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  73%|███████▎  | 82/112 [06:39<02:49,  5.64s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 29747, Requested 398. Please try again in 290ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  74%|███████▍  | 83/112 [06:41<02:12,  4.57s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 451. Please try again in 902ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  76%|███████▌  | 85/112 [06:48<01:55,  4.28s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  77%|███████▋  | 86/112 [06:52<01:45,  4.06s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 445. Please try again in 890ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  78%|███████▊  | 87/112 [07:09<03:25,  8.22s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 459. Please try again in 918ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  79%|███████▊  | 88/112 [07:15<03:01,  7.55s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  79%|███████▉  | 89/112 [07:22<02:49,  7.36s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 481. Please try again in 962ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  80%|████████  | 90/112 [07:26<02:18,  6.28s/it]unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 30000, Used 30000, Requested 455. Please try again in 910ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "                                                                                                              \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings, persona_list=personas)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 합성 데이터 생성\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m dataset = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkorean_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestset_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\ragas\\testset\\synthesizers\\generate.py:185\u001b[39m, in \u001b[36mTestsetGenerator.generate_with_langchain_docs\u001b[39m\u001b[34m(self, documents, testset_size, transforms, transforms_llm, transforms_embedding_model, query_distribution, run_config, callbacks, with_debugging_logs, raise_exceptions)\u001b[39m\n\u001b[32m    182\u001b[39m kg = KnowledgeGraph(nodes=nodes)\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# apply transforms and update the knowledge graph\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[43mapply_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28mself\u001b[39m.knowledge_graph = kg\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate(\n\u001b[32m    189\u001b[39m     testset_size=testset_size,\n\u001b[32m    190\u001b[39m     query_distribution=query_distribution,\n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m     raise_exceptions=raise_exceptions,\n\u001b[32m    195\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\ragas\\testset\\transforms\\engine.py:106\u001b[39m, in \u001b[36mapply_transforms\u001b[39m\u001b[34m(kg, transforms, run_config, callbacks)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transforms, t.List):\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m transforms:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m         \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_coroutines\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_execution_plan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m                \u001b[49m\u001b[43mget_desc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# if Parallel, collect inside it and run it all\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transforms, Parallel):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\selectors.py:323\u001b[39m, in \u001b[36mSelectSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    321\u001b[39m ready = []\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     r, w, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\selectors.py:314\u001b[39m, in \u001b[36mSelectSelector._select\u001b[39m\u001b[34m(self, r, w, _, timeout)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     r, w, x = \u001b[43mselect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w + x, []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "# TestsetGenerator 생성\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings, persona_list=personas)\n",
    "\n",
    "# 합성 데이터 생성\n",
    "dataset = generator.generate_with_langchain_docs(korean_docs, testset_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 저장\n",
    "dataset.to_pandas().to_csv('./data/ragas_testset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **평가 지표(Evaluation Metric)**\n",
    "\n",
    "#### 1) **검색(Retrieval) 평가**  \n",
    "\n",
    "- **Non-Rank Based Metrics**: Accuracy, Precision, Recall@k 등을 통해 관련성의 이진적 평가를 수행\n",
    "\n",
    "- **Rank-Based Metrics**: MRR(Mean Reciprocal Rank), MAP(Mean Average Precision)를 통해 검색 결과의 순위를 고려한 평가를 수행\n",
    "\n",
    "- **RAG 특화 지표**: 기존 검색 평가 방식의 한계를 보완하는 LLM-as-judge 방식 도입\n",
    "\n",
    "- **포괄적 평가**: 정확도, 관련성, 다양성, 강건성을 통합적으로 측정\n",
    "\n",
    "#### 2) **생성(Generation) 평가**\n",
    "\n",
    "- **전통적 평가**: ROUGE(요약), BLEU(번역), BertScore(의미 유사도) 지표 활용\n",
    "\n",
    "- **LLM 기반 평가**: 응집성, 관련성, 유창성을 종합적으로 판단하는 새로운 접근법 도입 (전통적인 참조 비교가 어려운 상황에서 유용)\n",
    "\n",
    "- **다차원 평가**: 품질, 일관성, 사실성, 가독성, 사용자 만족도를 포괄적 측정\n",
    "\n",
    "- **상세 프롬프트**와 **사용자 선호도** 기준으로 생성 텍스트 품질 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) RAG 체인 - 평가 대상`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tesla는 2003년 7월 1일에 Martin Eberhard와 Marc Tarpenning에 의해 설립되었습니다.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 벡터 저장소 검색기 생성\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# RAG 체인 \n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# 템플릿 생성\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "def format_docs(relevant_docs):\n",
    "    return \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "\n",
    "query = \"Tesla는 언제 누가 만들었나?\"\n",
    "\n",
    "relevant_docs = retriever.invoke(query)\n",
    "qa_chain.invoke({\"context\": format_docs(relevant_docs), \"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 평가 수행을 위한 데이터셋 전처리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tesla, Inc.는 미국에서 어떤 역할을 하고 있으며, 이 회사의 주요 제품과 ...</td>\n",
       "      <td>['Tesla, Inc.는 미국의 다국적 자동차 및 청정 에너지 회사입니다. 이 회...</td>\n",
       "      <td>Tesla, Inc.는 미국의 다국적 자동차 및 청정 에너지 회사로, 전기 자동차(...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forbes Global 2000에서 테슬라 순위 뭐야?</td>\n",
       "      <td>['Tesla의 차량 생산은 2008년 Roadster로 시작하여 Model S (...</td>\n",
       "      <td>테슬라는 Forbes Global 2000에서 69위에 랭크되었습니다.</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tesla는 언제 누가 만들었나?</td>\n",
       "      <td>['Tesla는 내부 고발자 보복, 근로자 권리 침해, 안전 결함, 홍보 부족, M...</td>\n",
       "      <td>Tesla Motors, Inc.는 2003년 7월 1일에 Martin Eberha...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Larry Page는 전기차 시장에서 어떤 역할을 했나요?</td>\n",
       "      <td>[\"Elon Musk는 주류 차량으로 확장하기 전에 프리미엄 스포츠카로 시작하는 전...</td>\n",
       "      <td>Larry Page는 전기차 시장에서 Tesla의 후속 자금 조달에 투자한 기업가 ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toyota와 Tesla의 관계는 무엇인가요?</td>\n",
       "      <td>['Roadster 생산은 2008년에 시작되었습니다. 2009년 1월까지 Tesl...</td>\n",
       "      <td>2010년 5월, Tesla는 캘리포니아 주 프리몬트의 NUMMI 공장을 Toyot...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Tesla, Inc.는 미국에서 어떤 역할을 하고 있으며, 이 회사의 주요 제품과 ...   \n",
       "1                    Forbes Global 2000에서 테슬라 순위 뭐야?   \n",
       "2                                 Tesla는 언제 누가 만들었나?   \n",
       "3                   Larry Page는 전기차 시장에서 어떤 역할을 했나요?   \n",
       "4                          Toyota와 Tesla의 관계는 무엇인가요?   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  ['Tesla, Inc.는 미국의 다국적 자동차 및 청정 에너지 회사입니다. 이 회...   \n",
       "1  ['Tesla의 차량 생산은 2008년 Roadster로 시작하여 Model S (...   \n",
       "2  ['Tesla는 내부 고발자 보복, 근로자 권리 침해, 안전 결함, 홍보 부족, M...   \n",
       "3  [\"Elon Musk는 주류 차량으로 확장하기 전에 프리미엄 스포츠카로 시작하는 전...   \n",
       "4  ['Roadster 생산은 2008년에 시작되었습니다. 2009년 1월까지 Tesl...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Tesla, Inc.는 미국의 다국적 자동차 및 청정 에너지 회사로, 전기 자동차(...   \n",
       "1            테슬라는 Forbes Global 2000에서 69위에 랭크되었습니다.   \n",
       "2  Tesla Motors, Inc.는 2003년 7월 1일에 Martin Eberha...   \n",
       "3  Larry Page는 전기차 시장에서 Tesla의 후속 자금 조달에 투자한 기업가 ...   \n",
       "4  2010년 5월, Tesla는 캘리포니아 주 프리몬트의 NUMMI 공장을 Toyot...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "1  single_hop_specifc_query_synthesizer  \n",
       "2  single_hop_specifc_query_synthesizer  \n",
       "3  single_hop_specifc_query_synthesizer  \n",
       "4  single_hop_specifc_query_synthesizer  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "import pandas as pd\n",
    "testset = pd.read_excel('data/testset.xlsx')\n",
    "\n",
    "# 데이터 확인\n",
    "testset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = []\n",
    "\n",
    "# 각 행에 대해 RAG 체인을 호출하여 결과를 저장\n",
    "for row in testset.itertuples():\n",
    "    query = row.user_input   # 사용자 입력\n",
    "    reference = row.reference  # 참조 답변\n",
    "    relevant_docs = retriever.invoke(query)  # 검색된 문서\n",
    "    response = qa_chain.invoke(      # RAG 체인 생성 답변 생성\n",
    "        {\n",
    "            \"context\": format_docs(relevant_docs),\n",
    "            \"query\": query,\n",
    "        }\n",
    "    )  \n",
    "    \n",
    "    dataset.append(\n",
    "        {\n",
    "            \"user_input\": query,\n",
    "            \"retrieved_contexts\": [d.page_content for d in relevant_docs],\n",
    "            \"response\": response,\n",
    "            \"reference\": reference,\n",
    "        }\n",
    "    )\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tesla, Inc.는 미국에서 어떤 역할을 하고 있으며, 이 회사의 주요 제품과 ...</td>\n",
       "      <td>[Tesla, Inc.는 미국의 다국적 자동차 및 청정 에너지 회사입니다. 이 회사...</td>\n",
       "      <td>Tesla, Inc.는 미국의 다국적 자동차 및 청정 에너지 회사로서 전기 자동차(...</td>\n",
       "      <td>Tesla, Inc.는 미국의 다국적 자동차 및 청정 에너지 회사로, 전기 자동차(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forbes Global 2000에서 테슬라 순위 뭐야?</td>\n",
       "      <td>[Tesla의 차량 생산은 2008년 Roadster로 시작하여 Model S (2...</td>\n",
       "      <td>Forbes Global 2000에서 Tesla의 순위는 69위입니다.</td>\n",
       "      <td>테슬라는 Forbes Global 2000에서 69위에 랭크되었습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tesla는 언제 누가 만들었나?</td>\n",
       "      <td>[Tesla는 내부 고발자 보복, 근로자 권리 침해, 안전 결함, 홍보 부족, Mu...</td>\n",
       "      <td>Tesla는 2003년 7월 1일에 Martin Eberhard와 Marc Tarp...</td>\n",
       "      <td>Tesla Motors, Inc.는 2003년 7월 1일에 Martin Eberha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Larry Page는 전기차 시장에서 어떤 역할을 했나요?</td>\n",
       "      <td>[Elon Musk는 주류 차량으로 확장하기 전에 프리미엄 스포츠카로 시작하는 전략...</td>\n",
       "      <td>Larry Page는 전기차 시장에서 기업가 투자자로서 역할을 했습니다. 그는 Se...</td>\n",
       "      <td>Larry Page는 전기차 시장에서 Tesla의 후속 자금 조달에 투자한 기업가 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toyota와 Tesla의 관계는 무엇인가요?</td>\n",
       "      <td>[## 파트너\\n\\nTesla는 Panasonic과 파트너십을 맺고 있으며 리튬 공...</td>\n",
       "      <td>Toyota는 Tesla의 이전 파트너입니다.</td>\n",
       "      <td>2010년 5월, Tesla는 캘리포니아 주 프리몬트의 NUMMI 공장을 Toyot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Tesla, Inc.는 미국에서 어떤 역할을 하고 있으며, 이 회사의 주요 제품과 ...   \n",
       "1                    Forbes Global 2000에서 테슬라 순위 뭐야?   \n",
       "2                                 Tesla는 언제 누가 만들었나?   \n",
       "3                   Larry Page는 전기차 시장에서 어떤 역할을 했나요?   \n",
       "4                          Toyota와 Tesla의 관계는 무엇인가요?   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [Tesla, Inc.는 미국의 다국적 자동차 및 청정 에너지 회사입니다. 이 회사...   \n",
       "1  [Tesla의 차량 생산은 2008년 Roadster로 시작하여 Model S (2...   \n",
       "2  [Tesla는 내부 고발자 보복, 근로자 권리 침해, 안전 결함, 홍보 부족, Mu...   \n",
       "3  [Elon Musk는 주류 차량으로 확장하기 전에 프리미엄 스포츠카로 시작하는 전략...   \n",
       "4  [## 파트너\\n\\nTesla는 Panasonic과 파트너십을 맺고 있으며 리튬 공...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Tesla, Inc.는 미국의 다국적 자동차 및 청정 에너지 회사로서 전기 자동차(...   \n",
       "1            Forbes Global 2000에서 Tesla의 순위는 69위입니다.   \n",
       "2  Tesla는 2003년 7월 1일에 Martin Eberhard와 Marc Tarp...   \n",
       "3  Larry Page는 전기차 시장에서 기업가 투자자로서 역할을 했습니다. 그는 Se...   \n",
       "4                          Toyota는 Tesla의 이전 파트너입니다.   \n",
       "\n",
       "                                           reference  \n",
       "0  Tesla, Inc.는 미국의 다국적 자동차 및 청정 에너지 회사로, 전기 자동차(...  \n",
       "1            테슬라는 Forbes Global 2000에서 69위에 랭크되었습니다.  \n",
       "2  Tesla Motors, Inc.는 2003년 7월 1일에 Martin Eberha...  \n",
       "3  Larry Page는 전기차 시장에서 Tesla의 후속 자금 조달에 투자한 기업가 ...  \n",
       "4  2010년 5월, Tesla는 캘리포니아 주 프리몬트의 NUMMI 공장을 Toyot...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터프레임 변환하여 확인 \n",
    "evaluation_dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 저장\n",
    "evaluation_dataset.to_pandas().to_csv('data/evaluation_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) 평가 수행`\n",
    "\n",
    "- **LLMContextRecall**\n",
    "    - 검색된 컨텍스트가 정답을 생성하는 데 필요한 **모든 정보를 포함**하고 있는지 평가\n",
    "\n",
    "- **Faithfulness** \n",
    "    - 생성된 응답이 검색된 컨텍스트와 얼마나 **사실적으로 일관**되는지를 측정\n",
    "\n",
    "- **FactualCorrectness**\n",
    "    - 생성된 응답과 reference의 **사실적 정확**성을 비교하고 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  34%|███▍      | 50/147 [02:05<09:46,  6.05s/it]Exception raised in Job[32]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 200000, Used 199894, Requested 1071. Please try again in 289ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  35%|███▍      | 51/147 [02:05<06:56,  4.34s/it]Exception raised in Job[49]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 200000, Used 199556, Requested 1272. Please try again in 248ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  39%|███▉      | 57/147 [02:29<04:32,  3.03s/it]Exception raised in Job[46]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  41%|████▏     | 61/147 [02:47<06:04,  4.24s/it]Exception raised in Job[51]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 200000, Used 198832, Requested 1208. Please try again in 12ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  43%|████▎     | 63/147 [02:48<03:13,  2.30s/it]Exception raised in Job[43]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 200000, Used 198892, Requested 1695. Please try again in 176ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  47%|████▋     | 69/147 [03:13<03:02,  2.34s/it]Exception raised in Job[48]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 200000, Used 200000, Requested 1303. Please try again in 390ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  51%|█████     | 75/147 [04:00<08:46,  7.32s/it]Exception raised in Job[61]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 200000, Used 199972, Requested 1283. Please try again in 376ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  52%|█████▏    | 76/147 [04:00<06:24,  5.41s/it]Exception raised in Job[67]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 200000, Used 200000, Requested 1272. Please try again in 381ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  52%|█████▏    | 77/147 [04:09<07:28,  6.41s/it]Exception raised in Job[63]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  54%|█████▎    | 79/147 [04:15<05:10,  4.57s/it]Exception raised in Job[78]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 200000, Used 199859, Requested 1385. Please try again in 373ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  55%|█████▌    | 81/147 [04:23<04:58,  4.52s/it]Exception raised in Job[66]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 200000, Used 199466, Requested 1333. Please try again in 239ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  58%|█████▊    | 85/147 [04:36<03:58,  3.84s/it]Exception raised in Job[82]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  59%|█████▊    | 86/147 [04:48<06:13,  6.12s/it]Exception raised in Job[77]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  59%|█████▉    | 87/147 [05:01<03:28,  3.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m evaluator_llm = LangchainLLMWrapper(llm)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 평가\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m result = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# 평가 데이터셋\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mLLMContextRecall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFaithfulness\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFactualCorrectness\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# 평가 메트릭\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluator_llm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# LLM 래퍼\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\ragas\\_analytics.py:227\u001b[39m, in \u001b[36mtrack_was_completed.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> t.Any:\n\u001b[32m    226\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\ragas\\evaluation.py:294\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar)\u001b[39m\n\u001b[32m    291\u001b[39m scores: t.List[t.Dict[\u001b[38;5;28mstr\u001b[39m, t.Any]] = []\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# get the results\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     results = \u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m results == []:\n\u001b[32m    296\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\ragas\\executor.py:213\u001b[39m, in \u001b[36mExecutor.results\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m             nest_asyncio.apply()\n\u001b[32m    211\u001b[39m             \u001b[38;5;28mself\u001b[39m._nest_asyncio_applied = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m results = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m sorted_results = \u001b[38;5;28msorted\u001b[39m(results, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m0\u001b[39m])\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [r[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m sorted_results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\selectors.py:323\u001b[39m, in \u001b[36mSelectSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    321\u001b[39m ready = []\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     r, w, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\selectors.py:314\u001b[39m, in \u001b[36mSelectSelector._select\u001b[39m\u001b[34m(self, r, w, _, timeout)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     r, w, x = \u001b[43mselect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w + x, []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[103]: AssertionError(LLM is not set)\n",
      "Exception raised in Job[104]: AssertionError(LLM must be set)\n",
      "Exception raised in Job[105]: AssertionError(set LLM before use)\n",
      "Exception raised in Job[106]: AssertionError(LLM is not set)\n",
      "Exception raised in Job[107]: AssertionError(LLM must be set)\n",
      "Exception raised in Job[108]: AssertionError(set LLM before use)\n",
      "Exception raised in Job[109]: AssertionError(LLM is not set)\n",
      "Exception raised in Job[110]: AssertionError(LLM must be set)\n",
      "Exception raised in Job[111]: AssertionError(set LLM before use)\n",
      "Exception raised in Job[112]: AssertionError(LLM is not set)\n",
      "Exception raised in Job[113]: AssertionError(LLM must be set)\n",
      "Exception raised in Job[114]: AssertionError(set LLM before use)\n",
      "Exception raised in Job[115]: AssertionError(LLM is not set)\n",
      "Exception raised in Job[116]: AssertionError(LLM must be set)\n",
      "Exception raised in Job[117]: AssertionError(set LLM before use)\n",
      "Exception raised in Job[118]: AssertionError(LLM is not set)\n",
      "Exception raised in Job[119]: AssertionError(LLM must be set)\n",
      "Exception raised in Job[120]: AssertionError(set LLM before use)\n",
      "Exception raised in Job[121]: AssertionError(LLM is not set)\n",
      "Exception raised in Job[122]: AssertionError(LLM must be set)\n",
      "Exception raised in Job[123]: AssertionError(set LLM before use)\n",
      "Exception raised in Job[124]: AssertionError(LLM is not set)\n",
      "Exception raised in Job[125]: AssertionError(LLM must be set)\n",
      "Exception raised in Job[126]: AssertionError(set LLM before use)\n",
      "Exception raised in Job[127]: AssertionError(LLM is not set)\n",
      "Exception raised in Job[128]: AssertionError(LLM must be set)\n",
      "Exception raised in Job[129]: AssertionError(set LLM before use)\n",
      "Exception raised in Job[130]: AssertionError(LLM is not set)\n",
      "Exception raised in Job[131]: AssertionError(LLM must be set)\n",
      "Exception raised in Job[132]: AssertionError(set LLM before use)\n",
      "Exception raised in Job[133]: AssertionError(LLM is not set)\n",
      "Exception raised in Job[134]: AssertionError(LLM must be set)\n",
      "Exception raised in Job[135]: AssertionError(set LLM before use)\n",
      "Exception raised in Job[136]: AssertionError(LLM is not set)\n",
      "Exception raised in Job[137]: AssertionError(LLM must be set)\n",
      "Exception raised in Job[138]: AssertionError(set LLM before use)\n",
      "Exception raised in Job[139]: AssertionError(LLM is not set)\n",
      "Exception raised in Job[140]: AssertionError(LLM must be set)\n",
      "Exception raised in Job[141]: AssertionError(set LLM before use)\n",
      "Exception raised in Job[142]: AssertionError(LLM is not set)\n",
      "Exception raised in Job[143]: AssertionError(LLM must be set)\n",
      "Exception raised in Job[144]: AssertionError(set LLM before use)\n",
      "Exception raised in Job[145]: AssertionError(LLM is not set)\n",
      "Exception raised in Job[146]: AssertionError(LLM must be set)\n",
      "Exception raised in Job[76]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 200000, Used 199437, Requested 1478. Please try again in 274ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[68]: TimeoutError()\n",
      "Exception raised in Job[86]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 200000, Used 199764, Requested 1228. Please try again in 297ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[100]: AssertionError(llm must be set to compute score)\n",
      "Exception raised in Job[97]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on tokens per min (TPM): Limit 200000, Used 198921, Requested 1595. Please try again in 154ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[84]: TimeoutError()\n",
      "Exception raised in Job[93]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-mini in organization org-BUKxCvFx9eMpKEDf3d67YmYZ on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[98]: TimeoutError()\n",
      "Exception raised in Job[101]: AssertionError(LLM must be set)\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM 래퍼 생성\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "# 평가\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,   # 평가 데이터셋\n",
    "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],   # 평가 메트릭\n",
    "    llm=evaluator_llm,   # LLM 래퍼\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 데이터프레임으로 변환 \n",
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임 저장\n",
    "result.to_pandas().to_csv('data/evaluation_result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktds-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
