{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f191d1cf",
   "metadata": {},
   "source": [
    "#  RAG Ï≤¥Ïù∏ Íµ¨ÏÑ±\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641ec21",
   "metadata": {},
   "source": [
    "## RAGÎûÄ Î¨¥ÏóáÏù∏Í∞Ä?\n",
    "\n",
    "### üéØ ÌïµÏã¨ Í∞úÎÖê\n",
    "**Retrieval Augmented Generation (RAG)** Îäî ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏(LLM)Ïóê Ïô∏Î∂Ä ÏßÄÏãùÏùÑ Ïó∞Í≤∞ÌïòÏó¨ Îçî Ï†ïÌôïÌïòÍ≥† ÏµúÏã†Ïùò Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌïòÎäî AI ÌîÑÎ†àÏûÑÏõåÌÅ¨ÏûÖÎãàÎã§.\n",
    "\n",
    "### üîç RAGÏùò ÏûëÎèô ÏõêÎ¶¨\n",
    "```\n",
    "ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏ ‚Üí Í¥ÄÎ†® Î¨∏ÏÑú Í≤ÄÏÉâ ‚Üí Ïª®ÌÖçÏä§Ìä∏ÏôÄ Ìï®Íªò LLMÏóê Ï†ÑÎã¨ ‚Üí ÎãµÎ≥Ä ÏÉùÏÑ±\n",
    "```\n",
    "\n",
    "### üìä RAG vs ÏùºÎ∞ò LLM ÎπÑÍµê\n",
    "| Íµ¨Î∂Ñ | ÏùºÎ∞ò LLM | RAG |\n",
    "|------|----------|-----|\n",
    "| Ï†ïÎ≥¥ ÏÜåÏä§ | ÏÇ¨Ï†Ñ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Îßå | Ïô∏Î∂Ä ÏßÄÏãùÎ≤†Ïù¥Ïä§ + ÏÇ¨Ï†Ñ ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ |\n",
    "| ÏµúÏã†ÏÑ± | ÌõàÎ†® ÏãúÏ†êÍπåÏßÄ | Ïã§ÏãúÍ∞Ñ ÏóÖÎç∞Ïù¥Ìä∏ Í∞ÄÎä• |\n",
    "| Ï†ïÌôïÏÑ± | ÌôòÍ∞Å(hallucination) Í∞ÄÎä•ÏÑ± | Í≤ÄÏ¶ùÎêú Î¨∏ÏÑú Í∏∞Î∞ò ÎãµÎ≥Ä |\n",
    "| ÏÇ¨Ïö© ÏÇ¨Î°Ä | ÏùºÎ∞òÏ†ÅÏù∏ ÏßàÎ¨∏ ÎãµÎ≥Ä | ÌäπÏ†ï ÎèÑÎ©îÏù∏Ïùò Ï†ÑÎ¨∏Ï†Å ÎãµÎ≥Ä |\n",
    "\n",
    "---\n",
    "\n",
    "## ÌôòÍ≤Ω ÏÑ§Ï†ï\n",
    "\n",
    "### üõ†Ô∏è ÌïÑÏàò ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò\n",
    "\n",
    "```bash\n",
    "# Í∏∞Î≥∏ LangChain Ìå®ÌÇ§ÏßÄ\n",
    "pip install langchain langchain-community langchain-core\n",
    "\n",
    "# ÌÖçÏä§Ìä∏ Î∂ÑÌï†\n",
    "pip install langchain-text-splitters\n",
    "\n",
    "# ÏûÑÎ≤†Îî© Î™®Îç∏\n",
    "pip install langchain-openai langchain-huggingface\n",
    "\n",
    "# Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå\n",
    "pip install langchain-chroma\n",
    "\n",
    "# Î¨∏ÏÑú Ï≤òÎ¶¨\n",
    "pip install pypdf python-dotenv\n",
    "\n",
    "# Ïõπ Ïä§ÌÅ¨ÎûòÌïë\n",
    "pip install beautifulsoup4\n",
    "\n",
    "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä\n",
    "pip install tiktoken transformers sentence-transformers\n",
    "\n",
    "# Ïã§ÌóòÏ†Å Í∏∞Îä• (SemanticChunker)\n",
    "pip install langchain-experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be5eb8",
   "metadata": {},
   "source": [
    "### üîë ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829d8712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .env ÌååÏùº ÏÉùÏÑ±\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API ÌÇ§ ÏÑ§Ï†ï (ÌïÑÏöîÏãú)\n",
    "# OPENAI_API_KEY=your_openai_api_key_here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5989430",
   "metadata": {},
   "source": [
    "### üìã Í∏∞Î≥∏ ÎùºÏù¥Î∏åÎü¨Î¶¨ import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa02fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c23a89",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Î¨∏ÏÑú Î°úÎçî (Document Loaders)\n",
    "\n",
    "### üéØ Î¨∏ÏÑú Î°úÎçîÎûÄ?\n",
    "**Document Loader**Îäî Îã§ÏñëÌïú ÏÜåÏä§ÏóêÏÑú Î¨∏ÏÑúÎ•º Î°úÎìúÌïòÏó¨ LangChainÏùò `Document` Í∞ùÏ≤¥Î°ú Î≥ÄÌôòÌïòÎäî ÎèÑÍµ¨ÏûÖÎãàÎã§.\n",
    "\n",
    "### üìÑ Document Í∞ùÏ≤¥ Íµ¨Ï°∞\n",
    "```python\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Document Í∞ùÏ≤¥Ïùò Í∏∞Î≥∏ Íµ¨Ï°∞\n",
    "document = Document(\n",
    "    page_content=\"Î¨∏ÏÑúÏùò ÌÖçÏä§Ìä∏ ÎÇ¥Ïö©\",\n",
    "    metadata={\n",
    "        \"source\": \"Î¨∏ÏÑú Ï∂úÏ≤ò\",\n",
    "        \"page\": 1,\n",
    "        \"title\": \"Î¨∏ÏÑú Ï†úÎ™©\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### üìÑ Î¨∏ÏÑú Î°úÎçîÏùò Ï¢ÖÎ•ò\n",
    "- PDF ÌååÏùº Î°úÎçî\n",
    "- Ïõπ ÌéòÏù¥ÏßÄ Î°úÎçî \n",
    "- CSV Îç∞Ïù¥ÌÑ∞ Î°úÎçî\n",
    "- ÎîîÎ†âÌÜ†Î¶¨ Î°úÎçî\n",
    "- HTML Îç∞Ïù¥ÌÑ∞ Î°úÎçî\n",
    "- JSON Îç∞Ïù¥ÌÑ∞ Î°úÎçî\n",
    "- Markdown Îç∞Ïù¥ÌÑ∞ Î°úÎçî\n",
    "- Microsoft Office Îç∞Ïù¥ÌÑ∞ Î°úÎçî\n",
    "\n",
    "\n",
    "### 1. üåê Ïõπ Î¨∏ÏÑú Î°úÎçî (WebBaseLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c00ab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î°úÎìúÎêú Î¨∏ÏÑú Ïàò: 2\n",
      "Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# Í∏∞Î≥∏ Ïõπ Î¨∏ÏÑú Î°úÎìú\n",
    "web_loader = WebBaseLoader(\n",
    "    web_paths=[\n",
    "        \"https://python.langchain.com/docs/tutorials/rag/\",\n",
    "        \"https://js.langchain.com/docs/tutorials/rag/\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Î¨∏ÏÑú Î°úÎìú\n",
    "web_docs = web_loader.load()\n",
    "print(f\"Î°úÎìúÎêú Î¨∏ÏÑú Ïàò: {len(web_docs)}\")\n",
    "print(f\"Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {web_docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eebd4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Retrieval Augmented Generation (RAG) App: Part 1On this pageBuild a Retrieval Augmented Generation (RAG) App: Part 1\n",
      "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\n",
      "This is a multi-part tutorial:\n",
      "\n",
      "Part 1 (this guide) introduces RAG and walks through a minimal implementation.\n",
      "Part 2 extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\n",
      "\n",
      "This tutorial will show how to build a simple Q&A application\n",
      "over a text data source. Along the way we‚Äôll go over a typical Q&A\n",
      "architecture and highlight additional resources for more advanced Q&A techniques. We‚Äôll also see\n",
      "how LangSmith can help us trace and understand our application.\n",
      "LangSmith will become increasingly helpful as our application grows in\n",
      "complexity.\n",
      "If you're already familiar with basic retrieval, you might also be interested in\n",
      "this high-level overview of different retrieval techniques.\n",
      "Note: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing question/answering over SQL data.\n",
      "Overview‚Äã\n",
      "A typical RAG application has two main components:\n",
      "Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
      "Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
      "Note: the indexing portion of this tutorial will largely follow the semantic search tutorial.\n",
      "The most common full sequence from raw data to answer looks like:\n",
      "Indexing‚Äã\n",
      "\n",
      "Load: First we need to load our data. This is done with Document Loaders.\n",
      "Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
      "Store: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\n",
      "\n",
      "\n",
      "Retrieval and generation‚Äã\n",
      "\n",
      "Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
      "Generate: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data\n",
      "\n",
      "\n",
      "Once we've indexed our data, we will use LangGraph as our orchestration framework to implement the retrieval and generation steps.\n",
      "Setup‚Äã\n",
      "Jupyter Notebook‚Äã\n",
      "This and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\n",
      "Installation‚Äã\n",
      "This tutorial requires these langchain dependencies:\n",
      "\n",
      "PipConda%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraphconda install langchain-text-splitters langchain-community langgraph -c conda-forge\n",
      "For more details, see our Installation guide.\n",
      "LangSmith‚Äã\n",
      "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\n",
      "As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\n",
      "The best way to do this is with LangSmith.\n",
      "After you sign up at the link above, make sure to set your environment variables to start logging traces:\n",
      "export LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\n",
      "Or, if in a notebook, you can set them with:\n",
      "import getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
      "Components‚Äã\n",
      "We will need to select three components from LangChain's suite of integrations.\n",
      "\n",
      "Select chat model:Google Gemini‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexitypip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n",
      "\n",
      "Select embeddings model:OpenAI‚ñæOpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakepip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
      "\n",
      "Select vector store:In-memory‚ñæIn-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPGVectorStorePineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\n",
      "Preview‚Äã\n",
      "In this guide we‚Äôll build an app that answers questions about the website's content. The specific website we will use is the LLM Powered Autonomous\n",
      "Agents blog post\n",
      "by Lilian Weng, which allows us to ask questions about the contents of\n",
      "the post.\n",
      "We can create a simple indexing pipeline and RAG chain to do this in ~50\n",
      "lines of code.\n",
      "import bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.graph import START, StateGraphfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)# Index chunks_ = vector_store.add_documents(documents=all_splits)# Define prompt for question-answering# N.B. for non-US LangSmith endpoints, you may need to specify# api_url=\"https://api.smith.langchain.com\" in hub.pull.prompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict):    question: str    context: List[Document]    answer: str# Define application stepsdef retrieve(state: State):    retrieved_docs = vector_store.similarity_search(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}# Compile application and testgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()API Reference:Document | StateGraph\n",
      "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(response[\"answer\"])\n",
      "Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps to facilitate easier execution and understanding. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) guide models to think step-by-step, allowing them to explore multiple reasoning possibilities. This method enhances performance on complex tasks and provides insight into the model's thinking process.\n",
      "Check out the LangSmith\n",
      "trace.\n",
      "Detailed walkthrough‚Äã\n",
      "Let‚Äôs go through the above code step-by-step to really understand what‚Äôs\n",
      "going on.\n",
      "1. Indexing‚Äã\n",
      "noteThis section is an abbreviated version of the content in the semantic search tutorial.\n",
      "If you're comfortable with document loaders, embeddings, and vector stores,\n",
      "feel free to skip to the next section on retrieval and generation.\n",
      "Loading documents‚Äã\n",
      "We need to first load the blog post contents. We can use\n",
      "DocumentLoaders\n",
      "for this, which are objects that load in data from a source and return a\n",
      "list of\n",
      "Document\n",
      "objects.\n",
      "In this case we‚Äôll use the\n",
      "WebBaseLoader,\n",
      "which uses urllib to load HTML from web URLs and BeautifulSoup to\n",
      "parse it to text. We can customize the HTML -> text parsing by passing\n",
      "in parameters into the BeautifulSoup parser via bs_kwargs (see\n",
      "BeautifulSoup\n",
      "docs).\n",
      "In this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or\n",
      "‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\n",
      "import bs4from langchain_community.document_loaders import WebBaseLoader# Only keep post title, headers, and content from the full HTML.bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))loader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs={\"parse_only\": bs4_strainer},)docs = loader.load()assert len(docs) == 1print(f\"Total characters: {len(docs[0].page_content)}\")\n",
      "Total characters: 43131\n",
      "print(docs[0].page_content[:500])\n",
      "      LLM Powered Autonomous Agents    Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian WengBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.Agent System Overview#In\n",
      "Go deeper‚Äã\n",
      "DocumentLoader: Object that loads data from a source as list of Documents.\n",
      "\n",
      "Docs:\n",
      "Detailed documentation on how to use DocumentLoaders.\n",
      "Integrations: 160+\n",
      "integrations to choose from.\n",
      "Interface:\n",
      "API reference for the base interface.\n",
      "\n",
      "Splitting documents‚Äã\n",
      "Our loaded document is over 42k characters which is too long to fit\n",
      "into the context window of many models. Even for those models that could\n",
      "fit the full post in their context window, models can struggle to find\n",
      "information in very long inputs.\n",
      "To handle this we‚Äôll split the Document into chunks for embedding and\n",
      "vector storage. This should help us retrieve only the most relevant parts\n",
      "of the blog post at run time.\n",
      "As in the semantic search tutorial, we use a\n",
      "RecursiveCharacterTextSplitter,\n",
      "which will recursively split the document using common separators like\n",
      "new lines until each chunk is the appropriate size. This is the\n",
      "recommended text splitter for generic text use cases.\n",
      "from langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000,  # chunk size (characters)    chunk_overlap=200,  # chunk overlap (characters)    add_start_index=True,  # track index in original document)all_splits = text_splitter.split_documents(docs)print(f\"Split blog post into {len(all_splits)} sub-documents.\")\n",
      "Split blog post into 66 sub-documents.\n",
      "Go deeper‚Äã\n",
      "TextSplitter: Object that splits a list of Documents into smaller\n",
      "chunks. Subclass of DocumentTransformers.\n",
      "\n",
      "Learn more about splitting text using different methods by reading the how-to docs\n",
      "Code (py or js)\n",
      "Scientific papers\n",
      "Interface: API reference for the base interface.\n",
      "\n",
      "DocumentTransformer: Object that performs a transformation on a list\n",
      "of Document objects.\n",
      "\n",
      "Docs: Detailed documentation on how to use DocumentTransformers\n",
      "Integrations\n",
      "Interface: API reference for the base interface.\n",
      "\n",
      "Storing documents‚Äã\n",
      "Now we need to index our 66 text chunks so that we can search over them\n",
      "at runtime. Following the semantic search tutorial,\n",
      "our approach is to embed the contents of each document split and insert these embeddings\n",
      "into a vector store. Given an input query, we can then use\n",
      "vector search to retrieve relevant documents.\n",
      "We can embed and store all of our document splits in a single command\n",
      "using the vector store and embeddings model selected at the start of the tutorial.\n",
      "document_ids = vector_store.add_documents(documents=all_splits)print(document_ids[:3])\n",
      "['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']\n",
      "Go deeper‚Äã\n",
      "Embeddings: Wrapper around a text embedding model, used for converting\n",
      "text to embeddings.\n",
      "\n",
      "Docs: Detailed documentation on how to use embeddings.\n",
      "Integrations: 30+ integrations to choose from.\n",
      "Interface: API reference for the base interface.\n",
      "\n",
      "VectorStore: Wrapper around a vector database, used for storing and\n",
      "querying embeddings.\n",
      "\n",
      "Docs: Detailed documentation on how to use vector stores.\n",
      "Integrations: 40+ integrations to choose from.\n",
      "Interface: API reference for the base interface.\n",
      "\n",
      "This completes the Indexing portion of the pipeline. At this point\n",
      "we have a query-able vector store containing the chunked contents of our\n",
      "blog post. Given a user question, we should ideally be able to return\n",
      "the snippets of the blog post that answer the question.\n",
      "2. Retrieval and Generation‚Äã\n",
      "Now let‚Äôs write the actual application logic. We want to create a simple\n",
      "application that takes a user question, searches for documents relevant\n",
      "to that question, passes the retrieved documents and initial question to\n",
      "a model, and returns an answer.\n",
      "For generation, we will use the chat model selected at the start of the tutorial.\n",
      "We‚Äôll use a prompt for RAG that is checked into the LangChain prompt hub\n",
      "(here).\n",
      "from langchain import hub# N.B. for non-US LangSmith endpoints, you may need to specify# api_url=\"https://api.smith.langchain.com\" in hub.pull.prompt = hub.pull(\"rlm/rag-prompt\")example_messages = prompt.invoke(    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}).to_messages()assert len(example_messages) == 1print(example_messages[0].content)\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.Question: (question goes here) Context: (context goes here) Answer:\n",
      "We'll use LangGraph to tie together the retrieval and generation steps into a single application. This will bring a number of benefits:\n",
      "\n",
      "We can define our application logic once and automatically support multiple invocation modes, including streaming, async, and batched calls.\n",
      "We get streamlined deployments via LangGraph Platform.\n",
      "LangSmith will automatically trace the steps of our application together.\n",
      "We can easily add key features to our application, including persistence and human-in-the-loop approval, with minimal code changes.\n",
      "\n",
      "To use LangGraph, we need to define three things:\n",
      "\n",
      "The state of our application;\n",
      "The nodes of our application (i.e., application steps);\n",
      "The \"control flow\" of our application (e.g., the ordering of the steps).\n",
      "\n",
      "State:‚Äã\n",
      "The state of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a TypedDict, but can also be a Pydantic BaseModel.\n",
      "For a simple RAG application, we can just keep track of the input question, retrieved context, and generated answer:\n",
      "from langchain_core.documents import Documentfrom typing_extensions import List, TypedDictclass State(TypedDict):    question: str    context: List[Document]    answer: strAPI Reference:Document\n",
      "Nodes (application steps)‚Äã\n",
      "Let's start with a simple sequence of two steps: retrieval and generation.\n",
      "def retrieve(state: State):    retrieved_docs = vector_store.similarity_search(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}\n",
      "Our retrieval step simply runs a similarity search using the input question, and the generation step formats the retrieved context and original question into a prompt for the chat model.\n",
      "Control flow‚Äã\n",
      "Finally, we compile our application into a single graph object. In this case, we are just connecting the retrieval and generation steps into a single sequence.\n",
      "from langgraph.graph import START, StateGraphgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()API Reference:StateGraph\n",
      "LangGraph also comes with built-in utilities for visualizing the control flow of your application:\n",
      "from IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\n",
      "\n",
      "Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:question = \"...\"retrieved_docs = vector_store.similarity_search(question)docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)prompt = prompt.invoke({\"question\": question, \"context\": docs_content})answer = llm.invoke(prompt)The benefits of LangGraph include:\n",
      "Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n",
      "Automatic support for tracing via LangSmith and deployments via LangGraph Platform;\n",
      "Support for persistence, human-in-the-loop, and other features.\n",
      "Many use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in Part 2 of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n",
      "Usage‚Äã\n",
      "Let's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n",
      "Invoke:\n",
      "result = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(f\"Context: {result['context']}\\n\\n\")print(f\"Answer: {result['answer']}\")\n",
      "Context: [Document(id='a42dc78b-8f76-472a-9e25-180508af74f3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.'), Document(id='c0e45887-d0b0-483d-821a-bb5d8316d51d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='4cc7f318-35f5-440f-a4a4-145b5f0b918d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='f621ade4-9b0d-471f-a522-44eb5feeba0c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]Answer: Task decomposition is a technique used to break down complex tasks into smaller, manageable steps, allowing for more efficient problem-solving. This can be achieved through methods like chain of thought prompting or the tree of thoughts approach, which explores multiple reasoning possibilities at each step. It can be initiated through simple prompts, task-specific instructions, or human inputs.\n",
      "Stream steps:\n",
      "for step in graph.stream(    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"):    print(f\"{step}\\n\\n----------------\\n\")\n",
      "{'retrieve': {'context': [Document(id='a42dc78b-8f76-472a-9e25-180508af74f3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.'), Document(id='c0e45887-d0b0-483d-821a-bb5d8316d51d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='4cc7f318-35f5-440f-a4a4-145b5f0b918d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='f621ade4-9b0d-471f-a522-44eb5feeba0c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]}}----------------{'generate': {'answer': 'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This technique, often enhanced by methods like Chain of Thought (CoT) or Tree of Thoughts, allows models to reason through tasks systematically and improves performance by clarifying the thought process. It can be achieved through simple prompts, task-specific instructions, or human inputs.'}}----------------\n",
      "Stream tokens:\n",
      "for message, metadata in graph.stream(    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"messages\"):    print(message.content, end=\"|\")\n",
      "|Task| decomposition| is| the| process| of| breaking| down| complex| tasks| into| smaller|,| more| manageable| steps|.| It| can| be| achieved| through| techniques| like| Chain| of| Thought| (|Co|T|)| prompting|,| which| encourages| the| model| to| think| step| by| step|,| or| through| more| structured| methods| like| the| Tree| of| Thoughts|.| This| approach| not| only| simplifies| task| execution| but| also| provides| insights| into| the| model|'s| reasoning| process|.||\n",
      "tipFor async invocations, use:result = await graph.ainvoke(...)andasync for step in graph.astream(...):\n",
      "Returning sources‚Äã\n",
      "Note that by storing the retrieved context in the state of the graph, we recover sources for the model's generated answer in the \"context\" field of the state. See this guide on returning sources for more detail.\n",
      "Go deeper‚Äã\n",
      "Chat models take in a sequence of messages and return a message.\n",
      "\n",
      "Docs\n",
      "Integrations: 25+ integrations to choose from.\n",
      "Interface: API reference for the base interface.\n",
      "\n",
      "Customizing the prompt\n",
      "As shown above, we can load prompts (e.g., this RAG\n",
      "prompt) from the prompt\n",
      "hub. The prompt can also be easily customized. For example:\n",
      "from langchain_core.prompts import PromptTemplatetemplate = \"\"\"Use the following pieces of context to answer the question at the end.If you don't know the answer, just say that you don't know, don't try to make up an answer.Use three sentences maximum and keep the answer as concise as possible.Always say \"thanks for asking!\" at the end of the answer.{context}Question: {question}Helpful Answer:\"\"\"custom_rag_prompt = PromptTemplate.from_template(template)API Reference:PromptTemplate\n",
      "Query analysis‚Äã\n",
      "So far, we are executing the retrieval using the raw input query. However, there are some advantages to allowing a model to generate the query for retrieval purposes. For example:\n",
      "\n",
      "In addition to semantic search, we can build in structured filters (e.g., \"Find documents since the year 2020.\");\n",
      "The model can rewrite user queries, which may be multifaceted or include irrelevant language, into more effective search queries.\n",
      "\n",
      "Query analysis employs models to transform or construct optimized search queries from raw user input. We can easily incorporate a query analysis step into our application. For illustrative purposes, let's add some metadata to the documents in our vector store. We will add some (contrived) sections to the document which we can filter on later.\n",
      "total_documents = len(all_splits)third = total_documents // 3for i, document in enumerate(all_splits):    if i < third:        document.metadata[\"section\"] = \"beginning\"    elif i < 2 * third:        document.metadata[\"section\"] = \"middle\"    else:        document.metadata[\"section\"] = \"end\"all_splits[0].metadata\n",
      "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 8, 'section': 'beginning'}\n",
      "We will need to update the documents in our vector store. We will use a simple InMemoryVectorStore for this, as we will use some of its specific features (i.e., metadata filtering). Refer to the vector store integration documentation for relevant features of your chosen vector store.\n",
      "from langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)_ = vector_store.add_documents(all_splits)API Reference:InMemoryVectorStore\n",
      "Let's next define a schema for our search query. We will use structured output for this purpose. Here we define a query as containing a string query and a document section (either \"beginning\", \"middle\", or \"end\"), but this can be defined however you like.\n",
      "from typing import Literalfrom typing_extensions import Annotatedclass Search(TypedDict):    \"\"\"Search query.\"\"\"    query: Annotated[str, ..., \"Search query to run.\"]    section: Annotated[        Literal[\"beginning\", \"middle\", \"end\"],        ...,        \"Section to query.\",    ]\n",
      "Finally, we add a step to our LangGraph application to generate a query from the user's raw input:\n",
      "class State(TypedDict):    question: str    query: Search    context: List[Document]    answer: strdef analyze_query(state: State):    structured_llm = llm.with_structured_output(Search)    query = structured_llm.invoke(state[\"question\"])    return {\"query\": query}def retrieve(state: State):    query = state[\"query\"]    retrieved_docs = vector_store.similarity_search(        query[\"query\"],        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],    )    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])graph_builder.add_edge(START, \"analyze_query\")graph = graph_builder.compile()\n",
      "Full Code:from typing import Literalimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.graph import START, StateGraphfrom typing_extensions import Annotated, List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)# Update metadata (illustration purposes)total_documents = len(all_splits)third = total_documents // 3for i, document in enumerate(all_splits):    if i < third:        document.metadata[\"section\"] = \"beginning\"    elif i < 2 * third:        document.metadata[\"section\"] = \"middle\"    else:        document.metadata[\"section\"] = \"end\"# Index chunksvector_store = InMemoryVectorStore(embeddings)_ = vector_store.add_documents(all_splits)# Define schema for searchclass Search(TypedDict):    \"\"\"Search query.\"\"\"    query: Annotated[str, ..., \"Search query to run.\"]    section: Annotated[        Literal[\"beginning\", \"middle\", \"end\"],        ...,        \"Section to query.\",    ]# Define prompt for question-answeringprompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict):    question: str    query: Search    context: List[Document]    answer: strdef analyze_query(state: State):    structured_llm = llm.with_structured_output(Search)    query = structured_llm.invoke(state[\"question\"])    return {\"query\": query}def retrieve(state: State):    query = state[\"query\"]    retrieved_docs = vector_store.similarity_search(        query[\"query\"],        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],    )    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])graph_builder.add_edge(START, \"analyze_query\")graph = graph_builder.compile()API Reference:Document | InMemoryVectorStore | StateGraph\n",
      "display(Image(graph.get_graph().draw_mermaid_png()))\n",
      "\n",
      "We can test our implementation by specifically asking for context from the end of the post. Note that the model includes different information in its answer.\n",
      "for step in graph.stream(    {\"question\": \"What does the end of the post say about Task Decomposition?\"},    stream_mode=\"updates\",):    print(f\"{step}\\n\\n----------------\\n\")\n",
      "{'analyze_query': {'query': {'query': 'Task Decomposition', 'section': 'end'}}}----------------{'retrieve': {'context': [Document(id='d6cef137-e1e8-4ddc-91dc-b62bd33c6020', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39221, 'section': 'end'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.'), Document(id='d1834ae1-eb6a-43d7-a023-08dfa5028799', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39086, 'section': 'end'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'), Document(id='ca7f06e4-2c2e-4788-9a81-2418d82213d9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 32942, 'section': 'end'}, page_content='}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:'), Document(id='1fcc2736-30f4-4ef6-90f2-c64af92118cb', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 35127, 'section': 'end'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease')]}}----------------{'generate': {'answer': 'The end of the post highlights that task decomposition faces challenges in long-term planning and adapting to unexpected errors. LLMs struggle with adjusting their plans, making them less robust compared to humans who learn from trial and error. This indicates a limitation in effectively exploring the solution space and handling complex tasks.'}}----------------\n",
      "In both the streamed steps and the LangSmith trace, we can now observe the structured query that was fed into the retrieval step.\n",
      "Query Analysis is a rich problem with a wide range of approaches. Refer to the how-to guides for more examples.\n",
      "Next steps‚Äã\n",
      "We've covered the steps to build a basic Q&A app over data:\n",
      "\n",
      "Loading data with a Document Loader\n",
      "Chunking the indexed data with a Text Splitter to make it more easily usable by a model\n",
      "Embedding the data and storing the data in a vectorstore\n",
      "Retrieving the previously stored chunks in response to incoming questions\n",
      "Generating an answer using the retrieved chunks as context.\n",
      "\n",
      "In Part 2 of the tutorial, we will extend the implementation here to accommodate conversation-style interactions and multi-step retrieval processes.\n",
      "Further reading:\n",
      "\n",
      "Return sources: Learn how to return source documents\n",
      "Streaming: Learn how to stream outputs and intermediate steps\n",
      "Add chat history: Learn how to add chat history to your app\n",
      "Retrieval conceptual guide: A high-level overview of specific retrieval techniques\n",
      "Edit this pagePreviousTaggingNextBuild a semantic search engineOverviewIndexingRetrieval and generationSetupJupyter NotebookInstallationLangSmithComponentsPreviewDetailed walkthrough1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationQuery analysisNext stepsCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(web_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "190061b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î°úÎìúÎêú Î¨∏ÏÑú Ïàò: 1\n",
      "Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {'source': 'https://python.langchain.com/docs/tutorials/rag/'}\n"
     ]
    }
   ],
   "source": [
    "# Beautiful Soup ÌååÏÑú ÏòµÏÖò ÏÑ§Ï†ï\n",
    "web_loader_advanced = WebBaseLoader(\n",
    "    web_paths=[\"https://python.langchain.com/docs/tutorials/rag/\"],\n",
    "    bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# Î¨∏ÏÑú Î°úÎìú\n",
    "web_docs_advanced = web_loader_advanced.load()\n",
    "print(f\"Î°úÎìúÎêú Î¨∏ÏÑú Ïàò: {len(web_docs_advanced)}\")\n",
    "print(f\"Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {web_docs_advanced[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17883f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î°úÎìúÎêú Î¨∏ÏÑú Ïàò: 1\n",
      "Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {'source': 'https://example.com', 'title': 'Example Domain', 'language': 'No language found.'}\n"
     ]
    }
   ],
   "source": [
    "# Ìó§Îçî ÏÑ§Ï†ï (Ïòà: User-Agent)\n",
    "web_loader_with_headers = WebBaseLoader(\n",
    "    web_paths=[\"https://example.com\"],\n",
    "    header_template={\n",
    "        \"User-Agent\": \"Mozilla/5.0 (compatible; LangChain)\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Î¨∏ÏÑú Î°úÎìú\n",
    "web_docs_with_headers = web_loader_with_headers.load()\n",
    "print(f\"Î°úÎìúÎêú Î¨∏ÏÑú Ïàò: {len(web_docs_with_headers)}\")\n",
    "print(f\"Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {web_docs_with_headers[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e239cbc0",
   "metadata": {},
   "source": [
    "### 2. üìä CSV ÌååÏùº Î°úÎçî (CSVLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43ee6c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î¨∏ÏÑú Ïàò: 10\n",
      "Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú:\n",
      "Team: KIA ÌÉÄÏù¥Í±∞Ï¶à\n",
      "City: Í¥ëÏ£º\n",
      "Founded: 1982\n",
      "Home Stadium: Í¥ëÏ£º-Í∏∞ÏïÑ Ï±îÌîºÏñ∏Ïä§ ÌïÑÎìú\n",
      "Championships: 11\n",
      "Introduction: KBO Î¶¨Í∑∏Ïùò Ï†ÑÌÜµ Í∞ïÌò∏Î°ú, Ïó≠ÎåÄ ÏµúÎã§ Ïö∞Ïäπ Í∏∞Î°ùÏùÑ Î≥¥Ïú†ÌïòÍ≥† ÏûàÎã§. 'ÌÉÄÏù¥Í±∞Ï¶à Ïä§ÌîºÎ¶ø'ÏúºÎ°ú Ïú†Î™ÖÌïòÎ©∞, ÏñëÌòÑÏ¢Ö, ÏïàÏπòÌôç Îì± Ïä§ÌÉÄ ÏÑ†ÏàòÎì§ÏùÑ Î∞∞Ï∂úÌñàÎã§. Í¥ëÏ£ºÎ•º Ïó∞Í≥†Î°ú ÌïòÎäî Ïú†ÏùºÌïú ÌîÑÎ°úÏïºÍµ¨ÌåÄÏúºÎ°ú ÏßÄÏó≠ ÏÇ¨ÎûëÏù¥ Í∞ïÌïòÎã§.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# Í∏∞Î≥∏ CSV Î°úÎìú\n",
    "csv_loader = CSVLoader(\"./data/kbo_teams_2023.csv\", encoding=\"utf-8\")\n",
    "csv_docs = csv_loader.load()\n",
    "\n",
    "print(f\"Î¨∏ÏÑú Ïàò: {len(csv_docs)}\")\n",
    "print(f\"Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú:\\n{csv_docs[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb0731b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './data/kbo_teams_2023.csv', 'row': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68d1fa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î¨∏ÏÑú Ïàò: 10\n",
      "Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {'source': 'KIA ÌÉÄÏù¥Í±∞Ï¶à', 'row': 0, 'Founded': '1982'}\n",
      "Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú ÎÇ¥Ïö©:\n",
      "Introduction: KBO Î¶¨Í∑∏Ïùò Ï†ÑÌÜµ Í∞ïÌò∏Î°ú, Ïó≠ÎåÄ ÏµúÎã§ Ïö∞Ïäπ Í∏∞Î°ùÏùÑ Î≥¥Ïú†ÌïòÍ≥† ÏûàÎã§. 'ÌÉÄÏù¥Í±∞Ï¶à Ïä§ÌîºÎ¶ø'ÏúºÎ°ú Ïú†Î™ÖÌïòÎ©∞, ÏñëÌòÑÏ¢Ö, ÏïàÏπòÌôç Îì± Ïä§ÌÉÄ ÏÑ†ÏàòÎì§ÏùÑ Î∞∞Ï∂úÌñàÎã§. Í¥ëÏ£ºÎ•º Ïó∞Í≥†Î°ú ÌïòÎäî Ïú†ÏùºÌïú ÌîÑÎ°úÏïºÍµ¨ÌåÄÏúºÎ°ú ÏßÄÏó≠ ÏÇ¨ÎûëÏù¥ Í∞ïÌïòÎã§.\n"
     ]
    }
   ],
   "source": [
    "# ÏÜåÏä§ Ïª¨Îüº ÏßÄÏ†ï Î∞è Ïù∏ÏΩîÎî© ÏÑ§Ï†ï\n",
    "csv_loader_advanced = CSVLoader(\n",
    "    file_path=\"./data/kbo_teams_2023.csv\",\n",
    "    source_column=\"Team\",      # Ïù¥ Ïª¨ÎüºÏù¥ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Ïùò sourceÍ∞Ä Îê®\n",
    "    content_columns=[\"Introduction\"],  # Ïù¥ Ïª¨ÎüºÏù¥ Î¨∏ÏÑúÏùò ÎÇ¥Ïö©Ïù¥ Îê®\n",
    "    metadata_columns=[\"Founded\"],  # Ïù¥ Ïª¨ÎüºÏù¥ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Ïóê Ï∂îÍ∞ÄÎê®\n",
    "    encoding=\"utf-8\",          # Ïù∏ÏΩîÎî© Î™ÖÏãú\n",
    "    csv_args={\n",
    "        \"delimiter\": \",\",      # Íµ¨Î∂ÑÏûê\n",
    "        \"quotechar\": '\"',      # Ïù∏Ïö© Î¨∏Ïûê\n",
    "    }\n",
    ")\n",
    "\n",
    "csv_docs_advanced = csv_loader_advanced.load()\n",
    "\n",
    "# Î¨∏ÏÑú ÏàòÏôÄ Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú ÎÇ¥Ïö© Ï∂úÎ†•\n",
    "print(f\"Î¨∏ÏÑú Ïàò: {len(csv_docs_advanced)}\")\n",
    "print(f\"Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {csv_docs_advanced[0].metadata}\")\n",
    "print(f\"Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú ÎÇ¥Ïö©:\\n{csv_docs_advanced[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e9558",
   "metadata": {},
   "source": [
    "### 3. üìñ PDF ÌååÏùº Î°úÎçî \n",
    "\n",
    "- **PyPDFLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d82df94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Î¨∏ÏÑú Í∞úÏàò: 20\n",
      "ÌéòÏù¥ÏßÄ 1: 1811 Î¨∏Ïûê\n",
      "Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {'producer': 'iText 2.1.7 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2024-10-15T14:45:34+09:00', 'moddate': '2024-10-15T14:45:34+09:00', 'source': './data/labor_law.pdf', 'total_pages': 20, 'page': 0, 'page_label': '1'}\n",
      "ÌéòÏù¥ÏßÄ 2: 1709 Î¨∏Ïûê\n",
      "Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {'producer': 'iText 2.1.7 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2024-10-15T14:45:34+09:00', 'moddate': '2024-10-15T14:45:34+09:00', 'source': './data/labor_law.pdf', 'total_pages': 20, 'page': 1, 'page_label': '2'}\n",
      "ÌéòÏù¥ÏßÄ 3: 2164 Î¨∏Ïûê\n",
      "Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {'producer': 'iText 2.1.7 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2024-10-15T14:45:34+09:00', 'moddate': '2024-10-15T14:45:34+09:00', 'source': './data/labor_law.pdf', 'total_pages': 20, 'page': 2, 'page_label': '3'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF Î°úÎçî Ï¥àÍ∏∞Ìôî\n",
    "pdf_loader = PyPDFLoader('./data/labor_law.pdf')\n",
    "\n",
    "# ÎèôÍ∏∞ Î°úÎî©\n",
    "pdf_docs = pdf_loader.load()\n",
    "print(f'PDF Î¨∏ÏÑú Í∞úÏàò: {len(pdf_docs)}')\n",
    "\n",
    "# Í∞Å ÌéòÏù¥ÏßÄÎ≥Ñ Ï†ïÎ≥¥ ÌôïÏù∏\n",
    "for i, doc in enumerate(pdf_docs[:3]):\n",
    "    print(f\"ÌéòÏù¥ÏßÄ {i+1}: {len(doc.page_content)} Î¨∏Ïûê\")\n",
    "    print(f\"Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e68eaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î≤ïÏ†úÏ≤ò                                                            1                                                       Íµ≠Í∞ÄÎ≤ïÎ†πÏ†ïÎ≥¥ÏÑºÌÑ∞\n",
      "Í∑ºÎ°úÍ∏∞Ï§ÄÎ≤ï\n",
      " \n",
      "Í∑ºÎ°úÍ∏∞Ï§ÄÎ≤ï\n",
      "[ÏãúÌñâ 2021. 11. 19.] [Î≤ïÎ•† Ï†ú18176Ìò∏, 2021. 5. 18., ÏùºÎ∂ÄÍ∞úÏ†ï]\n",
      "Í≥†Ïö©ÎÖ∏ÎèôÎ∂Ä (Í∑ºÎ°úÍ∏∞Ï§ÄÏ†ïÏ±ÖÍ≥º - Ìï¥Í≥†, Ï∑®ÏóÖÍ∑úÏπô, Í∏∞ÌÉÄ) 044-202-7534\n",
      "Í≥†Ïö©ÎÖ∏ÎèôÎ∂Ä (Í∑ºÎ°úÍ∏∞Ï§ÄÏ†ïÏ±ÖÍ≥º - ÏÜåÎÖÑ) 044-202-7535\n",
      "Í≥†Ïö©ÎÖ∏ÎèôÎ∂Ä (Í∑ºÎ°úÍ∏∞Ï§ÄÏ†ïÏ±ÖÍ≥º - ÏûÑÍ∏à) 044-202-7548\n",
      "Í≥†Ïö©ÎÖ∏ÎèôÎ∂Ä (Ïó¨ÏÑ±Í≥†Ïö©Ï†ïÏ±ÖÍ≥º - Ïó¨ÏÑ±) 044-202-7475\n",
      "Í≥†Ïö©ÎÖ∏ÎèôÎ∂Ä (ÏûÑÍ∏àÍ∑ºÎ°úÏãúÍ∞ÑÏ†ïÏ±ÖÍ≥º - Í∑ºÎ°úÏãúÍ∞Ñ, Ìú¥Í≤å) 044-202-7545\n",
      "Í≥†Ïö©ÎÖ∏ÎèôÎ∂Ä (ÏûÑÍ∏àÍ∑ºÎ°úÏãúÍ∞ÑÏ†ïÏ±ÖÍ≥º - Ìú¥Ïùº, Ïó∞Ï∞®Ìú¥Í∞Ä) 044-202-7973\n",
      "Í≥†Ïö©ÎÖ∏ÎèôÎ∂Ä (ÏûÑÍ∏àÍ∑ºÎ°úÏãúÍ∞ÑÏ†ïÏ±ÖÍ≥º - Ï†ú63Ï°∞ Ï†ÅÏö©Ï†úÏô∏, ÌäπÎ°ÄÏóÖÏ¢Ö) 044-202-7530\n",
      "Í≥†Ïö©ÎÖ∏ÎèôÎ∂Ä (ÏûÑÍ∏àÍ∑ºÎ°úÏãúÍ∞ÑÏ†ïÏ±ÖÍ≥º - Ïú†Ïó∞Í∑ºÎ°úÏãúÍ∞ÑÏ†ú) 044-202-7549\n",
      "       Ï†ú1Ïû• Ï¥ùÏπô\n",
      " \n",
      "Ï†ú1Ï°∞(Î™©Ï†Å) Ïù¥ Î≤ïÏùÄ ÌóåÎ≤ïÏóê Îî∞Îùº Í∑ºÎ°úÏ°∞Í±¥Ïùò Í∏∞Ï§ÄÏùÑ Ï†ïÌï®ÏúºÎ°úÏç® Í∑ºÎ°úÏûêÏùò Í∏∞Î≥∏Ï†Å ÏÉùÌôúÏùÑ Î≥¥Ïû•, Ìñ•ÏÉÅÏãúÌÇ§Î©∞ Í∑†Ìòï ÏûàÎäî\n",
      "Íµ≠ÎØºÍ≤ΩÏ†úÏùò Î∞úÏ†ÑÏùÑ ÍæÄÌïòÎäî Í≤ÉÏùÑ Î™©Ï†ÅÏúºÎ°ú ÌïúÎã§.\n",
      " \n",
      "Ï†ú2Ï°∞(Ï†ïÏùò) ‚ë† Ïù¥ Î≤ïÏóêÏÑú ÏÇ¨Ïö©ÌïòÎäî Ïö©Ïñ¥Ïùò ÎúªÏùÄ Îã§ÏùåÍ≥º Í∞ôÎã§. <Í∞úÏ†ï 2018. 3. 20., 2019. 1. 15., 2020. 5. 26.>\n",
      "1. ‚ÄúÍ∑ºÎ°úÏûê‚ÄùÎûÄ ÏßÅÏóÖÏùò Ï¢ÖÎ•òÏôÄ Í¥ÄÍ≥ÑÏóÜÏù¥ ÏûÑÍ∏àÏùÑ Î™©Ï†ÅÏúºÎ°ú ÏÇ¨ÏóÖÏù¥ÎÇò ÏÇ¨ÏóÖÏû•Ïóê Í∑ºÎ°úÎ•º Ï†úÍ≥µÌïòÎäî ÏÇ¨ÎûåÏùÑ ÎßêÌïúÎã§.\n",
      "2. ‚ÄúÏÇ¨Ïö©Ïûê‚ÄùÎûÄ ÏÇ¨ÏóÖÏ£º ÎòêÎäî ÏÇ¨ÏóÖ Í≤ΩÏòÅ Îã¥ÎãπÏûê, Í∑∏ Î∞ñÏóê Í∑ºÎ°úÏûêÏóê Í¥ÄÌïú ÏÇ¨Ìï≠Ïóê ÎåÄÌïòÏó¨ ÏÇ¨ÏóÖÏ£ºÎ•º ÏúÑÌïòÏó¨ ÌñâÏúÑÌïòÎäî ÏûêÎ•º\n",
      "ÎßêÌïúÎã§.\n",
      "3. ‚ÄúÍ∑ºÎ°ú‚ÄùÎûÄ Ï†ïÏã†ÎÖ∏ÎèôÍ≥º Ïú°Ï≤¥ÎÖ∏ÎèôÏùÑ ÎßêÌïúÎã§.\n",
      "4. ‚ÄúÍ∑ºÎ°úÍ≥ÑÏïΩ‚ÄùÏù¥ÎûÄ Í∑ºÎ°úÏûêÍ∞Ä ÏÇ¨Ïö©ÏûêÏóêÍ≤å Í∑ºÎ°úÎ•º Ï†úÍ≥µÌïòÍ≥† ÏÇ¨Ïö©ÏûêÎäî Ïù¥Ïóê ÎåÄÌïòÏó¨ ÏûÑÍ∏àÏùÑ ÏßÄÍ∏âÌïòÎäî Í≤ÉÏùÑ Î™©Ï†ÅÏúºÎ°ú Ï≤¥\n",
      "Í≤∞Îêú Í≥ÑÏïΩÏùÑ ÎßêÌïúÎã§.\n",
      "5. ‚ÄúÏûÑÍ∏à‚ÄùÏù¥ÎûÄ ÏÇ¨Ïö©ÏûêÍ∞Ä Í∑ºÎ°úÏùò ÎåÄÍ∞ÄÎ°ú Í∑ºÎ°úÏûêÏóêÍ≤å ÏûÑÍ∏à, Î¥âÍ∏â, Í∑∏ Î∞ñÏóê Ïñ¥Îñ†Ìïú Î™ÖÏπ≠ÏúºÎ°úÎì†ÏßÄ ÏßÄÍ∏âÌïòÎäî Î™®Îì† Í∏àÌíà\n"
     ]
    }
   ],
   "source": [
    "print(pdf_docs[0].page_content[:1000])  # Ï≤´ ÌéòÏù¥ÏßÄÏùò ÎÇ¥Ïö© ÏùºÎ∂Ä Ï∂úÎ†•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c964d1",
   "metadata": {},
   "source": [
    "- **Îã§Î•∏ PDF Î°úÎçîÎì§**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a60f3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "    UnstructuredPDFLoader,\n",
    "    PyMuPDFLoader,\n",
    "    PDFMinerLoader\n",
    ")\n",
    "\n",
    "# Unstructured PDF Î°úÎçî (Ïù¥ÎØ∏ÏßÄ, ÌÖåÏù¥Î∏î Ï≤òÎ¶¨ Í∞ÄÎä•) : Î≥ÑÎèÑ ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò ÌïÑÏöî\n",
    "# unstructured_loader = UnstructuredPDFLoader(\"./data/labor_law.pdf\")\n",
    "\n",
    "# PyMuPDF Î°úÎçî (Îπ†Î•∏ Ï≤òÎ¶¨)\n",
    "pymupdf_loader = PyMuPDFLoader(\"./data/labor_law.pdf\")\n",
    "\n",
    "# PDFMiner Î°úÎçî (Ï†ïÌôïÌïú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú)\n",
    "pdfminer_loader = PDFMinerLoader(\"./data/labor_law.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28116083",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "pymupdf package not found, please install it with `pip install pymupdf`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:954\u001b[39m, in \u001b[36mPyMuPDFParser._lazy_parse\u001b[39m\u001b[34m(self, blob, text_kwargs)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpymupdf\u001b[39;00m\n\u001b[32m    956\u001b[39m     text_kwargs = text_kwargs \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_kwargs\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pymupdf'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# pymupdf_loader = PyMuPDFLoader(\"./data/labor_law.pdf\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m pdf_docs = \u001b[43mpymupdf_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mPDF Î¨∏ÏÑú Í∞úÏàò: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pdf_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Í∞Å ÌéòÏù¥ÏßÄÎ≥Ñ Ï†ïÎ≥¥ ÌôïÏù∏\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:853\u001b[39m, in \u001b[36mPyMuPDFLoader.load\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    852\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m--> \u001b[39m\u001b[32m853\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:850\u001b[39m, in \u001b[36mPyMuPDFLoader._lazy_load\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    849\u001b[39m     blob = Blob.from_path(\u001b[38;5;28mself\u001b[39m.file_path)\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m parser._lazy_parse(blob, text_kwargs=kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:991\u001b[39m, in \u001b[36mPyMuPDFParser._lazy_parse\u001b[39m\u001b[34m(self, blob, text_kwargs)\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[38;5;28mself\u001b[39m.extract_tables_settings = {\n\u001b[32m    966\u001b[39m             \u001b[38;5;66;03m# See https://pymupdf.readthedocs.io/en/latest/page.html#Page.find_tables\u001b[39;00m\n\u001b[32m    967\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mclip\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    988\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33madd_lines\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# optional user-specified lines\u001b[39;00m\n\u001b[32m    989\u001b[39m         }\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpymupdf package not found, please install it \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwith `pip install pymupdf`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    994\u001b[39m     )\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m PyMuPDFParser._lock:\n\u001b[32m    997\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m blob.as_bytes_io() \u001b[38;5;28;01mas\u001b[39;00m file_path:\n",
      "\u001b[31mImportError\u001b[39m: pymupdf package not found, please install it with `pip install pymupdf`"
     ]
    }
   ],
   "source": [
    "# pymupdf_loader = PyMuPDFLoader(\"./data/labor_law.pdf\")\n",
    "pdf_docs = pymupdf_loader.load()\n",
    "print(f'PDF Î¨∏ÏÑú Í∞úÏàò: {len(pdf_docs)}')\n",
    "\n",
    "# Í∞Å ÌéòÏù¥ÏßÄÎ≥Ñ Ï†ïÎ≥¥ ÌôïÏù∏\n",
    "for i, doc in enumerate(pdf_docs[:3]):\n",
    "    print(f\"ÌéòÏù¥ÏßÄ {i+1}: {len(doc.page_content)} Î¨∏Ïûê\")\n",
    "    print(f\"Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d660549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdf_docs[0].page_content[:1000])  # Ï≤´ ÌéòÏù¥ÏßÄÏùò ÎÇ¥Ïö© ÏùºÎ∂Ä Ï∂úÎ†•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdfminer_loader = PDFMinerLoader(\"./data/labor_law.pdf\")\n",
    "pdf_docs = pdfminer_loader.load()\n",
    "print(f'PDF Î¨∏ÏÑú Í∞úÏàò: {len(pdf_docs)}')\n",
    "\n",
    "# Í∞Å ÌéòÏù¥ÏßÄÎ≥Ñ Ï†ïÎ≥¥ ÌôïÏù∏\n",
    "for i, doc in enumerate(pdf_docs[:3]):\n",
    "    print(f\"ÌéòÏù¥ÏßÄ {i+1}: {len(doc.page_content)} Î¨∏Ïûê\")\n",
    "    print(f\"Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f249b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdf_docs[0].page_content[:1000])  # Ï≤´ ÌéòÏù¥ÏßÄÏùò ÎÇ¥Ïö© ÏùºÎ∂Ä Ï∂úÎ†•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2907759c",
   "metadata": {},
   "source": [
    "### 4. üìù ÌÖçÏä§Ìä∏ ÌååÏùº Î°úÎçî (TextLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60e731e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î¨∏ÏÑú Ïàò: 1\n",
      "Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú ÎÇ¥Ïö©:\n",
      "1. ÏãúÍ∑∏ÎãàÏ≤ò Ïä§ÌÖåÏù¥ÌÅ¨\n",
      "   ‚Ä¢ Í∞ÄÍ≤©: ‚Ç©35,000\n",
      "   ‚Ä¢ Ï£ºÏöî ÏãùÏû¨Î£å: ÏµúÏÉÅÍ∏â ÌïúÏö∞ Îì±Ïã¨, Î°úÏ¶àÎ©îÎ¶¨ Í∞êÏûê, Í∑∏Î¶¥Îìú ÏïÑÏä§ÌååÎùºÍ±∞Ïä§\n",
      "   ‚Ä¢ ÏÑ§Î™Ö: ÏÖ∞ÌîÑÏùò ÌäπÏ†ú ÏãúÍ∑∏ÎãàÏ≤ò Î©îÎâ¥Î°ú, 21ÏùºÍ∞Ñ Í±¥Ï°∞ ÏàôÏÑ±Ìïú ÏµúÏÉÅÍ∏â ÌïúÏö∞ Îì±Ïã¨ÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§. ÎØ∏ÎîîÏóÑ Î†àÏñ¥Î°ú Ï°∞Î¶¨ÌïòÏó¨ Ïú°Ï¶ôÏùÑ ÏµúÎåÄÌïú Î≥¥Ï°¥ÌïòÎ©∞, Î°úÏ¶àÎ©îÎ¶¨ Ìñ•Ïùò Í∞êÏûêÏôÄ ÏïÑÏÇ≠Ìïú Í∑∏Î¶¥Îìú ÏïÑÏä§ÌååÎùºÍ±∞Ïä§Í∞Ä Í≥ÅÎì§Ïó¨ÏßëÎãàÎã§. Î†àÎìúÏôÄÏù∏ ÏÜåÏä§ÏôÄ Ìï®Íªò Ï†úÍ≥µÎêòÏñ¥ ÌíçÎ∂ÄÌïú ÎßõÏùÑ ÎçîÌï©ÎãàÎã§.\n",
      "\n",
      "2. Ìä∏Îü¨Ìîå Î¶¨Ï°∞Îòê\n",
      "   ‚Ä¢ Í∞ÄÍ≤©: ‚Ç©22,000\n",
      "   ‚Ä¢ Ï£ºÏöî ÏãùÏû¨Î£å: Ïù¥ÌÉàÎ¶¨ÏïÑÏÇ∞ ÏïÑÎ•¥Î≥¥Î¶¨Ïò§ ÏåÄ, Î∏îÎûô Ìä∏Îü¨Ìîå, ÌååÎ•¥ÎØ∏ÏßÄÏïÑÎÖ∏ Î†àÏßÄÏïÑÎÖ∏ ÏπòÏ¶à\n",
      "   ‚Ä¢ ÏÑ§Î™Ö: ÌÅ¨Î¶¨ÎØ∏Ìïú ÌÖçÏä§Ï≤òÏùò Î¶¨Ï°∞ÎòêÏóê Í≥†Í∏â Î∏îÎûô Ìä∏Îü¨ÌîåÏùÑ Îì¨Îøç ÏñπÏñ¥ ÌíçÎ∂ÄÌïú Ìñ•Í≥º ÎßõÏùÑ Ï¶êÍ∏∏ Ïàò ÏûàÎäî Î©îÎâ¥ÏûÖÎãàÎã§. 24Í∞úÏõî ÏàôÏÑ±Îêú ÌååÎ•¥ÎØ∏ÏßÄÏïÑÎÖ∏ Î†àÏßÄÏïÑÎÖ∏ ÏπòÏ¶àÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÍπäÏùÄ ÎßõÏùÑ ÎçîÌñàÏúºÎ©∞, Ï£ºÎ¨∏ Ï¶âÏãú Ï°∞Î¶¨ÌïòÏó¨ ÏµúÏÉÅÏùò ÏÉÅÌÉúÎ°ú Ï†úÍ≥µÎê©ÎãàÎã§.\n",
      "\n",
      "3. Ïó∞Ïñ¥ ÌÉÄÎ•¥ÌÉÄÎ•¥\n",
      "   ‚Ä¢ Í∞ÄÍ≤©: ‚Ç©18,000\n",
      "   ‚Ä¢ Ï£ºÏöî ÏãùÏû¨Î£å: ÎÖ∏Î•¥Ïõ®Ïù¥ÏÇ∞ ÏÉùÏó∞Ïñ¥, ÏïÑÎ≥¥Ïπ¥ÎèÑ, ÏºÄÏù¥Ìçº, Ï†ÅÏñëÌåå\n",
      "   ‚Ä¢ ÏÑ§Î™Ö: Ïã†ÏÑ†Ìïú ÎÖ∏Î•¥Ïõ®Ïù¥ÏÇ∞ ÏÉùÏó∞Ïñ¥Î•º Í≥±Í≤å Îã§Ï†∏ ÏïÑÎ≥¥Ïπ¥ÎèÑ, ÏºÄÏù¥Ìçº, Ï†ÅÏñëÌååÏôÄ Ìï®Íªò ÏÑûÏñ¥ ÎßåÎì† ÌÉÄÎ•¥ÌÉÄÎ•¥ÏûÖÎãàÎã§. Î†àÎ™¨ ÎìúÎ†àÏã±ÏúºÎ°ú ÏÉÅÌÅºÌïú ÎßõÏùÑ ÎçîÌñàÏúºÎ©∞, Î∞îÏÇ≠Ìïú Î∏åÎ¶¨Ïò§Ïâ¨ ÌÜ†Ïä§Ìä∏ÏôÄ Ìï®Íªò Ï†úÍ≥µÎê©ÎãàÎã§. Ï†ÑÏ±ÑÏöîÎ¶¨Î°ú ÏôÑÎ≤ΩÌïú Î©îÎâ¥ÏûÖÎãàÎã§.\n",
      "\n",
      "4. Î≤ÑÏÑØ ÌÅ¨Î¶º ÏàòÌîÑ\n",
      "   ‚Ä¢ Í∞ÄÍ≤©: ‚Ç©10,000\n",
      "   ‚Ä¢ Ï£ºÏöî ÏãùÏû¨Î£å: ÏñëÏÜ°Ïù¥Î≤ÑÏÑØ, ÌëúÍ≥†Î≤ÑÏÑØ, ÏÉùÌÅ¨Î¶º, Ìä∏Îü¨Ìîå Ïò§Ïùº\n",
      "   ‚Ä¢ ÏÑ§Î™Ö: ÏñëÏÜ°Ïù¥Î≤ÑÏÑØÍ≥º ÌëúÍ≥†Î≤ÑÏÑØÏùÑ Ïò§Îûú ÏãúÍ∞Ñ Ï†ïÏÑ±Ïä§Î†à ÎÅìÏó¨ ÎßåÎì† ÌÅ¨Î¶º ÏàòÌîÑÏûÖÎãàÎã§. Î∂ÄÎìúÎü¨Ïö¥ ÌÖçÏä§Ï≤òÏôÄ ÍπäÏùÄ Î≤ÑÏÑØ Ìñ•Ïù¥ ÌäπÏßïÏù¥Î©∞, ÏµúÏÉÅÍ∏â Ìä∏Îü¨Ìîå Ïò§ÏùºÏùÑ ÏÇ¥Ïßù ÎøåÎ†§ Í≥†Í∏âÏä§Îü¨Ïö¥ Ìñ•ÏùÑ ÎçîÌñàÏäµÎãàÎã§. ÌååÏä¨Î¶¨Î•º Í≥±Í≤å Îã§Ï†∏ Í≥†Î™ÖÏúºÎ°ú Ïò¨Î†§ Ï†úÍ≥µÎê©ÎãàÎã§.\n",
      "\n",
      "5. Í∞ÄÎì† ÏÉêÎü¨Îìú\n",
      "   ‚Ä¢ Í∞ÄÍ≤©: ‚Ç©12,000\n",
      "   ‚Ä¢ Ï£ºÏöî ÏãùÏû¨Î£å: Ïú†Í∏∞ÎÜç ÎØπÏä§ Í∑∏Î¶∞, Ï≤¥Î¶¨ ÌÜ†ÎßàÌÜ†, Ïò§Ïù¥, ÎãπÍ∑º, Î∞úÏÇ¨ÎØπ ÎìúÎ†àÏã±\n",
      "   ‚Ä¢ ÏÑ§Î™Ö: Ïã†ÏÑ†Ìïú Ïú†Í∏∞ÎÜç Ï±ÑÏÜåÎì§Î°ú Íµ¨ÏÑ±Îêú Í±¥Í∞ïÌïú ÏÉêÎü¨ÎìúÏûÖÎãàÎã§. ÏïÑÏÇ≠Ìïú ÏãùÍ∞êÏùò ÎØπÏä§ Í∑∏Î¶∞Ïóê Îã¨ÏΩ§Ìïú Ï≤¥Î¶¨ ÌÜ†ÎßàÌÜ†, Ïò§Ïù¥, ÎãπÍ∑ºÏùÑ ÎçîÌï¥ Îã§ÏñëÌïú ÎßõÍ≥º ÏãùÍ∞êÏùÑ Ï¶êÍ∏∏ \n",
      "Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {'source': './data/restaurant_menu.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Îã®Ïùº ÌÖçÏä§Ìä∏ ÌååÏùº Î°úÎìú\n",
    "text_loader = TextLoader(\"./data/restaurant_menu.txt\", encoding=\"utf-8\")\n",
    "text_docs = text_loader.load()\n",
    "\n",
    "print(f\"Î¨∏ÏÑú Ïàò: {len(text_docs)}\")\n",
    "print(f\"Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú ÎÇ¥Ïö©:\\n{text_docs[0].page_content[:1000]}\")  # Ï≤´ 1000Ïûê Ï∂úÎ†•\n",
    "print(f\"Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {text_docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4519ae46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï†ÑÏ≤¥ Î¨∏ÏÑú Ïàò: 2\n",
      "Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú ÎÇ¥Ïö©:\n",
      "1. ÏãúÍ∑∏ÎãàÏ≤ò Ïä§ÌÖåÏù¥ÌÅ¨\n",
      "   ‚Ä¢ Í∞ÄÍ≤©: ‚Ç©35,000\n",
      "   ‚Ä¢ Ï£ºÏöî ÏãùÏû¨Î£å: ÏµúÏÉÅÍ∏â ÌïúÏö∞ Îì±Ïã¨, Î°úÏ¶àÎ©îÎ¶¨ Í∞êÏûê, Í∑∏Î¶¥Îìú ÏïÑÏä§ÌååÎùºÍ±∞Ïä§\n",
      "   ‚Ä¢ ÏÑ§Î™Ö: ÏÖ∞ÌîÑÏùò ÌäπÏ†ú ÏãúÍ∑∏ÎãàÏ≤ò Î©îÎâ¥Î°ú, 21ÏùºÍ∞Ñ Í±¥Ï°∞ ÏàôÏÑ±Ìïú ÏµúÏÉÅÍ∏â ÌïúÏö∞ Îì±Ïã¨ÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§. ÎØ∏ÎîîÏóÑ Î†àÏñ¥Î°ú Ï°∞Î¶¨ÌïòÏó¨ Ïú°Ï¶ôÏùÑ ÏµúÎåÄÌïú Î≥¥Ï°¥ÌïòÎ©∞, Î°úÏ¶àÎ©îÎ¶¨ Ìñ•Ïùò Í∞êÏûêÏôÄ ÏïÑÏÇ≠Ìïú Í∑∏Î¶¥Îìú ÏïÑÏä§ÌååÎùºÍ±∞Ïä§Í∞Ä Í≥ÅÎì§Ïó¨ÏßëÎãàÎã§. Î†àÎìúÏôÄÏù∏ ÏÜåÏä§ÏôÄ Ìï®Íªò Ï†úÍ≥µÎêòÏñ¥ ÌíçÎ∂ÄÌïú ÎßõÏùÑ ÎçîÌï©ÎãàÎã§.\n",
      "\n",
      "2. Ìä∏Îü¨Ìîå Î¶¨Ï°∞Îòê\n",
      "   ‚Ä¢ Í∞ÄÍ≤©: ‚Ç©22,000\n",
      "   ‚Ä¢ Ï£ºÏöî ÏãùÏû¨Î£å: Ïù¥ÌÉàÎ¶¨ÏïÑÏÇ∞ ÏïÑÎ•¥Î≥¥Î¶¨Ïò§ ÏåÄ, Î∏îÎûô Ìä∏Îü¨Ìîå, ÌååÎ•¥ÎØ∏ÏßÄÏïÑÎÖ∏ Î†àÏßÄÏïÑÎÖ∏ ÏπòÏ¶à\n",
      "   ‚Ä¢ ÏÑ§Î™Ö: ÌÅ¨Î¶¨ÎØ∏Ìïú ÌÖçÏä§Ï≤òÏùò Î¶¨Ï°∞ÎòêÏóê Í≥†Í∏â Î∏îÎûô Ìä∏Îü¨ÌîåÏùÑ Îì¨Îøç ÏñπÏñ¥ ÌíçÎ∂ÄÌïú Ìñ•Í≥º ÎßõÏùÑ Ï¶êÍ∏∏ Ïàò ÏûàÎäî Î©îÎâ¥ÏûÖÎãàÎã§. 24Í∞úÏõî ÏàôÏÑ±Îêú ÌååÎ•¥ÎØ∏ÏßÄÏïÑÎÖ∏ Î†àÏßÄÏïÑÎÖ∏ ÏπòÏ¶àÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÍπäÏùÄ ÎßõÏùÑ ÎçîÌñàÏúºÎ©∞, Ï£ºÎ¨∏ Ï¶âÏãú Ï°∞Î¶¨ÌïòÏó¨ ÏµúÏÉÅÏùò ÏÉÅÌÉúÎ°ú Ï†úÍ≥µÎê©ÎãàÎã§.\n",
      "\n",
      "3. Ïó∞Ïñ¥ ÌÉÄÎ•¥ÌÉÄÎ•¥\n",
      "   ‚Ä¢ Í∞ÄÍ≤©: ‚Ç©18,000\n",
      "   ‚Ä¢ Ï£ºÏöî ÏãùÏû¨Î£å: ÎÖ∏Î•¥Ïõ®Ïù¥ÏÇ∞ ÏÉùÏó∞Ïñ¥, ÏïÑÎ≥¥Ïπ¥ÎèÑ, ÏºÄÏù¥Ìçº, Ï†ÅÏñëÌåå\n",
      "   ‚Ä¢ ÏÑ§Î™Ö: Ïã†ÏÑ†Ìïú ÎÖ∏Î•¥Ïõ®Ïù¥ÏÇ∞ ÏÉùÏó∞Ïñ¥Î•º Í≥±Í≤å Îã§Ï†∏ ÏïÑÎ≥¥Ïπ¥ÎèÑ, ÏºÄÏù¥Ìçº, Ï†ÅÏñëÌååÏôÄ Ìï®Íªò ÏÑûÏñ¥ ÎßåÎì† ÌÉÄÎ•¥ÌÉÄÎ•¥ÏûÖÎãàÎã§. Î†àÎ™¨ ÎìúÎ†àÏã±ÏúºÎ°ú ÏÉÅÌÅºÌïú ÎßõÏùÑ ÎçîÌñàÏúºÎ©∞, Î∞îÏÇ≠Ìïú Î∏åÎ¶¨Ïò§Ïâ¨ ÌÜ†Ïä§Ìä∏ÏôÄ Ìï®Íªò Ï†úÍ≥µÎê©ÎãàÎã§. Ï†ÑÏ±ÑÏöîÎ¶¨Î°ú ÏôÑÎ≤ΩÌïú Î©îÎâ¥ÏûÖÎãàÎã§.\n",
      "\n",
      "4. Î≤ÑÏÑØ ÌÅ¨Î¶º ÏàòÌîÑ\n",
      "   ‚Ä¢ Í∞ÄÍ≤©: ‚Ç©10,000\n",
      "   ‚Ä¢ Ï£ºÏöî ÏãùÏû¨Î£å: ÏñëÏÜ°Ïù¥Î≤ÑÏÑØ, ÌëúÍ≥†Î≤ÑÏÑØ, ÏÉùÌÅ¨Î¶º, Ìä∏Îü¨Ìîå Ïò§Ïùº\n",
      "   ‚Ä¢ ÏÑ§Î™Ö: ÏñëÏÜ°Ïù¥Î≤ÑÏÑØÍ≥º ÌëúÍ≥†Î≤ÑÏÑØÏùÑ Ïò§Îûú ÏãúÍ∞Ñ Ï†ïÏÑ±Ïä§Î†à ÎÅìÏó¨ ÎßåÎì† ÌÅ¨Î¶º ÏàòÌîÑÏûÖÎãàÎã§. Î∂ÄÎìúÎü¨Ïö¥ ÌÖçÏä§Ï≤òÏôÄ ÍπäÏùÄ Î≤ÑÏÑØ Ìñ•Ïù¥ ÌäπÏßïÏù¥Î©∞, ÏµúÏÉÅÍ∏â Ìä∏Îü¨Ìîå Ïò§ÏùºÏùÑ ÏÇ¥Ïßù ÎøåÎ†§ Í≥†Í∏âÏä§Îü¨Ïö¥ Ìñ•ÏùÑ ÎçîÌñàÏäµÎãàÎã§. ÌååÏä¨Î¶¨Î•º Í≥±Í≤å Îã§Ï†∏ Í≥†Î™ÖÏúºÎ°ú Ïò¨Î†§ Ï†úÍ≥µÎê©ÎãàÎã§.\n",
      "\n",
      "5. Í∞ÄÎì† ÏÉêÎü¨Îìú\n",
      "   ‚Ä¢ Í∞ÄÍ≤©: ‚Ç©12,000\n",
      "   ‚Ä¢ Ï£ºÏöî ÏãùÏû¨Î£å: Ïú†Í∏∞ÎÜç ÎØπÏä§ Í∑∏Î¶∞, Ï≤¥Î¶¨ ÌÜ†ÎßàÌÜ†, Ïò§Ïù¥, ÎãπÍ∑º, Î∞úÏÇ¨ÎØπ ÎìúÎ†àÏã±\n",
      "   ‚Ä¢ ÏÑ§Î™Ö: Ïã†ÏÑ†Ìïú Ïú†Í∏∞ÎÜç Ï±ÑÏÜåÎì§Î°ú Íµ¨ÏÑ±Îêú Í±¥Í∞ïÌïú ÏÉêÎü¨ÎìúÏûÖÎãàÎã§. ÏïÑÏÇ≠Ìïú ÏãùÍ∞êÏùò ÎØπÏä§ Í∑∏Î¶∞Ïóê Îã¨ÏΩ§Ìïú Ï≤¥Î¶¨ ÌÜ†ÎßàÌÜ†, Ïò§Ïù¥, ÎãπÍ∑ºÏùÑ ÎçîÌï¥ Îã§ÏñëÌïú ÎßõÍ≥º ÏãùÍ∞êÏùÑ Ï¶êÍ∏∏ \n",
      "Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {'source': 'data\\\\restaurant_menu.txt'}\n"
     ]
    }
   ],
   "source": [
    "# ÎîîÎ†âÌÜ†Î¶¨ ÎÇ¥ Î™®Îì† ÌÖçÏä§Ìä∏ ÌååÏùº Î°úÎìú\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "directory_loader = DirectoryLoader(\n",
    "    \"./data/\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "all_text_docs = directory_loader.load()\n",
    "\n",
    "print(f\"Ï†ÑÏ≤¥ Î¨∏ÏÑú Ïàò: {len(all_text_docs)}\")\n",
    "print(f\"Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú ÎÇ¥Ïö©:\\n{all_text_docs[0].page_content[:1000]}\")  # Ï≤´ 1000Ïûê Ï∂úÎ†•\n",
    "print(f\"Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {all_text_docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5cb485",
   "metadata": {},
   "source": [
    "### üéØ Ïã§Ïäµ 1: Ïõπ Î¨∏ÏÑú Î°úÎçî Ïó∞Ïäµ   ~ 14:53Î∂ÑÍπåÏßÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îã§Ïùå Ïõπ ÌéòÏù¥ÏßÄÎì§ÏùÑ Î°úÎìúÌïòÍ≥† Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î•º Ï∂úÎ†•Ìï¥Î≥¥ÏÑ∏Ïöî\n",
    "urls = [\n",
    "    \"https://python.langchain.com/docs/tutorials/\",\n",
    "    \"https://python.langchain.com/docs/concepts/\"\n",
    "]\n",
    "\n",
    "# Ïó¨Í∏∞Ïóê ÏΩîÎìúÎ•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb33946",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ÌÖçÏä§Ìä∏ Î∂ÑÌï† (Text Splitting)\n",
    "\n",
    "### üéØ ÌÖçÏä§Ìä∏ Î∂ÑÌï†Ïù¥ ÌïÑÏöîÌïú Ïù¥Ïú†\n",
    "1. **ÌÜ†ÌÅ∞ Ï†úÌïú**: LLMÏùÄ ÏûÖÎ†• ÌÜ†ÌÅ∞ ÏàòÏóê Ï†úÌïúÏù¥ ÏûàÏùå\n",
    "2. **Í≤ÄÏÉâ Ï†ïÌôïÎèÑ**: ÏûëÏùÄ Ï≤≠ÌÅ¨Í∞Ä Îçî Ï†ïÌôïÌïú Í≤ÄÏÉâ Í≤∞Í≥º Ï†úÍ≥µ\n",
    "3. **Î©îÎ™®Î¶¨ Ìö®Ïú®ÏÑ±**: ÎåÄÏö©Îüâ Î¨∏ÏÑúÏùò Ìö®Ïú®Ï†Å Ï≤òÎ¶¨\n",
    "\n",
    "### üìä Î∂ÑÌï† Ï†ÑÎûµ ÎπÑÍµê\n",
    "| Î∞©Î≤ï | Ïû•Ï†ê | Îã®Ï†ê | ÏÇ¨Ïö© ÏÇ¨Î°Ä |\n",
    "|------|------|------|----------|\n",
    "| CharacterTextSplitter | Îã®Ïàú, Îπ†Î¶Ñ | Î¨∏Îß• Í≥†Î†§ ÏïàÌï® | Í∞ÑÎã®Ìïú ÌÖçÏä§Ìä∏ |\n",
    "| RecursiveCharacterTextSplitter | Î¨∏Îß• Î≥¥Ï°¥ Ïö∞Ïàò | Í≥ÑÏÇ∞ Î≥µÏû° | ÏùºÎ∞òÏ†ÅÏù∏ Î¨∏ÏÑú |\n",
    "| SemanticChunker | ÏùòÎØ∏ Í∏∞Î∞ò Î∂ÑÌï† | ÎäêÎ¶º, ÎπÑÏö© ÎßéÏùå | Ï§ëÏöîÌïú Î¨∏ÏÑú |\n",
    "| TokenTextSplitter | Ï†ïÌôïÌïú ÌÜ†ÌÅ∞ Ïàò | ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÏùòÏ°¥ | API ÎπÑÏö© ÏµúÏ†ÅÌôî |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa31164",
   "metadata": {},
   "source": [
    "### 1. CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2cb664d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï≤´ Î≤àÏß∏ Î¨∏ÏÑúÏùò ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: 1811\n"
     ]
    }
   ],
   "source": [
    "long_text = pdf_docs[0].page_content\n",
    "print(f'Ï≤´ Î≤àÏß∏ Î¨∏ÏÑúÏùò ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: {len(long_text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a007e618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î∂ÑÌï†Îêú Ï≤≠ÌÅ¨ Ïàò: 3\n",
      "1000\n",
      "1000\n",
      "210\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Í∏∞Î≥∏ ÏÑ§Ï†ï\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",        # Íµ¨Î∂ÑÏûê\n",
    "    chunk_size=1000,         # Ï≤≠ÌÅ¨ ÌÅ¨Í∏∞\n",
    "    chunk_overlap=200,       # Ï§ëÎ≥µ ÌÅ¨Í∏∞\n",
    "    length_function=len,     # Í∏∏Ïù¥ Ï∏°Ï†ï Ìï®Ïàò\n",
    "    is_separator_regex=False # Ï†ïÍ∑úÏãù Ïó¨Î∂Ä\n",
    ")\n",
    "\n",
    "# ÌÖçÏä§Ìä∏ Î∂ÑÌï†\n",
    "chunks = text_splitter.split_text(long_text)\n",
    "print(f\"Î∂ÑÌï†Îêú Ï≤≠ÌÅ¨ Ïàò: {len(chunks)}\")\n",
    "for chunk in chunks:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f3dedbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 620, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î¨∏Ïû• Îã®ÏúÑÎ°ú Î∂ÑÌï†Îêú Ï≤≠ÌÅ¨ Ïàò: 4\n",
      "Ï≤´ Î≤àÏß∏ Î¨∏Ïû• Ï≤≠ÌÅ¨ ÎÇ¥Ïö©: Î≤ïÏ†úÏ≤ò                                                            1                                                       Íµ≠Í∞ÄÎ≤ïÎ†πÏ†ïÎ≥¥ÏÑºÌÑ∞\n",
      "Í∑ºÎ°úÍ∏∞Ï§ÄÎ≤ï\n",
      " \n",
      "Í∑ºÎ°úÍ∏∞Ï§ÄÎ≤ï\n",
      "[ÏãúÌñâ 2021. 11. 19.] [Î≤ïÎ•† Ï†ú18176Ìò∏, 2021. 5. 18., ÏùºÎ∂ÄÍ∞úÏ†ï]\n",
      "Í≥†Ïö©ÎÖ∏ÎèôÎ∂Ä (Í∑ºÎ°úÍ∏∞Ï§ÄÏ†ïÏ±ÖÍ≥º - Ìï¥Í≥†, Ï∑®ÏóÖÍ∑úÏπô, Í∏∞ÌÉÄ) 044-202-7534\n",
      "Í≥†Ïö©ÎÖ∏ÎèôÎ∂Ä (Í∑ºÎ°úÍ∏∞Ï§ÄÏ†ïÏ±ÖÍ≥º - ÏÜåÎÖÑ) 044-202-7535\n",
      "Í≥†Ïö©ÎÖ∏ÎèôÎ∂Ä (Í∑ºÎ°úÍ∏∞Ï§ÄÏ†ïÏ±ÖÍ≥º - ÏûÑÍ∏à) 044-202-7548\n",
      "Í≥†Ïö©ÎÖ∏ÎèôÎ∂Ä (Ïó¨ÏÑ±Í≥†Ïö©Ï†ïÏ±ÖÍ≥º - Ïó¨ÏÑ±) 044-202-7475\n",
      "Í≥†Ïö©ÎÖ∏ÎèôÎ∂Ä (ÏûÑÍ∏àÍ∑ºÎ°úÏãúÍ∞ÑÏ†ïÏ±ÖÍ≥º - Í∑ºÎ°úÏãúÍ∞Ñ, Ìú¥Í≤å) 044-202-7545\n",
      "Í≥†Ïö©ÎÖ∏ÎèôÎ∂Ä (ÏûÑÍ∏àÍ∑ºÎ°úÏãúÍ∞ÑÏ†ïÏ±ÖÍ≥º - Ìú¥Ïùº, Ïó∞Ï∞®Ìú¥Í∞Ä) 044-202-7973\n",
      "Í≥†Ïö©ÎÖ∏ÎèôÎ∂Ä (ÏûÑÍ∏àÍ∑ºÎ°úÏãúÍ∞ÑÏ†ïÏ±ÖÍ≥º - Ï†ú63Ï°∞ Ï†ÅÏö©Ï†úÏô∏, ÌäπÎ°ÄÏóÖÏ¢Ö) 044-202-7530\n",
      "Í≥†Ïö©ÎÖ∏ÎèôÎ∂Ä (ÏûÑÍ∏àÍ∑ºÎ°úÏãúÍ∞ÑÏ†ïÏ±ÖÍ≥º - Ïú†Ïó∞Í∑ºÎ°úÏãúÍ∞ÑÏ†ú) 044-202-7549\n",
      "       Ï†ú1Ïû• Ï¥ùÏπô\n",
      " \n",
      "Ï†ú1Ï°∞(Î™©Ï†Å) Ïù¥ Î≤ïÏùÄ ÌóåÎ≤ïÏóê Îî∞Îùº Í∑ºÎ°úÏ°∞Í±¥Ïùò Í∏∞Ï§ÄÏùÑ Ï†ïÌï®ÏúºÎ°úÏç® Í∑ºÎ°úÏûêÏùò Í∏∞Î≥∏Ï†Å ÏÉùÌôúÏùÑ Î≥¥Ïû•, Ìñ•ÏÉÅÏãúÌÇ§Î©∞ Í∑†Ìòï ÏûàÎäî\n",
      "Íµ≠ÎØºÍ≤ΩÏ†úÏùò Î∞úÏ†ÑÏùÑ ÍæÄÌïòÎäî Í≤ÉÏùÑ Î™©Ï†ÅÏúºÎ°ú ÌïúÎã§.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Ï†ïÍ∑úÏãùÏùÑ ÏÇ¨Ïö©Ìïú Î¨∏Ïû• Îã®ÏúÑ Î∂ÑÌï†\n",
    "def create_improved_sentence_splitter():\n",
    "    \"\"\"\n",
    "    Î≤ïÎ•† Î¨∏ÏÑúÏôÄ Í≥µÎ¨∏ÏÑúÏóê Ï†ÅÌï©Ìïú Î¨∏Ïû• Î∂ÑÌï†Í∏∞Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Í∞ÑÎã®ÌïòÍ≥† ÏïàÏ†ÑÌïú Ï†ïÍ∑úÏãù Ìå®ÌÑ¥\n",
    "    # Î¨∏Ïû• ÎÅù ÌõÑ Í≥µÎ∞±Ïù¥ ÏûàÍ≥†, ÌäπÏ†ï Ìå®ÌÑ¥Ïù¥ Îî∞ÎùºÏò§ÏßÄ ÏïäÎäî Í≤ΩÏö∞Îßå Î∂ÑÌï†\n",
    "    improved_pattern = r'(?<=[.!?])\\s+(?!\\s*(?:\\d+|Ìò∏|Ï°∞|Ìï≠|]|\\)|[Í∞Ä-Ìû£]{1,2}\\s*\\)|[A-Za-z]\\s*\\)|[,;:]))'\n",
    "    \n",
    "    sentence_splitter = CharacterTextSplitter(\n",
    "        separator=improved_pattern,\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100,\n",
    "        is_separator_regex=True,\n",
    "        keep_separator=True\n",
    "    )\n",
    "    \n",
    "    return sentence_splitter\n",
    "\n",
    "\n",
    "# Ï†ïÍ∑úÏãùÏùÑ ÏÇ¨Ïö©Ìïú Î¨∏Ïû• Îã®ÏúÑ Î∂ÑÌï†Í∏∞ ÏÉùÏÑ±\n",
    "sentence_splitter = create_improved_sentence_splitter()\n",
    "\n",
    "# Î¨∏Ïû• Îã®ÏúÑÎ°ú Î∂ÑÌï†\n",
    "sentence_chunks = sentence_splitter.split_text(long_text)\n",
    "print(f\"Î¨∏Ïû• Îã®ÏúÑÎ°ú Î∂ÑÌï†Îêú Ï≤≠ÌÅ¨ Ïàò: {len(sentence_chunks)}\")\n",
    "print(f\"Ï≤´ Î≤àÏß∏ Î¨∏Ïû• Ï≤≠ÌÅ¨ ÎÇ¥Ïö©: {sentence_chunks[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a04b70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Îëê Î≤àÏß∏ Î¨∏Ïû• Ï≤≠ÌÅ¨ ÎÇ¥Ïö©: Ï†ú2Ï°∞(Ï†ïÏùò) ‚ë† Ïù¥ Î≤ïÏóêÏÑú ÏÇ¨Ïö©ÌïòÎäî Ïö©Ïñ¥Ïùò ÎúªÏùÄ Îã§ÏùåÍ≥º Í∞ôÎã§. <Í∞úÏ†ï 2018. 3. 20., 2019. 1. 15., 2020. 5. 26.>\n",
      "1. ‚ÄúÍ∑ºÎ°úÏûê‚ÄùÎûÄ ÏßÅÏóÖÏùò Ï¢ÖÎ•òÏôÄ Í¥ÄÍ≥ÑÏóÜÏù¥ ÏûÑÍ∏àÏùÑ Î™©Ï†ÅÏúºÎ°ú ÏÇ¨ÏóÖÏù¥ÎÇò ÏÇ¨ÏóÖÏû•Ïóê Í∑ºÎ°úÎ•º Ï†úÍ≥µÌïòÎäî ÏÇ¨ÎûåÏùÑ ÎßêÌïúÎã§.\n",
      "2. ‚ÄúÏÇ¨Ïö©Ïûê‚ÄùÎûÄ ÏÇ¨ÏóÖÏ£º ÎòêÎäî ÏÇ¨ÏóÖ Í≤ΩÏòÅ Îã¥ÎãπÏûê, Í∑∏ Î∞ñÏóê Í∑ºÎ°úÏûêÏóê Í¥ÄÌïú ÏÇ¨Ìï≠Ïóê ÎåÄÌïòÏó¨ ÏÇ¨ÏóÖÏ£ºÎ•º ÏúÑÌïòÏó¨ ÌñâÏúÑÌïòÎäî ÏûêÎ•º\n",
      "ÎßêÌïúÎã§.\n",
      "3. ‚ÄúÍ∑ºÎ°ú‚ÄùÎûÄ Ï†ïÏã†ÎÖ∏ÎèôÍ≥º Ïú°Ï≤¥ÎÖ∏ÎèôÏùÑ ÎßêÌïúÎã§.\n",
      "4. ‚ÄúÍ∑ºÎ°úÍ≥ÑÏïΩ‚ÄùÏù¥ÎûÄ Í∑ºÎ°úÏûêÍ∞Ä ÏÇ¨Ïö©ÏûêÏóêÍ≤å Í∑ºÎ°úÎ•º Ï†úÍ≥µÌïòÍ≥† ÏÇ¨Ïö©ÏûêÎäî Ïù¥Ïóê ÎåÄÌïòÏó¨ ÏûÑÍ∏àÏùÑ ÏßÄÍ∏âÌïòÎäî Í≤ÉÏùÑ Î™©Ï†ÅÏúºÎ°ú Ï≤¥\n",
      "Í≤∞Îêú Í≥ÑÏïΩÏùÑ ÎßêÌïúÎã§.\n",
      "5. ‚ÄúÏûÑÍ∏à‚ÄùÏù¥ÎûÄ ÏÇ¨Ïö©ÏûêÍ∞Ä Í∑ºÎ°úÏùò ÎåÄÍ∞ÄÎ°ú Í∑ºÎ°úÏûêÏóêÍ≤å ÏûÑÍ∏à, Î¥âÍ∏â, Í∑∏ Î∞ñÏóê Ïñ¥Îñ†Ìïú Î™ÖÏπ≠ÏúºÎ°úÎì†ÏßÄ ÏßÄÍ∏âÌïòÎäî Î™®Îì† Í∏àÌíàÏùÑ\n",
      "ÎßêÌïúÎã§.\n",
      "6. ‚ÄúÌèâÍ∑†ÏûÑÍ∏à‚ÄùÏù¥ÎûÄ Ïù¥Î•º ÏÇ∞Ï†ïÌïòÏó¨Ïïº Ìï† ÏÇ¨Ïú†Í∞Ä Î∞úÏÉùÌïú ÎÇ† Ïù¥Ï†Ñ 3Í∞úÏõî ÎèôÏïàÏóê Í∑∏ Í∑ºÎ°úÏûêÏóêÍ≤å ÏßÄÍ∏âÎêú ÏûÑÍ∏àÏùò Ï¥ùÏï°ÏùÑ\n",
      "Í∑∏ Í∏∞Í∞ÑÏùò Ï¥ùÏùºÏàòÎ°ú ÎÇòÎàà Í∏àÏï°ÏùÑ ÎßêÌïúÎã§.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Îëê Î≤àÏß∏ Î¨∏Ïû• Ï≤≠ÌÅ¨ ÎÇ¥Ïö©: {sentence_chunks[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824d6d1",
   "metadata": {},
   "source": [
    "### 2. RecursiveCharacterTextSplitter\n",
    "\n",
    "- Ïû¨Í∑ÄÏ†ÅÏúºÎ°ú ÌÖçÏä§Ìä∏Î•º Î∂ÑÌï†ÌïòÏó¨ Î¨∏Îß•ÏùÑ ÏµúÎåÄÌïú Î≥¥Ï°¥ÌïòÎäî Î∂ÑÌï†Í∏∞ÏûÖÎãàÎã§.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93c8a641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏÉùÏÑ±Îêú Ï≤≠ÌÅ¨ Ïàò: 56\n",
      "Ï≤≠ÌÅ¨ 1: 936 Î¨∏Ïûê\n",
      "Ï≤≠ÌÅ¨ 2: 944 Î¨∏Ïûê\n",
      "Ï≤≠ÌÅ¨ 3: 273 Î¨∏Ïûê\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Í∏∞Î≥∏ Ïû¨Í∑Ä Î∂ÑÌï†Í∏∞\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", r'(?<=[.!?])\\s+(?!\\s*(?:\\d+|Ìò∏|Ï°∞|Ìï≠|]|\\)|[Í∞Ä-Ìû£]{1,2}\\s*\\)|[A-Za-z]\\s*\\)|[,;:]))']  # Ïö∞ÏÑ†ÏàúÏúÑ ÏàúÏÑú\n",
    ")\n",
    "\n",
    "# Document Í∞ùÏ≤¥ Î∂ÑÌï†\n",
    "chunks = text_splitter.split_documents(pdf_docs)\n",
    "print(f\"ÏÉùÏÑ±Îêú Ï≤≠ÌÅ¨ Ïàò: {len(chunks)}\")\n",
    "\n",
    "# Í∞Å Ï≤≠ÌÅ¨Ïùò Í∏∏Ïù¥ ÌôïÏù∏\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Ï≤≠ÌÅ¨ {i+1}: {len(chunk.page_content)} Î¨∏Ïûê\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfe18e1",
   "metadata": {},
   "source": [
    "### 3. ÌÜ†ÌÅ∞ Í∏∞Î∞ò Î∂ÑÌï†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34cd7a",
   "metadata": {},
   "source": [
    "#### üîß TikToken Í∏∞Î∞ò Î∂ÑÌï†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c97df91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï≤≠ÌÅ¨ 1: 492 ÌÜ†ÌÅ∞\n",
      "Ï≤≠ÌÅ¨ 2: 449 ÌÜ†ÌÅ∞\n",
      "Ï≤≠ÌÅ¨ 3: 475 ÌÜ†ÌÅ∞\n"
     ]
    }
   ],
   "source": [
    "# OpenAI ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÏÇ¨Ïö©\n",
    "token_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",  # GPT-4 Ïù∏ÏΩîÎî©\n",
    "    chunk_size=500,              # ÌÜ†ÌÅ∞ Ïàò Í∏∞Ï§Ä\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "chunks = token_splitter.split_documents([pdf_docs[0]])\n",
    "\n",
    "# ÌÜ†ÌÅ∞ Ïàò ÌôïÏù∏\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    token_count = len(tokenizer.encode(chunk.page_content))\n",
    "    print(f\"Ï≤≠ÌÅ¨ {i+1}: {token_count} ÌÜ†ÌÅ∞\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ba7a0",
   "metadata": {},
   "source": [
    "#### ü§ó Hugging Face ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e50854e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï≤≠ÌÅ¨ 1: 284 ÌÜ†ÌÅ∞\n",
      "Ï≤≠ÌÅ¨ 2: 298 ÌÜ†ÌÅ∞\n",
      "Ï≤≠ÌÅ¨ 3: 273 ÌÜ†ÌÅ∞\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# BGE-M3 ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÏÇ¨Ïö©\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "\n",
    "hf_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunks = hf_splitter.split_documents([pdf_docs[0]])\n",
    "\n",
    "# ÌÜ†ÌÅ∞ Ïàò ÌôïÏù∏\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    token_count = len(tokenizer(chunk.page_content)[\"input_ids\"])\n",
    "    print(f\"Ï≤≠ÌÅ¨ {i+1}: {token_count} ÌÜ†ÌÅ∞\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0bbc2e",
   "metadata": {},
   "source": [
    "### 4. **Semantic Chunking**\n",
    "\n",
    "- **SemanticChunker**Îäî ÌÖçÏä§Ìä∏Î•º ÏùòÎØ∏ Îã®ÏúÑÎ°ú **Î∂ÑÌï†**ÌïòÎäî ÌäπÏàòÌïú ÌÖçÏä§Ìä∏ Î∂ÑÌï†ÎèÑÍµ¨ \n",
    "\n",
    "- Îã®Ïàú Í∏∏Ïù¥ Í∏∞Î∞òÏù¥ ÏïÑÎãå **ÏùòÎØ∏ Í∏∞Î∞ò**ÏúºÎ°ú ÌÖçÏä§Ìä∏Î•º Ï≤≠ÌÅ¨ÌôîÌïòÎäîÎç∞ Ìö®Í≥ºÏ†Å\n",
    "\n",
    "- **breakpoint_threshold_type**: Text SplittingÏùò Îã§ÏñëÌïú ÏûÑÍ≥ÑÍ∞í(Threshold) ÏÑ§Ï†ï Î∞©Ïãù (ÌÜµÍ≥ÑÏ†Å Í∏∞Î≤ï) \n",
    "\n",
    "    - **Gradient** Î∞©Ïãù: ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ Í∞ÑÏùò **Í∏∞Ïö∏Í∏∞ Î≥ÄÌôî**Î•º Í∏∞Ï§ÄÏúºÎ°ú ÌÖçÏä§Ìä∏Î•º Î∂ÑÌï†\n",
    "    - **Percentile** Î∞©Ïãù: ÏûÑÎ≤†Îî© Í±∞Î¶¨Ïùò **Î∞±Î∂ÑÏúÑÏàò**Î•º Í∏∞Ï§ÄÏúºÎ°ú Î∂ÑÌï† ÏßÄÏ†êÏùÑ Í≤∞Ï†ï (Í∏∞Î≥∏Í∞í: 95%)\n",
    "    - **Standard Deviation** Î∞©Ïãù: ÏûÑÎ≤†Îî© Í±∞Î¶¨Ïùò **ÌëúÏ§ÄÌé∏Ï∞®**Î•º ÌôúÏö©ÌïòÏó¨ Ïú†ÏùòÎØ∏Ìïú Î≥ÄÌôîÏ†êÏùÑ Ï∞æÏïÑÏÑú Î∂ÑÌï†\n",
    "    - **Interquartile** Î∞©Ïãù: ÏûÑÎ≤†Îî© Í±∞Î¶¨Ïùò **ÏÇ¨Î∂ÑÏúÑÏàò Î≤îÏúÑ**Î•º Í∏∞Ï§ÄÏúºÎ°ú Ïù¥ÏÉÅÏπòÎ•º Í∞êÏßÄÌïòÏó¨ Î∂ÑÌï†\n",
    "\n",
    "- ÏÑ§Ïπò: pip install langchain_experimental ÎòêÎäî uv add langchain_experimental\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker \n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# ÏûÑÎ≤†Îî© Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ SemanticChunkerÎ•º Ï¥àÍ∏∞Ìôî \n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"),         # OpenAI ÏûÑÎ≤†Îî© ÏÇ¨Ïö©\n",
    "    breakpoint_threshold_type=\"gradient\",  # ÏûÑÍ≥ÑÍ∞í ÌÉÄÏûÖ ÏÑ§Ï†ï (gradient, percentile, standard_deviation, interquartile)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956587ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_documents([pdf_docs[0]])\n",
    "\n",
    "print(f\"ÏÉùÏÑ±Îêú Ï≤≠ÌÅ¨ Ïàò: {len(chunks)}\")\n",
    "print(f\"Í∞Å Ï≤≠ÌÅ¨Ïùò Í∏∏Ïù¥: {list(len(chunk.page_content) for chunk in chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eb3dd5",
   "metadata": {},
   "source": [
    "### 5. Íµ¨Ï°∞ Í∏∞Î∞ò Î∂ÑÌï†\n",
    "\n",
    "#### üìÑ HTML Î∂ÑÌï†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30c51832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Ìó§ÎçîÎ°ú Î∂ÑÌï†Îêú Ï≤≠ÌÅ¨ Ïàò: 4\n",
      "Ï≤≠ÌÅ¨ 1:\n",
      "page_content='Ï†úÎ™© 1' metadata={'Header 1': 'Ï†úÎ™© 1'}\n",
      "----------------------------------------\n",
      "Ï≤≠ÌÅ¨ 2:\n",
      "page_content='ÎÇ¥Ïö© 1' metadata={'Header 1': 'Ï†úÎ™© 1'}\n",
      "----------------------------------------\n",
      "Ï≤≠ÌÅ¨ 3:\n",
      "page_content='ÏÜåÏ†úÎ™© 1.1' metadata={'Header 1': 'Ï†úÎ™© 1', 'Header 2': 'ÏÜåÏ†úÎ™© 1.1'}\n",
      "----------------------------------------\n",
      "Ï≤≠ÌÅ¨ 4:\n",
      "page_content='ÎÇ¥Ïö© 1.1' metadata={'Header 1': 'Ï†úÎ™© 1', 'Header 2': 'ÏÜåÏ†úÎ™© 1.1'}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <div>\n",
    "        <h1>Ï†úÎ™© 1</h1>\n",
    "        <p>ÎÇ¥Ïö© 1</p>\n",
    "        <h2>ÏÜåÏ†úÎ™© 1.1</h2>\n",
    "        <p>ÎÇ¥Ïö© 1.1</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "\n",
    "# Í≤∞Í≥º Ï∂úÎ†•\n",
    "print(f\"HTML Ìó§ÎçîÎ°ú Î∂ÑÌï†Îêú Ï≤≠ÌÅ¨ Ïàò: {len(html_header_splits)}\")\n",
    "for i, chunk in enumerate(html_header_splits):\n",
    "    print(f\"Ï≤≠ÌÅ¨ {i+1}:\\n{chunk}\\n{'-'*40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5764c948",
   "metadata": {},
   "source": [
    "#### üßë‚Äçüíª ÏΩîÎìú Î∂ÑÌï†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47f2273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python ÏΩîÎìú Ï≤≠ÌÅ¨ Ïàò: 2\n",
      "Ï≤≠ÌÅ¨ 1:\n",
      "def hello_world():\n",
      "    print(\"Hello, World!\")\n",
      "----------------------------------------\n",
      "Ï≤≠ÌÅ¨ 2:\n",
      "class MyClass:\n",
      "    def __init__(self):\n",
      "        self.value = 42\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter\n",
    ")\n",
    "\n",
    "# Python ÏΩîÎìú Î∂ÑÌï†\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "python_code = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self.value = 42\n",
    "\"\"\"\n",
    "\n",
    "python_docs = python_splitter.create_documents([python_code])\n",
    "\n",
    "print(f\"Python ÏΩîÎìú Ï≤≠ÌÅ¨ Ïàò: {len(python_docs)}\")\n",
    "for i, doc in enumerate(python_docs):\n",
    "    print(f\"Ï≤≠ÌÅ¨ {i+1}:\\n{doc.page_content}\\n{'-'*40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568bc61e",
   "metadata": {},
   "source": [
    "### üéØ Ïã§Ïäµ 2: ÌÖçÏä§Ìä∏ Î∂ÑÌï† ÎπÑÍµê ~ 15:47Î∂ÑÍπåÏßÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îã§Ïùå ÌÖçÏä§Ìä∏Î•º Îã§ÏñëÌïú Î∞©Î≤ïÏúºÎ°ú Î∂ÑÌï†ÌïòÍ≥† Í≤∞Í≥ºÎ•º ÎπÑÍµêÌï¥Î≥¥ÏÑ∏Ïöî\n",
    "sample_text = \"\"\"\n",
    "Ïù∏Í≥µÏßÄÎä•ÏùÄ ÌòÑÎåÄ Í∏∞Ïà†Ïùò ÌïµÏã¨ÏûÖÎãàÎã§. \n",
    "Î®∏Ïã†Îü¨ÎãùÏùÑ ÌÜµÌï¥ Ïª¥Ìì®ÌÑ∞Îäî ÌïôÏäµÌï† Ïàò ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "Îî•Îü¨ÎãùÏùÄ Ïã†Í≤ΩÎßùÏùÑ Í∏∞Î∞òÏúºÎ°ú Ìï©ÎãàÎã§.\n",
    "ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨Îäî ÌÖçÏä§Ìä∏Î•º Ïù¥Ìï¥ÌïòÎäî Í∏∞Ïà†ÏûÖÎãàÎã§.\n",
    "\n",
    "Ïª¥Ìì®ÌÑ∞ ÎπÑÏ†ÑÏùÄ Ïù¥ÎØ∏ÏßÄÎ•º Î∂ÑÏÑùÌï©ÎãàÎã§.\n",
    "Í∞ïÌôîÌïôÏäµÏùÄ ÌñâÎèôÏùÑ ÌÜµÌï¥ ÌïôÏäµÌï©ÎãàÎã§.\n",
    "\"\"\"\n",
    "\n",
    "# Ïó¨Í∏∞Ïóê ÏΩîÎìúÎ•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec29fae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Î¨∏ÏÑú ÏûÑÎ≤†Îî© (Document Embedding)\n",
    "\n",
    "### üéØ ÏûÑÎ≤†Îî©Ïù¥ÎûÄ?\n",
    "ÌÖçÏä§Ìä∏Î•º Í≥†Ï∞®Ïõê Î≤°ÌÑ∞ Í≥µÍ∞ÑÏùò Ïà´Ïûê Î∞∞Ïó¥Î°ú Î≥ÄÌôòÌïòÏó¨ ÏùòÎØ∏Ï†Å Ïú†ÏÇ¨ÎèÑÎ•º Í≥ÑÏÇ∞Ìï† Ïàò ÏûàÍ≤å ÌïòÎäî Í∏∞Ïà†ÏûÖÎãàÎã§.\n",
    "\n",
    "### üìä ÏûÑÎ≤†Îî© Î™®Îç∏ ÎπÑÍµê\n",
    "| Î™®Îç∏ | Ï∞®Ïõê | Ïñ∏Ïñ¥ ÏßÄÏõê | ÎπÑÏö© | ÏÑ±Îä• | ÏÇ¨Ïö© ÏÇ¨Î°Ä |\n",
    "|------|------|----------|------|------|----------|\n",
    "| OpenAI text-embedding-3-small | 1536 | Îã§Íµ≠Ïñ¥ | Ïú†Î£å | ÎÜíÏùå | ÌîÑÎ°úÎçïÏÖò |\n",
    "| OpenAI text-embedding-3-large | 3072 | Îã§Íµ≠Ïñ¥ | Ïú†Î£å | ÏµúÍ≥† | Í≥†ÏÑ±Îä• ÏöîÍµ¨ |\n",
    "| BAAI/bge-m3 | 1024 | Îã§Íµ≠Ïñ¥ | Î¨¥Î£å | ÎÜíÏùå | ÌïúÍµ≠Ïñ¥ ÌäπÌôî |\n",
    "| sentence-transformers/all-MiniLM-L6-v2 | 384 | ÏòÅÏñ¥ | Î¨¥Î£å | Ï§ëÍ∞Ñ | Î°úÏª¨ Í∞úÎ∞ú |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97d3272",
   "metadata": {},
   "source": [
    "### 1. OpenAI ÏûÑÎ≤†Îî©\n",
    "\n",
    "#### üîß Í∏∞Î≥∏ ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf5008c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏûÑÎ≤†Îî© Ï∞®Ïõê: 1536\n",
      "Ïª®ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: 8191\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Í∏∞Î≥∏ ÏûÑÎ≤†Îî© Î™®Îç∏\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    dimensions=1536,           # Ï∞®Ïõê Ïàò (Í∏∞Î≥∏Í∞í)\n",
    "    show_progress_bar=True,    # ÏßÑÌñâÎ•† ÌëúÏãú\n",
    "    max_retries=3             # Ïû¨ÏãúÎèÑ ÌöüÏàò\n",
    ")\n",
    "\n",
    "print(f\"ÏûÑÎ≤†Îî© Ï∞®Ïõê: {embeddings_model.dimensions}\")\n",
    "print(f\"Ïª®ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: {embeddings_model.embedding_ctx_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9801824",
   "metadata": {},
   "source": [
    "#### üìù Î¨∏ÏÑú ÏûÑÎ≤†Îî©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19286db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ Ïàò: 5\n",
      "Í∞Å Î≤°ÌÑ∞ Ï∞®Ïõê: 1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Î¨∏ÏÑú Ïª¨Î†âÏÖò ÏûÑÎ≤†Îî©\n",
    "documents = [\n",
    "    \"Ïù∏Í≥µÏßÄÎä•ÏùÄ Ïª¥Ìì®ÌÑ∞ Í≥ºÌïôÏùò Ìïú Î∂ÑÏïºÏûÖÎãàÎã§.\",\n",
    "    \"Î®∏Ïã†Îü¨ÎãùÏùÄ Ïù∏Í≥µÏßÄÎä•Ïùò ÌïòÏúÑ Î∂ÑÏïºÏûÖÎãàÎã§.\",\n",
    "    \"Îî•Îü¨ÎãùÏùÄ Î®∏Ïã†Îü¨ÎãùÏùò Ìïú Ï¢ÖÎ•òÏûÖÎãàÎã§.\",\n",
    "    \"ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨Îäî Ïª¥Ìì®ÌÑ∞Í∞Ä Ïù∏Í∞ÑÏùò Ïñ∏Ïñ¥Î•º Ïù¥Ìï¥ÌïòÎäî Í∏∞Ïà†ÏûÖÎãàÎã§.\",\n",
    "    \"Ïª¥Ìì®ÌÑ∞ ÎπÑÏ†ÑÏùÄ Ïù¥ÎØ∏ÏßÄÎ•º Î∂ÑÏÑùÌïòÎäî Í∏∞Ïà†ÏûÖÎãàÎã§.\"\n",
    "]\n",
    "\n",
    "# Î∞∞Ïπò ÏûÑÎ≤†Îî© (Ìö®Ïú®Ï†Å)\n",
    "doc_embeddings = embeddings_model.embed_documents(documents)\n",
    "print(f\"ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ Ïàò: {len(doc_embeddings)}\")\n",
    "print(f\"Í∞Å Î≤°ÌÑ∞ Ï∞®Ïõê: {len(doc_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfa6525d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏøºÎ¶¨ ÏûÑÎ≤†Îî© Ï∞®Ïõê: 1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ÏøºÎ¶¨ ÏûÑÎ≤†Îî©\n",
    "query = \"AI Í∏∞Ïà†Ïóê ÎåÄÌï¥ ÏïåÎ†§Ï£ºÏÑ∏Ïöî\"\n",
    "query_embedding = embeddings_model.embed_query(query)\n",
    "print(f\"ÏøºÎ¶¨ ÏûÑÎ≤†Îî© Ï∞®Ïõê: {len(query_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67317e58",
   "metadata": {},
   "source": [
    "#### üí° Ï∞®Ïõê Ï∂ïÏÜå ÌôúÏö©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ab31808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï∂ïÏÜåÎêú ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ Ïàò: 5\n",
      "Ï∂ïÏÜåÎêú Í∞Å Î≤°ÌÑ∞ Ï∞®Ïõê: 512\n"
     ]
    }
   ],
   "source": [
    "# ÎπÑÏö© Ï†àÏïΩÏùÑ ÏúÑÌïú Ï∞®Ïõê Ï∂ïÏÜå\n",
    "compact_embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\", \n",
    "    dimensions=512  # ÏõêÎûò 1536ÏóêÏÑú 512Î°ú Ï∂ïÏÜå\n",
    ")\n",
    "\n",
    "# ÏÑ±Îä•Í≥º ÎπÑÏö©Ïùò Í∑†ÌòïÏ†ê Ï∞æÎäî Í≤ÉÏù¥ Ï§ëÏöî!!!\n",
    "compact_doc_embeddings = compact_embeddings.embed_documents(documents)\n",
    "print(f\"Ï∂ïÏÜåÎêú ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ Ïàò: {len(compact_doc_embeddings)}\")\n",
    "print(f\"Ï∂ïÏÜåÎêú Í∞Å Î≤°ÌÑ∞ Ï∞®Ïõê: {len(compact_doc_embeddings[0])}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137bbef1",
   "metadata": {},
   "source": [
    "### 2. Hugging Face ÏûÑÎ≤†Îî©\n",
    "\n",
    "#### ü§ó BGE-M3 Î™®Îç∏ (ÌïúÍµ≠Ïñ¥ Ïö∞Ïàò)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83222ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌïúÍµ≠Ïñ¥ ÏûÑÎ≤†Îî© Ï∞®Ïõê: 1024\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# BGE-M3 Î™®Îç∏ (Îã§Íµ≠Ïñ¥, ÌïúÍµ≠Ïñ¥ ÏÑ±Îä• Ïö∞Ïàò)\n",
    "embeddings_bge = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={'device': 'cpu'},        # 'cuda' for GPU\n",
    "    encode_kwargs={'normalize_embeddings': True}  # L2 Ï†ïÍ∑úÌôî\n",
    ")\n",
    "\n",
    "# BGE-M3 Î™®Îç∏Î°ú Î¨∏ÏÑú ÏûÑÎ≤†Îî©\n",
    "bge_hf_embeddings = embeddings_bge.embed_documents(documents)\n",
    "print(f\"ÌïúÍµ≠Ïñ¥ ÏûÑÎ≤†Îî© Ï∞®Ïõê: {len(bge_hf_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a79ffb",
   "metadata": {},
   "source": [
    "#### üì± Í≤ΩÎüâ Î™®Îç∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2ab84bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Í≤ΩÎüâ Î™®Îç∏ ÌïúÍµ≠Ïñ¥ ÏûÑÎ≤†Îî© Ï∞®Ïõê: 768\n"
     ]
    }
   ],
   "source": [
    "# Îπ†Î•∏ Ï≤òÎ¶¨Î•º ÏúÑÌïú Í≤ΩÎüâ Î™®Îç∏\n",
    "embedding_gte = HuggingFaceEmbeddings(\n",
    "    model_name=\"Alibaba-NLP/gte-multilingual-base\",\n",
    "    model_kwargs={'device': 'cpu', 'trust_remote_code': True},  # trust_remote_code for custom models\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "    \n",
    "# Í≤ΩÎüâ Î™®Îç∏Î°ú Î¨∏ÏÑú ÏûÑÎ≤†Îî©\n",
    "alibaba_hf_embeddings = embedding_gte.embed_documents(documents)\n",
    "print(f\"Í≤ΩÎüâ Î™®Îç∏ ÌïúÍµ≠Ïñ¥ ÏûÑÎ≤†Îî© Ï∞®Ïõê: {len(alibaba_hf_embeddings[0])}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc083cd",
   "metadata": {},
   "source": [
    "### 3. Ollama ÏûÑÎ≤†Îî© (Î°úÏª¨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ad5bbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î°úÏª¨ ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ Ïàò: 5\n",
      "Í∞Å Î≤°ÌÑ∞ Ï∞®Ïõê: 1024\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# Ollama ÏÑúÎ≤ÑÍ∞Ä Ïã§Ìñâ Ï§ëÏù¥Ïñ¥Ïïº Ìï®\n",
    "embeddings_ollama = OllamaEmbeddings(\n",
    "    model=\"bge-m3\",                    # ÏÇ¨Ïö©Ìï† Î™®Îç∏\n",
    "    # base_url=\"http://localhost:11434\"  # Ollama ÏÑúÎ≤Ñ Ï£ºÏÜå\n",
    ")\n",
    "\n",
    "# Î°úÏª¨ ÏûÑÎ≤†Îî©\n",
    "local_embeddings = embeddings_ollama.embed_documents(documents)\n",
    "\n",
    "print(f\"Î°úÏª¨ ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ Ïàò: {len(local_embeddings)}\")\n",
    "print(f\"Í∞Å Î≤°ÌÑ∞ Ï∞®Ïõê: {len(local_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441df2cb",
   "metadata": {},
   "source": [
    "### 4. Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ Î∞è Í≤ÄÏÉâ\n",
    "\n",
    "#### üìè ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e87b9bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏøºÎ¶¨: Îî•Îü¨ÎãùÏóê ÎåÄÌï¥ ÏïåÎ†§Ï£ºÏÑ∏Ïöî\n",
      "Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Î¨∏ÏÑú: Îî•Îü¨ÎãùÏùÄ Î®∏Ïã†Îü¨ÎãùÏùò Ìïú Ï¢ÖÎ•òÏûÖÎãàÎã§.\n",
      "Ïú†ÏÇ¨ÎèÑ Ï†êÏàò: 0.5868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.utils.math import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def find_most_similar(query, doc_embeddings, documents, embeddings_model):\n",
    "    \"\"\"Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Î¨∏ÏÑú Ï∞æÍ∏∞\"\"\"\n",
    "    # ÏøºÎ¶¨ ÏûÑÎ≤†Îî©\n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "    \n",
    "    # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞\n",
    "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    \n",
    "    # Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Î¨∏ÏÑú Ïù∏Îç±Ïä§\n",
    "    most_similar_idx = np.argmax(similarities)\n",
    "    \n",
    "    return {\n",
    "        \"document\": documents[most_similar_idx],\n",
    "        \"similarity\": similarities[most_similar_idx],\n",
    "        \"index\": most_similar_idx\n",
    "    }\n",
    "\n",
    "# ÏøºÎ¶¨ÏôÄ Î¨∏ÏÑú ÏûÑÎ≤†Îî©ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Î¨∏ÏÑú Ï∞æÍ∏∞ (OpenAI)\n",
    "query = \"Îî•Îü¨ÎãùÏóê ÎåÄÌï¥ ÏïåÎ†§Ï£ºÏÑ∏Ïöî\"\n",
    "result = find_most_similar(query, doc_embeddings, documents, embeddings_model)\n",
    "\n",
    "print(f\"ÏøºÎ¶¨: {query}\")\n",
    "print(f\"Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Î¨∏ÏÑú: {result['document']}\")\n",
    "print(f\"Ïú†ÏÇ¨ÎèÑ Ï†êÏàò: {result['similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45d78393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏøºÎ¶¨: Îî•Îü¨ÎãùÏóê ÎåÄÌï¥ ÏïåÎ†§Ï£ºÏÑ∏Ïöî\n",
      "Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Î¨∏ÏÑú: Îî•Îü¨ÎãùÏùÄ Î®∏Ïã†Îü¨ÎãùÏùò Ìïú Ï¢ÖÎ•òÏûÖÎãàÎã§.\n",
      "Ïú†ÏÇ¨ÎèÑ Ï†êÏàò: 0.7360\n"
     ]
    }
   ],
   "source": [
    "# HuggingFaceEmbeddingsÎ•º ÏÇ¨Ïö©Ìïú Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ (BGE-M3)\n",
    "result = find_most_similar(query, bge_hf_embeddings, documents, embeddings_bge)\n",
    "print(f\"ÏøºÎ¶¨: {query}\")\n",
    "print(f\"Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Î¨∏ÏÑú: {result['document']}\")\n",
    "print(f\"Ïú†ÏÇ¨ÎèÑ Ï†êÏàò: {result['similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34a9fe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏøºÎ¶¨: Îî•Îü¨ÎãùÏóê ÎåÄÌï¥ ÏïåÎ†§Ï£ºÏÑ∏Ïöî\n",
      "Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Î¨∏ÏÑú: Îî•Îü¨ÎãùÏùÄ Î®∏Ïã†Îü¨ÎãùÏùò Ìïú Ï¢ÖÎ•òÏûÖÎãàÎã§.\n",
      "Ïú†ÏÇ¨ÎèÑ Ï†êÏàò: 0.7886\n"
     ]
    }
   ],
   "source": [
    "# Alibaba-NLP/gte-multilingual-base Î™®Îç∏Î°ú Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ\n",
    "result = find_most_similar(query, alibaba_hf_embeddings, documents, embedding_gte)\n",
    "print(f\"ÏøºÎ¶¨: {query}\")\n",
    "print(f\"Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Î¨∏ÏÑú: {result['document']}\")\n",
    "print(f\"Ïú†ÏÇ¨ÎèÑ Ï†êÏàò: {result['similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "157eced6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏøºÎ¶¨: Îî•Îü¨ÎãùÏóê ÎåÄÌï¥ ÏïåÎ†§Ï£ºÏÑ∏Ïöî\n",
      "Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Î¨∏ÏÑú: Îî•Îü¨ÎãùÏùÄ Î®∏Ïã†Îü¨ÎãùÏùò Ìïú Ï¢ÖÎ•òÏûÖÎãàÎã§.\n",
      "Ïú†ÏÇ¨ÎèÑ Ï†êÏàò: 0.7352\n"
     ]
    }
   ],
   "source": [
    "# Ollama Î™®Îç∏Î°ú Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ (bge-m3)\n",
    "result = find_most_similar(query, local_embeddings, documents, embeddings_ollama)\n",
    "print(f\"ÏøºÎ¶¨: {query}\")\n",
    "print(f\"Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Î¨∏ÏÑú: {result['document']}\")\n",
    "print(f\"Ïú†ÏÇ¨ÎèÑ Ï†êÏàò: {result['similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d40c4",
   "metadata": {},
   "source": [
    "### üéØ Ïã§Ïäµ 3: ÏûÑÎ≤†Îî© Î™®Îç∏ ÎπÑÍµê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec6277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îã§Ïùå ÏßàÎ¨∏Îì§Ïóê ÎåÄÌï¥ Îã§Î•∏ ÏûÑÎ≤†Îî© Î™®Îç∏Îì§Ïùò Í≤ÄÏÉâ ÏÑ±Îä•ÏùÑ ÎπÑÍµêÌï¥Î≥¥ÏÑ∏Ïöî\n",
    "queries = [\n",
    "    \"Í∏∞Í≥ÑÌïôÏäµÏù¥ÎûÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?\",\n",
    "    \"Ïù¥ÎØ∏ÏßÄ Ïù∏Ïãù Í∏∞Ïà†Ïóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî\",\n",
    "    \"Ïñ∏Ïñ¥ Î™®Îç∏Ïùò ÏûëÎèô ÏõêÎ¶¨Îäî?\"\n",
    "]\n",
    "\n",
    "# Ïó¨Í∏∞Ïóê ÏΩîÎìúÎ•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ffa75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå (Vector Store)\n",
    "\n",
    "### üéØ Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜåÎûÄ?\n",
    "ÏûÑÎ≤†Îî©Îêú Î≤°ÌÑ∞Î•º Ìö®Ïú®Ï†ÅÏúºÎ°ú Ï†ÄÏû•ÌïòÍ≥† Ïú†ÏÇ¨ÎèÑ Í∏∞Î∞ò Í≤ÄÏÉâÏùÑ ÏàòÌñâÌïòÎäî ÌäπÏàò Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏûÖÎãàÎã§.\n",
    "\n",
    "### üìä Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå ÎπÑÍµê\n",
    "| Ï¢ÖÎ•ò | Ïû•Ï†ê | Îã®Ï†ê | ÏÇ¨Ïö© ÏÇ¨Î°Ä |\n",
    "|------|------|------|----------|\n",
    "| Chroma | ÏÑ§Ïπò Í∞ÑÎã®, Î°úÏª¨ ÏπúÌôîÏ†Å | ÎåÄÏö©Îüâ Ï≤òÎ¶¨ ÌïúÍ≥Ñ | Í∞úÎ∞ú, ÌîÑÎ°úÌÜ†ÌÉÄÏûÖ |\n",
    "| FAISS | Îß§Ïö∞ Îπ†Î¶Ñ, ÌôïÏû•ÏÑ± Ïö∞Ïàò | ÏÑ§Ï†ï Î≥µÏû° | ÎåÄÏö©Îüâ Í≤ÄÏÉâ |\n",
    "| Pinecone | ÏôÑÏ†Ñ Í¥ÄÎ¶¨Ìòï, Í≥†ÏÑ±Îä• | Ïú†Î£å, ÌÅ¥ÎùºÏö∞Îìú ÏùòÏ°¥ | ÌîÑÎ°úÎçïÏÖò |\n",
    "| Weaviate | GraphQL ÏßÄÏõê, ÌïòÏù¥Î∏åÎ¶¨Îìú Í≤ÄÏÉâ | ÌïôÏäµ Í≥°ÏÑ† | Î≥µÌï© Í≤ÄÏÉâ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bd40b",
   "metadata": {},
   "source": [
    "### 1. Chroma Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå\n",
    "\n",
    "#### üöÄ Chroma ÏÑ§Ïπò Î∞è ÏÑ§Ï†ï\n",
    "```bash\n",
    "pip install langchain-chroma\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bb5d82",
   "metadata": {},
   "source": [
    "#### üìö Í∏∞Î≥∏ ÏÇ¨Ïö©Î≤ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61cc15d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï†ÄÏû•Îêú Î¨∏ÏÑú Ïàò: 95\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Î¨∏ÏÑú Ï§ÄÎπÑ\n",
    "pdf_loader = PyPDFLoader('./data/labor_law.pdf')\n",
    "pdf_docs = pdf_loader.load()\n",
    "\n",
    "# ÌÖçÏä§Ìä∏ Î∂ÑÌï†\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "chunks = text_splitter.split_documents(pdf_docs)\n",
    "\n",
    "# ÏûÑÎ≤†Îî© Î™®Îç∏\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Chroma Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå ÏÉùÏÑ±\n",
    "chroma_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"labor_law\",\n",
    "    persist_directory=\"./local_chroma_db\",\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}  # Ïú†ÏÇ¨ÎèÑ Î©îÌä∏Î¶≠ (l2)\n",
    ")\n",
    "\n",
    "print(f\"Ï†ÄÏû•Îêú Î¨∏ÏÑú Ïàò: {chroma_db._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d65858",
   "metadata": {},
   "source": [
    "#### üíæ Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå Î°úÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26d62249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î°úÎìúÎêú Î¨∏ÏÑú Ïàò: 95\n"
     ]
    }
   ],
   "source": [
    "# Í∏∞Ï°¥ Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå Î°úÎìú\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"labor_law\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./local_chroma_db\"\n",
    ")\n",
    "\n",
    "print(f\"Î°úÎìúÎêú Î¨∏ÏÑú Ïàò: {chroma_db._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581bc215",
   "metadata": {},
   "source": [
    "#### üîç Í∏∞Î≥∏ Í≤ÄÏÉâ Í∏∞Îä•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1e74be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_pages': 20,\n",
       " 'creator': 'PyPDF',\n",
       " 'creationdate': '2024-10-15T14:45:34+09:00',\n",
       " 'moddate': '2024-10-15T14:45:34+09:00',\n",
       " 'producer': 'iText 2.1.7 by 1T3XT',\n",
       " 'page_label': '3',\n",
       " 'page': 2,\n",
       " 'source': './data/labor_law.pdf'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1887e37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Í≤ÄÏÉâ Í≤∞Í≥º Ïàò: 5\n",
      "Í≤∞Í≥º 1: [Ï†úÎ™©Í∞úÏ†ï 2021. 5. 18.]\n",
      " \n",
      "Ï†ú49Ï°∞(ÏûÑÍ∏àÏùò ÏãúÌö®) Ïù¥ Î≤ïÏóê Îî∞Î•∏ ÏûÑÍ∏àÏ±ÑÍ∂åÏùÄ 3ÎÖÑÍ∞Ñ ÌñâÏÇ¨ÌïòÏßÄ ÏïÑÎãàÌïòÎ©¥ ÏãúÌö®Î°ú ÏÜåÎ©∏ÌïúÎã§.\n",
      " \n",
      "       Ï†ú4Ïû• Í∑ºÎ°úÏãúÍ∞ÑÍ≥º Ìú¥Ïãù\n",
      " \n",
      "Ï†ú50Ï°∞(Í∑ºÎ°úÏãúÍ∞Ñ) ‚ë† 1Ï£º Í∞ÑÏùò Í∑ºÎ°úÏãúÍ∞ÑÏùÄ Ìú¥Í≤åÏãúÍ∞ÑÏùÑ Ï†úÏô∏ÌïòÍ≥† 40ÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌï† Ïàò ÏóÜÎã§.\n",
      "‚ë° 1ÏùºÏùò Í∑ºÎ°úÏãúÍ∞ÑÏùÄ Ìú¥Í≤åÏãúÍ∞ÑÏùÑ Ï†úÏô∏ÌïòÍ≥† 8ÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌï† Ïàò ÏóÜÎã§.\n",
      "‚ë¢ Ï†ú1Ìï≠ Î∞è Ï†ú2Ìï≠Ïóê Îî∞Îùº Í∑ºÎ°úÏãúÍ∞ÑÏùÑ ÏÇ∞Ï†ïÌïòÎäî Í≤ΩÏö∞ ÏûëÏóÖÏùÑ ÏúÑÌïòÏó¨ Í∑ºÎ°úÏûêÍ∞Ä ÏÇ¨Ïö©ÏûêÏùò ÏßÄÌúò„ÜçÍ∞êÎèÖ ÏïÑÎûòÏóê ÏûàÎäî ÎåÄ\n",
      "Í∏∞ÏãúÍ∞Ñ Îì±ÏùÄ Í∑ºÎ°úÏãúÍ∞ÑÏúºÎ°ú Î≥∏Îã§.<Ïã†ÏÑ§ 2012. 2. 1., 2020. 5. 26.>\n",
      " \n",
      "Ï†ú51Ï°∞(3Í∞úÏõî Ïù¥ÎÇ¥Ïùò ÌÉÑÎ†•Ï†Å Í∑ºÎ°úÏãúÍ∞ÑÏ†ú) ‚ë† ÏÇ¨Ïö©ÏûêÎäî Ï∑®ÏóÖÍ∑úÏπô(Ï∑®ÏóÖÍ∑úÏπôÏóê Ï§ÄÌïòÎäî Í≤ÉÏùÑ Ìè¨Ìï®ÌïúÎã§)ÏóêÏÑú Ï†ïÌïòÎäî Î∞îÏóê Îî∞\n",
      "Îùº 2Ï£º Ïù¥ÎÇ¥Ïùò ÏùºÏ†ïÌïú Îã®ÏúÑÍ∏∞Í∞ÑÏùÑ ÌèâÍ∑†ÌïòÏó¨ 1Ï£º Í∞ÑÏùò Í∑ºÎ°úÏãúÍ∞ÑÏù¥ Ï†ú50Ï°∞Ï†ú1Ìï≠Ïùò Í∑ºÎ°úÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌïòÏßÄ ÏïÑÎãàÌïòÎäî Î≤î\n",
      "ÏúÑÏóêÏÑú ÌäπÏ†ïÌïú Ï£ºÏóê Ï†ú50Ï°∞Ï†ú1Ìï≠Ïùò Í∑ºÎ°úÏãúÍ∞ÑÏùÑ, ÌäπÏ†ïÌïú ÎÇ†Ïóê Ï†ú50Ï°∞Ï†ú2Ìï≠Ïùò Í∑ºÎ°úÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌïòÏó¨ Í∑ºÎ°úÌïòÍ≤å Ìï† Ïàò...\n",
      "----------------------------------------\n",
      "Í≤∞Í≥º 2: Î≤ïÏ†úÏ≤ò                                                            9                                                       Íµ≠Í∞ÄÎ≤ïÎ†πÏ†ïÎ≥¥ÏÑºÌÑ∞\n",
      "Í∑ºÎ°úÍ∏∞Ï§ÄÎ≤ï\n",
      "[Ï†úÎ™©Í∞úÏ†ï 2021. 1. 5.]\n",
      " \n",
      "Ï†ú51Ï°∞Ïùò2(3Í∞úÏõîÏùÑ Ï¥àÍ≥ºÌïòÎäî ÌÉÑÎ†•Ï†Å Í∑ºÎ°úÏãúÍ∞ÑÏ†ú) ‚ë† ÏÇ¨Ïö©ÏûêÎäî Í∑ºÎ°úÏûêÎåÄÌëúÏôÄÏùò ÏÑúÎ©¥ Ìï©ÏùòÏóê Îî∞Îùº Îã§Ïùå Í∞Å Ìò∏Ïùò ÏÇ¨Ìï≠ÏùÑ\n",
      "Ï†ïÌïòÎ©¥ 3Í∞úÏõîÏùÑ Ï¥àÍ≥ºÌïòÍ≥† 6Í∞úÏõî Ïù¥ÎÇ¥Ïùò Îã®ÏúÑÍ∏∞Í∞ÑÏùÑ ÌèâÍ∑†ÌïòÏó¨ 1Ï£ºÍ∞ÑÏùò Í∑ºÎ°úÏãúÍ∞ÑÏù¥ Ï†ú50Ï°∞Ï†ú1Ìï≠Ïùò Í∑ºÎ°úÏãúÍ∞ÑÏùÑ Ï¥àÍ≥º\n",
      "ÌïòÏßÄ ÏïÑÎãàÌïòÎäî Î≤îÏúÑÏóêÏÑú ÌäπÏ†ïÌïú Ï£ºÏóê Ï†ú50Ï°∞Ï†ú1Ìï≠Ïùò Í∑ºÎ°úÏãúÍ∞ÑÏùÑ, ÌäπÏ†ïÌïú ÎÇ†Ïóê Ï†ú50Ï°∞Ï†ú2Ìï≠Ïùò Í∑ºÎ°úÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌïòÏó¨\n",
      "Í∑ºÎ°úÌïòÍ≤å Ìï† Ïàò ÏûàÎã§. Îã§Îßå, ÌäπÏ†ïÌïú Ï£ºÏùò Í∑ºÎ°úÏãúÍ∞ÑÏùÄ 52ÏãúÍ∞ÑÏùÑ, ÌäπÏ†ïÌïú ÎÇ†Ïùò Í∑ºÎ°úÏãúÍ∞ÑÏùÄ 12ÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌï† Ïàò ÏóÜÎã§.\n",
      "1. ÎåÄÏÉÅ Í∑ºÎ°úÏûêÏùò Î≤îÏúÑ\n",
      "2. Îã®ÏúÑÍ∏∞Í∞Ñ(3Í∞úÏõîÏùÑ Ï¥àÍ≥ºÌïòÍ≥† 6Í∞úÏõî Ïù¥ÎÇ¥Ïùò ÏùºÏ†ïÌïú Í∏∞Í∞ÑÏúºÎ°ú Ï†ïÌïòÏó¨Ïïº ÌïúÎã§)\n",
      "3. Îã®ÏúÑÍ∏∞Í∞ÑÏùò Ï£ºÎ≥Ñ Í∑ºÎ°úÏãúÍ∞Ñ\n",
      "4. Í∑∏ Î∞ñÏóê ÎåÄÌÜµÎ†πÎ†πÏúºÎ°ú...\n",
      "----------------------------------------\n",
      "Í≤∞Í≥º 3: ÎèôÏïàÏùò ÏÜåÏ†ïÍ∑ºÎ°úÏãúÍ∞ÑÏóê ÎπÑÌïòÏó¨ ÏßßÏùÄ Í∑ºÎ°úÏûêÎ•º ÎßêÌïúÎã§.\n",
      "‚ë° Ï†ú1Ìï≠Ï†ú6Ìò∏Ïóê Îî∞Îùº ÏÇ∞Ï∂úÎêú Í∏àÏï°Ïù¥ Í∑∏ Í∑ºÎ°úÏûêÏùò ÌÜµÏÉÅÏûÑÍ∏àÎ≥¥Îã§ Ï†ÅÏúºÎ©¥ Í∑∏ ÌÜµÏÉÅÏûÑÍ∏àÏï°ÏùÑ ÌèâÍ∑†ÏûÑÍ∏àÏúºÎ°ú ÌïúÎã§.\n",
      " \n",
      "Ï†ú3Ï°∞(Í∑ºÎ°úÏ°∞Í±¥Ïùò Í∏∞Ï§Ä) Ïù¥ Î≤ïÏóêÏÑú Ï†ïÌïòÎäî Í∑ºÎ°úÏ°∞Í±¥ÏùÄ ÏµúÏ†ÄÍ∏∞Ï§ÄÏù¥ÎØÄÎ°ú Í∑ºÎ°ú Í¥ÄÍ≥Ñ ÎãπÏÇ¨ÏûêÎäî Ïù¥ Í∏∞Ï§ÄÏùÑ Ïù¥Ïú†Î°ú Í∑ºÎ°úÏ°∞Í±¥\n",
      "ÏùÑ ÎÇÆÏ∂ú Ïàò ÏóÜÎã§.\n",
      " \n",
      "Ï†ú4Ï°∞(Í∑ºÎ°úÏ°∞Í±¥Ïùò Í≤∞Ï†ï) Í∑ºÎ°úÏ°∞Í±¥ÏùÄ Í∑ºÎ°úÏûêÏôÄ ÏÇ¨Ïö©ÏûêÍ∞Ä ÎèôÎì±Ìïú ÏßÄÏúÑÏóêÏÑú ÏûêÏú†ÏùòÏÇ¨Ïóê Îî∞Îùº Í≤∞Ï†ïÌïòÏó¨Ïïº ÌïúÎã§.\n",
      " \n",
      "Ï†ú5Ï°∞(Í∑ºÎ°úÏ°∞Í±¥Ïùò Ï§ÄÏàò) Í∑ºÎ°úÏûêÏôÄ ÏÇ¨Ïö©ÏûêÎäî Í∞ÅÏûêÍ∞Ä Îã®Ï≤¥ÌòëÏïΩ, Ï∑®ÏóÖÍ∑úÏπôÍ≥º Í∑ºÎ°úÍ≥ÑÏïΩÏùÑ ÏßÄÌÇ§Í≥† ÏÑ±Ïã§ÌïòÍ≤å Ïù¥ÌñâÌï† ÏùòÎ¨¥Í∞Ä\n",
      "ÏûàÎã§.\n",
      " \n",
      "Ï†ú6Ï°∞(Í∑†Îì±Ìïú Ï≤òÏö∞) ÏÇ¨Ïö©ÏûêÎäî Í∑ºÎ°úÏûêÏóê ÎåÄÌïòÏó¨ ÎÇ®ÎÖÄÏùò ÏÑ±(ÊÄß)ÏùÑ Ïù¥Ïú†Î°ú Ï∞®Î≥ÑÏ†Å ÎåÄÏö∞Î•º ÌïòÏßÄ Î™ªÌïòÍ≥†, Íµ≠Ï†Å„ÜçÏã†Ïïô ÎòêÎäî ÏÇ¨\n",
      "ÌöåÏ†Å Ïã†Î∂ÑÏùÑ Ïù¥Ïú†Î°ú Í∑ºÎ°úÏ°∞Í±¥Ïóê ÎåÄÌïú Ï∞®Î≥ÑÏ†Å Ï≤òÏö∞Î•º ÌïòÏßÄ Î™ªÌïúÎã§.\n",
      " \n",
      "Ï†ú7Ï°∞(Í∞ïÏ†ú Í∑ºÎ°úÏùò Í∏àÏßÄ) ÏÇ¨Ïö©ÏûêÎäî Ìè≠Ìñâ, ÌòëÎ∞ï, Í∞êÍ∏à, Í∑∏ Î∞ñÏóê Ï†ïÏã†ÏÉÅ ÎòêÎäî Ïã†Ï≤¥ÏÉÅÏùò ÏûêÏú†Î•º Î∂ÄÎãπÌïòÍ≤å Íµ¨ÏÜçÌïòÎäî ÏàòÎã®ÏúºÎ°ú...\n",
      "----------------------------------------\n",
      "Í≤∞Í≥º 4: ÌÜµÏÉÅÏ†ÅÏúºÎ°ú ÏÜåÏ†ïÍ∑ºÎ°úÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌïòÏó¨ Í∑ºÎ°úÌï† ÌïÑÏöîÍ∞Ä ÏûàÎäî Í≤ΩÏö∞ÏóêÎäî Í∑∏ ÏóÖÎ¨¥Ïùò ÏàòÌñâÏóê ÌÜµÏÉÅ ÌïÑÏöîÌïú ÏãúÍ∞ÑÏùÑ Í∑ºÎ°úÌïú\n",
      "Í≤ÉÏúºÎ°ú Î≥∏Îã§.\n",
      "‚ë° Ï†ú1Ìï≠ Îã®ÏÑúÏóêÎèÑ Î∂àÍµ¨ÌïòÍ≥† Í∑∏ ÏóÖÎ¨¥Ïóê Í¥ÄÌïòÏó¨ Í∑ºÎ°úÏûêÎåÄÌëúÏôÄÏùò ÏÑúÎ©¥ Ìï©ÏùòÎ•º Ìïú Í≤ΩÏö∞ÏóêÎäî Í∑∏ Ìï©ÏùòÏóêÏÑú Ï†ïÌïòÎäî ÏãúÍ∞Ñ\n",
      "ÏùÑ Í∑∏ ÏóÖÎ¨¥Ïùò ÏàòÌñâÏóê ÌÜµÏÉÅ ÌïÑÏöîÌïú ÏãúÍ∞ÑÏúºÎ°ú Î≥∏Îã§.\n",
      "‚ë¢ ÏóÖÎ¨¥Ïùò ÏÑ±ÏßàÏóê ÎπÑÏ∂îÏñ¥ ÏóÖÎ¨¥ ÏàòÌñâ Î∞©Î≤ïÏùÑ Í∑ºÎ°úÏûêÏùò Ïû¨ÎüâÏóê ÏúÑÏûÑÌï† ÌïÑÏöîÍ∞Ä ÏûàÎäî ÏóÖÎ¨¥Î°úÏÑú ÎåÄÌÜµÎ†πÎ†πÏúºÎ°ú Ï†ïÌïòÎäî ÏóÖ\n",
      "Î¨¥Îäî ÏÇ¨Ïö©ÏûêÍ∞Ä Í∑ºÎ°úÏûêÎåÄÌëúÏôÄ ÏÑúÎ©¥ Ìï©ÏùòÎ°ú Ï†ïÌïú ÏãúÍ∞ÑÏùÑ Í∑ºÎ°úÌïú Í≤ÉÏúºÎ°ú Î≥∏Îã§. Ïù¥ Í≤ΩÏö∞ Í∑∏ ÏÑúÎ©¥ Ìï©ÏùòÏóêÎäî Îã§Ïùå Í∞Å Ìò∏Ïùò\n",
      "ÏÇ¨Ìï≠ÏùÑ Î™ÖÏãúÌïòÏó¨Ïïº ÌïúÎã§.\n",
      "1. ÎåÄÏÉÅ ÏóÖÎ¨¥\n",
      "2. ÏÇ¨Ïö©ÏûêÍ∞Ä ÏóÖÎ¨¥Ïùò ÏàòÌñâ ÏàòÎã® Î∞è ÏãúÍ∞Ñ Î∞∞Î∂Ñ Îì±Ïóê Í¥ÄÌïòÏó¨ Í∑ºÎ°úÏûêÏóêÍ≤å Íµ¨Ï≤¥Ï†ÅÏù∏ ÏßÄÏãúÎ•º ÌïòÏßÄ ÏïÑÎãàÌïúÎã§Îäî ÎÇ¥Ïö©\n",
      "3. Í∑ºÎ°úÏãúÍ∞ÑÏùò ÏÇ∞Ï†ïÏùÄ Í∑∏ ÏÑúÎ©¥ Ìï©ÏùòÎ°ú Ï†ïÌïòÎäî Î∞îÏóê Îî∞Î•∏Îã§Îäî ÎÇ¥Ïö©\n",
      "‚ë£ Ï†ú1Ìï≠Í≥º Ï†ú3Ìï≠Ïùò ÏãúÌñâÏóê ÌïÑÏöîÌïú ÏÇ¨Ìï≠ÏùÄ ÎåÄÌÜµÎ†πÎ†πÏúºÎ°ú Ï†ïÌïúÎã§.\n",
      " \n",
      "Ï†ú59Ï°∞(Í∑ºÎ°úÏãúÍ∞Ñ Î∞è Ìú¥Í≤åÏãúÍ∞ÑÏùò ÌäπÎ°Ä) ‚ë† „ÄåÌÜµÍ≥ÑÎ≤ï„Äç Ï†ú22Ï°∞Ï†ú1Ìï≠Ïóê Îî∞Îùº ÌÜµÍ≥ÑÏ≤≠Ïû•Ïù¥ Í≥†ÏãúÌïòÎäî ÏÇ∞ÏóÖÏóê Í¥Ä...\n",
      "----------------------------------------\n",
      "Í≤∞Í≥º 5: ÌïúÎã§.\n",
      " \n",
      "Ï†ú21Ï°∞(Ï†ÑÏ∞®Í∏à ÏÉÅÍ≥ÑÏùò Í∏àÏßÄ) ÏÇ¨Ïö©ÏûêÎäî Ï†ÑÏ∞®Í∏à(ÂâçÂÄüÔ§ä)Ïù¥ÎÇò Í∑∏ Î∞ñÏóê Í∑ºÎ°úÌï† Í≤ÉÏùÑ Ï°∞Í±¥ÏúºÎ°ú ÌïòÎäî Ï†ÑÎåÄ(ÂâçË≤∏)Ï±ÑÍ∂åÍ≥º ÏûÑÍ∏à\n",
      "ÏùÑ ÏÉÅÍ≥ÑÌïòÏßÄ Î™ªÌïúÎã§.\n",
      " \n",
      "Ï†ú22Ï°∞(Í∞ïÏ†ú Ï†ÄÍ∏àÏùò Í∏àÏßÄ) ‚ë† ÏÇ¨Ïö©ÏûêÎäî Í∑ºÎ°úÍ≥ÑÏïΩÏóê ÎçßÎ∂ôÏó¨ Í∞ïÏ†ú Ï†ÄÏ∂ï ÎòêÎäî Ï†ÄÏ∂ïÍ∏àÏùò Í¥ÄÎ¶¨Î•º Í∑úÏ†ïÌïòÎäî Í≥ÑÏïΩÏùÑ Ï≤¥Í≤∞ÌïòÏßÄ\n",
      "Î™ªÌïúÎã§.\n",
      "‚ë° ÏÇ¨Ïö©ÏûêÍ∞Ä Í∑ºÎ°úÏûêÏùò ÏúÑÌÉÅÏúºÎ°ú Ï†ÄÏ∂ïÏùÑ Í¥ÄÎ¶¨ÌïòÎäî Í≤ΩÏö∞ÏóêÎäî Îã§Ïùå Í∞Å Ìò∏Ïùò ÏÇ¨Ìï≠ÏùÑ ÏßÄÏºúÏïº ÌïúÎã§.\n",
      "1. Ï†ÄÏ∂ïÏùò Ï¢ÖÎ•ò„ÜçÍ∏∞Í∞Ñ Î∞è Í∏àÏúµÍ∏∞Í¥ÄÏùÑ Í∑ºÎ°úÏûêÍ∞Ä Í≤∞Ï†ïÌïòÍ≥†, Í∑ºÎ°úÏûê Î≥∏Ïù∏Ïùò Ïù¥Î¶ÑÏúºÎ°ú Ï†ÄÏ∂ïÌï† Í≤É\n",
      "2. Í∑ºÎ°úÏûêÍ∞Ä Ï†ÄÏ∂ïÏ¶ùÏÑú Îì± Í¥ÄÎ†® ÏûêÎ£åÏùò Ïó¥Îûå ÎòêÎäî Î∞òÌôòÏùÑ ÏöîÍµ¨Ìï† ÎïåÏóêÎäî Ï¶âÏãú Ïù¥Ïóê Îî∞Î•º Í≤É\n",
      " \n",
      "Ï†ú23Ï°∞(Ìï¥Í≥† Îì±Ïùò Ï†úÌïú) ‚ë† ÏÇ¨Ïö©ÏûêÎäî Í∑ºÎ°úÏûêÏóêÍ≤å Ï†ïÎãπÌïú Ïù¥Ïú† ÏóÜÏù¥ Ìï¥Í≥†, Ìú¥ÏßÅ, Ï†ïÏßÅ, Ï†ÑÏßÅ, Í∞êÎ¥â, Í∑∏ Î∞ñÏùò ÏßïÎ≤å(Êá≤ÁΩ∞)(Ïù¥\n",
      "Ìïò ‚ÄúÎ∂ÄÎãπÌï¥Í≥†Îì±‚ÄùÏù¥Îùº ÌïúÎã§)ÏùÑ ÌïòÏßÄ Î™ªÌïúÎã§.\n",
      "‚ë° ÏÇ¨Ïö©ÏûêÎäî Í∑ºÎ°úÏûêÍ∞Ä ÏóÖÎ¨¥ÏÉÅ Î∂ÄÏÉÅ ÎòêÎäî ÏßàÎ≥ëÏùò ÏöîÏñëÏùÑ ÏúÑÌïòÏó¨ Ìú¥ÏóÖÌïú Í∏∞Í∞ÑÍ≥º Í∑∏ ÌõÑ 30Ïùº ÎèôÏïà ÎòêÎäî ÏÇ∞Ï†Ñ(Áî£Ââç)„ÜçÏÇ∞...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ\n",
    "query = \"ÌÉÑÎ†• Í∑ºÎ°úÏóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî\"\n",
    "similar_docs = chroma_db.similarity_search(\n",
    "    query=query,\n",
    "    k=5,  # ÏÉÅÏúÑ 5Í∞ú Í≤∞Í≥º\n",
    "    filter={\"source\": \"./data/labor_law.pdf\"}  # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌïÑÌÑ∞\n",
    ")\n",
    "\n",
    "print(f\"Í≤ÄÏÉâ Í≤∞Í≥º Ïàò: {len(similar_docs)}\")\n",
    "for i, doc in enumerate(similar_docs):\n",
    "    print(f\"Í≤∞Í≥º {i+1}: {doc.page_content[:500]}...\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3be3abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï†êÏàò: 0.6994\n",
      "ÎÇ¥Ïö©: [Ï†úÎ™©Í∞úÏ†ï 2021. 5. 18.]\n",
      " \n",
      "Ï†ú49Ï°∞(ÏûÑÍ∏àÏùò ÏãúÌö®) Ïù¥ Î≤ïÏóê Îî∞Î•∏ ÏûÑÍ∏àÏ±ÑÍ∂åÏùÄ 3ÎÖÑÍ∞Ñ ÌñâÏÇ¨ÌïòÏßÄ ÏïÑÎãàÌïòÎ©¥ ÏãúÌö®Î°ú ÏÜåÎ©∏ÌïúÎã§.\n",
      " \n",
      "       Ï†ú4Ïû• Í∑ºÎ°úÏãúÍ∞ÑÍ≥º Ìú¥Ïãù\n",
      " \n",
      "Ï†ú50...\n",
      "--------------------------------------------------\n",
      "Ï†êÏàò: 0.7028\n",
      "ÎÇ¥Ïö©: Î≤ïÏ†úÏ≤ò                                                            9                                    ...\n",
      "--------------------------------------------------\n",
      "Ï†êÏàò: 0.7457\n",
      "ÎÇ¥Ïö©: ÎèôÏïàÏùò ÏÜåÏ†ïÍ∑ºÎ°úÏãúÍ∞ÑÏóê ÎπÑÌïòÏó¨ ÏßßÏùÄ Í∑ºÎ°úÏûêÎ•º ÎßêÌïúÎã§.\n",
      "‚ë° Ï†ú1Ìï≠Ï†ú6Ìò∏Ïóê Îî∞Îùº ÏÇ∞Ï∂úÎêú Í∏àÏï°Ïù¥ Í∑∏ Í∑ºÎ°úÏûêÏùò ÌÜµÏÉÅÏûÑÍ∏àÎ≥¥Îã§ Ï†ÅÏúºÎ©¥ Í∑∏ ÌÜµÏÉÅÏûÑÍ∏àÏï°ÏùÑ ÌèâÍ∑†ÏûÑÍ∏àÏúºÎ°ú ÌïúÎã§.\n",
      " \n",
      "Ï†ú3Ï°∞(Í∑ºÎ°úÏ°∞Í±¥Ïùò ...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2. Ï†êÏàòÏôÄ Ìï®Íªò Í≤ÄÏÉâ (Ïú†ÏÇ¨ÎèÑ Ï†êÏàò Ìè¨Ìï®)\n",
    "docs_with_scores = chroma_db.similarity_search_with_score(query, k=3)\n",
    "\n",
    "for doc, score in docs_with_scores:\n",
    "    print(f\"Ï†êÏàò: {score:.4f}\")\n",
    "    print(f\"ÎÇ¥Ïö©: {doc.page_content[:100]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4157b480",
   "metadata": {},
   "source": [
    "### 2. FAISS Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå\n",
    "\n",
    "#### ‚ö° FAISS ÏÑ§Ïπò Î∞è ÏÇ¨Ïö©\n",
    "```bash\n",
    "pip install faiss-cpu  # CPU Î≤ÑÏ†Ñ\n",
    "# pip install faiss-gpu  # GPU Î≤ÑÏ†Ñ\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c165e645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c54d957d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Í≤ÄÏÉâ Í≤∞Í≥º Ïàò: 5\n",
      "Í≤∞Í≥º 1: [Ï†úÎ™©Í∞úÏ†ï 2021. 5. 18.]\n",
      " \n",
      "Ï†ú49Ï°∞(ÏûÑÍ∏àÏùò ÏãúÌö®) Ïù¥ Î≤ïÏóê Îî∞Î•∏ ÏûÑÍ∏àÏ±ÑÍ∂åÏùÄ 3ÎÖÑÍ∞Ñ ÌñâÏÇ¨ÌïòÏßÄ ÏïÑÎãàÌïòÎ©¥ ÏãúÌö®Î°ú ÏÜåÎ©∏ÌïúÎã§.\n",
      " \n",
      "       Ï†ú4Ïû• Í∑ºÎ°úÏãúÍ∞ÑÍ≥º Ìú¥Ïãù\n",
      " \n",
      "Ï†ú50...\n",
      "----------------------------------------\n",
      "Í≤∞Í≥º 2: Î≤ïÏ†úÏ≤ò                                                            9                                    ...\n",
      "----------------------------------------\n",
      "Í≤∞Í≥º 3: ÎèôÏïàÏùò ÏÜåÏ†ïÍ∑ºÎ°úÏãúÍ∞ÑÏóê ÎπÑÌïòÏó¨ ÏßßÏùÄ Í∑ºÎ°úÏûêÎ•º ÎßêÌïúÎã§.\n",
      "‚ë° Ï†ú1Ìï≠Ï†ú6Ìò∏Ïóê Îî∞Îùº ÏÇ∞Ï∂úÎêú Í∏àÏï°Ïù¥ Í∑∏ Í∑ºÎ°úÏûêÏùò ÌÜµÏÉÅÏûÑÍ∏àÎ≥¥Îã§ Ï†ÅÏúºÎ©¥ Í∑∏ ÌÜµÏÉÅÏûÑÍ∏àÏï°ÏùÑ ÌèâÍ∑†ÏûÑÍ∏àÏúºÎ°ú ÌïúÎã§.\n",
      " \n",
      "Ï†ú3Ï°∞(Í∑ºÎ°úÏ°∞Í±¥Ïùò ...\n",
      "----------------------------------------\n",
      "Í≤∞Í≥º 4: ÌÜµÏÉÅÏ†ÅÏúºÎ°ú ÏÜåÏ†ïÍ∑ºÎ°úÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌïòÏó¨ Í∑ºÎ°úÌï† ÌïÑÏöîÍ∞Ä ÏûàÎäî Í≤ΩÏö∞ÏóêÎäî Í∑∏ ÏóÖÎ¨¥Ïùò ÏàòÌñâÏóê ÌÜµÏÉÅ ÌïÑÏöîÌïú ÏãúÍ∞ÑÏùÑ Í∑ºÎ°úÌïú\n",
      "Í≤ÉÏúºÎ°ú Î≥∏Îã§.\n",
      "‚ë° Ï†ú1Ìï≠ Îã®ÏÑúÏóêÎèÑ Î∂àÍµ¨ÌïòÍ≥† Í∑∏ ÏóÖÎ¨¥Ïóê Í¥ÄÌïòÏó¨ Í∑ºÎ°úÏûêÎåÄÌëúÏôÄ...\n",
      "----------------------------------------\n",
      "Í≤∞Í≥º 5: ÌïúÎã§.\n",
      " \n",
      "Ï†ú21Ï°∞(Ï†ÑÏ∞®Í∏à ÏÉÅÍ≥ÑÏùò Í∏àÏßÄ) ÏÇ¨Ïö©ÏûêÎäî Ï†ÑÏ∞®Í∏à(ÂâçÂÄüÔ§ä)Ïù¥ÎÇò Í∑∏ Î∞ñÏóê Í∑ºÎ°úÌï† Í≤ÉÏùÑ Ï°∞Í±¥ÏúºÎ°ú ÌïòÎäî Ï†ÑÎåÄ(ÂâçË≤∏)Ï±ÑÍ∂åÍ≥º ÏûÑÍ∏à\n",
      "ÏùÑ ÏÉÅÍ≥ÑÌïòÏßÄ Î™ªÌïúÎã§.\n",
      " \n",
      "Ï†ú22Ï°∞(Í∞ïÏ†ú Ï†ÄÍ∏àÏùò Í∏àÏßÄ...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# FAISS Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå ÏÉùÏÑ±\n",
    "faiss_db = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Î°úÏª¨ Ï†ÄÏû•\n",
    "faiss_db.save_local(\"./faiss_index\")\n",
    "\n",
    "# Î°úÎìú\n",
    "faiss_db = FAISS.load_local(\n",
    "    \"./faiss_index\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Í≤ÄÏÉâ\n",
    "similar_docs = faiss_db.similarity_search(query, k=5)\n",
    "\n",
    "print(f\"Í≤ÄÏÉâ Í≤∞Í≥º Ïàò: {len(similar_docs)}\")\n",
    "for i, doc in enumerate(similar_docs):\n",
    "    print(f\"Í≤∞Í≥º {i+1}: {doc.page_content[:100]}...\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e9d91",
   "metadata": {},
   "source": [
    "### 3. Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå Í≥†Í∏â Í∏∞Îä•\n",
    "\n",
    "#### üéõÔ∏è Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌïÑÌÑ∞ÎßÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dca4ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌïÑÌÑ∞ÎßÅÎêú Í≤ÄÏÉâ Í≤∞Í≥º Ïàò: 5\n",
      "Í≤∞Í≥º 1: ÌÜµÏÉÅÏ†ÅÏúºÎ°ú ÏÜåÏ†ïÍ∑ºÎ°úÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌïòÏó¨ Í∑ºÎ°úÌï† ÌïÑÏöîÍ∞Ä ÏûàÎäî Í≤ΩÏö∞ÏóêÎäî Í∑∏ ÏóÖÎ¨¥Ïùò ÏàòÌñâÏóê ÌÜµÏÉÅ ÌïÑÏöîÌïú ÏãúÍ∞ÑÏùÑ Í∑ºÎ°úÌïú\n",
      "Í≤ÉÏúºÎ°ú Î≥∏Îã§.\n",
      "‚ë° Ï†ú1Ìï≠ Îã®ÏÑúÏóêÎèÑ Î∂àÍµ¨ÌïòÍ≥† Í∑∏ ÏóÖÎ¨¥Ïóê Í¥ÄÌïòÏó¨ Í∑ºÎ°úÏûêÎåÄÌëúÏôÄ...\n",
      "10\n",
      "----------------------------------------\n",
      "Í≤∞Í≥º 2: Ïâ¨Ïö¥ Ï¢ÖÎ•òÏùò Í∑ºÎ°úÎ°ú Ï†ÑÌôòÌïòÏó¨Ïïº ÌïúÎã§.<Í∞úÏ†ï 2012. 2. 1.>\n",
      "‚ë• ÏÇ¨ÏóÖÏ£ºÎäî Ï†ú1Ìï≠Ïóê Îî∞Î•∏ Ï∂úÏÇ∞Ï†ÑÌõÑÌú¥Í∞Ä Ï¢ÖÎ£å ÌõÑÏóêÎäî Ìú¥Í∞Ä Ï†ÑÍ≥º ÎèôÏùºÌïú ÏóÖÎ¨¥ ÎòêÎäî ÎèôÎì±Ìïú ÏàòÏ§ÄÏùò ÏûÑÍ∏àÏùÑ ÏßÄÍ∏âÌïòÎäî\n",
      "ÏßÅ...\n",
      "13\n",
      "----------------------------------------\n",
      "Í≤∞Í≥º 3: Ï†ú59Ï°∞(Í∑ºÎ°úÏãúÍ∞Ñ Î∞è Ìú¥Í≤åÏãúÍ∞ÑÏùò ÌäπÎ°Ä) ‚ë† „ÄåÌÜµÍ≥ÑÎ≤ï„Äç Ï†ú22Ï°∞Ï†ú1Ìï≠Ïóê Îî∞Îùº ÌÜµÍ≥ÑÏ≤≠Ïû•Ïù¥ Í≥†ÏãúÌïòÎäî ÏÇ∞ÏóÖÏóê Í¥ÄÌïú ÌëúÏ§ÄÏùò Ï§ëÎ∂Ñ\n",
      "Î•ò ÎòêÎäî ÏÜåÎ∂ÑÎ•ò Ï§ë Îã§Ïùå Í∞Å Ìò∏Ïùò Ïñ¥Îäê ÌïòÎÇòÏóê Ìï¥ÎãπÌïòÎäî ÏÇ¨ÏóÖ...\n",
      "10\n",
      "----------------------------------------\n",
      "Í≤∞Í≥º 4: Î≤ïÏ†úÏ≤ò                                                            15                                   ...\n",
      "14\n",
      "----------------------------------------\n",
      "Í≤∞Í≥º 5: ÏÑúÎäî ÏïÑÎãà ÎêúÎã§.\n",
      "‚ë¶ Ï†ú2Ìï≠Ïóê Îî∞Îùº ÏßÅÏû• ÎÇ¥ Í¥¥Î°≠Ìûò Î∞úÏÉù ÏÇ¨Ïã§ÏùÑ Ï°∞ÏÇ¨Ìïú ÏÇ¨Îûå, Ï°∞ÏÇ¨ ÎÇ¥Ïö©ÏùÑ Î≥¥Í≥†Î∞õÏùÄ ÏÇ¨Îûå Î∞è Í∑∏ Î∞ñÏóê Ï°∞ÏÇ¨ Í≥ºÏ†ïÏóê Ï∞∏Ïó¨\n",
      "Ìïú ÏÇ¨ÎûåÏùÄ Ìï¥Îãπ Ï°∞ÏÇ¨ Í≥ºÏ†ïÏóêÏÑú ÏïåÍ≤å Îêú ÎπÑÎ∞Ä...\n",
      "14\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Î≥µÌï© ÌïÑÌÑ∞ Ï°∞Í±¥\n",
    "filter_criteria = {\n",
    "    \"$and\": [\n",
    "        {\"source\": {\"$eq\": \"./data/labor_law.pdf\"}},\n",
    "        {\"page\": {\"$gte\": 10}}  # 10ÌéòÏù¥ÏßÄ Ïù¥ÏÉÅ\n",
    "    ]\n",
    "}\n",
    "\n",
    "filtered_results = chroma_db.similarity_search(\n",
    "    query=query,\n",
    "    k=5,\n",
    "    filter=filter_criteria\n",
    ")\n",
    "\n",
    "print(f\"ÌïÑÌÑ∞ÎßÅÎêú Í≤ÄÏÉâ Í≤∞Í≥º Ïàò: {len(filtered_results)}\")\n",
    "for i, doc in enumerate(filtered_results):\n",
    "    print(f\"Í≤∞Í≥º {i+1}: {doc.page_content[:100]}...\")\n",
    "    print(doc.metadata['page'])\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5366f2ff",
   "metadata": {},
   "source": [
    "#### üîÑ Î¨∏ÏÑú ÏóÖÎç∞Ïù¥Ìä∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e9dab7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab922fa2-cb37-4c71-8d0e-4955947ec5a6']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÏÉà Î¨∏ÏÑú Ï∂îÍ∞Ä\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "new_docs = [Document(page_content=\"ÏÉàÎ°úÏö¥ ÎÇ¥Ïö©\", metadata={\"source\": \"new\"})]\n",
    "chroma_db.add_documents(new_docs)\n",
    "\n",
    "# Î¨∏ÏÑú ÏÇ≠Ï†ú (ID Í∏∞Î∞ò)\n",
    "# chroma_db.delete(ids=[\"doc_id_1\", \"doc_id_2\"])\n",
    "\n",
    "# Ï†ÑÏ≤¥ Ïª¨Î†âÏÖò ÏÇ≠Ï†ú\n",
    "# chroma_db.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e64df58",
   "metadata": {},
   "source": [
    "### üéØ Ïã§Ïäµ 4: Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå Íµ¨Ï∂ï ~ 16:42Î∂ÑÍπåÏßÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îã§Ïùå Îã®Í≥ÑÎ°ú ÎÇòÎßåÏùò Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜåÎ•º Íµ¨Ï∂ïÌï¥Î≥¥ÏÑ∏Ïöî:\n",
    "# 1. ÏõπÏóêÏÑú Î¨∏ÏÑú Î°úÎìú\n",
    "# 2. Ï†ÅÏ†àÌïú ÌÅ¨Í∏∞Î°ú Î∂ÑÌï†\n",
    "# 3. ÏûÑÎ≤†Îî© Î∞è Ï†ÄÏû•\n",
    "# 4. Í≤ÄÏÉâ ÌÖåÏä§Ìä∏\n",
    "\n",
    "# Ïó¨Í∏∞Ïóê ÏΩîÎìúÎ•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9528dd45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Í≤ÄÏÉâÍ∏∞ (Retriever)\n",
    "\n",
    "### üéØ RetrieverÎûÄ?\n",
    "Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜåÎ•º Í∏∞Î∞òÏúºÎ°ú ÏÇ¨Ïö©Ïûê ÏßàÏùòÏóê Í∞ÄÏû• Í¥ÄÎ†®ÏÑ± ÎÜíÏùÄ Î¨∏ÏÑúÎ•º Í≤ÄÏÉâÌïòÎäî Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ÏûÖÎãàÎã§.\n",
    "\n",
    "### üìä Í≤ÄÏÉâ Ï†ÑÎûµ ÎπÑÍµê\n",
    "| Ï†ÑÎûµ | ÏÑ§Î™Ö | Ïû•Ï†ê | Îã®Ï†ê | ÏÇ¨Ïö© ÏÇ¨Î°Ä |\n",
    "|------|------|------|------|----------|\n",
    "| similarity | Îã®Ïàú Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ | Îπ†Î¶Ñ, ÏßÅÍ¥ÄÏ†Å | Îã§ÏñëÏÑ± Î∂ÄÏ°± | ÏùºÎ∞òÏ†ÅÏù∏ Í≤ÄÏÉâ |\n",
    "| similarity_score_threshold | ÏûÑÍ≥ÑÍ∞í Í∏∞Î∞ò Í≤ÄÏÉâ | ÌíàÏßà Î≥¥Ïû• | Í≤∞Í≥º Ïàò Î∂àÏïàÏ†ï | Í≥†ÌíàÏßà Í≤∞Í≥º ÌïÑÏöî |\n",
    "| mmr | ÏµúÎåÄ ÌïúÍ≥Ñ Í¥ÄÎ†®ÏÑ± | Îã§ÏñëÏÑ± Ïö∞Ïàò | ÎäêÎ¶º | Ìè¨Í¥ÑÏ†Å Ï†ïÎ≥¥ ÌïÑÏöî |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e572d",
   "metadata": {},
   "source": [
    "### 1. Í∏∞Î≥∏ Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ\n",
    "\n",
    "#### üîç Top-K Í≤ÄÏÉâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f3c1f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Í≤ÄÏÉâÎêú Î¨∏ÏÑú Ïàò: 5\n",
      "Î¨∏ÏÑú 1:\n",
      "ÎÇ¥Ïö©: [Ï†úÎ™©Í∞úÏ†ï 2021. 5. 18.]\n",
      " \n",
      "Ï†ú49Ï°∞(ÏûÑÍ∏àÏùò ÏãúÌö®) Ïù¥ Î≤ïÏóê Îî∞Î•∏ ÏûÑÍ∏àÏ±ÑÍ∂åÏùÄ 3ÎÖÑÍ∞Ñ ÌñâÏÇ¨ÌïòÏßÄ ÏïÑÎãàÌïòÎ©¥ ÏãúÌö®Î°ú ÏÜåÎ©∏ÌïúÎã§.\n",
      " \n",
      "       Ï†ú4Ïû• Í∑ºÎ°úÏãúÍ∞ÑÍ≥º Ìú¥Ïãù\n",
      " \n",
      "Ï†ú50Ï°∞(Í∑ºÎ°úÏãúÍ∞Ñ) ‚ë† 1Ï£º Í∞ÑÏùò Í∑ºÎ°úÏãúÍ∞ÑÏùÄ Ìú¥Í≤åÏãúÍ∞ÑÏùÑ Ï†úÏô∏ÌïòÍ≥† 40ÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌï† Ïàò ÏóÜÎã§.\n",
      "‚ë° 1ÏùºÏùò Í∑ºÎ°úÏãúÍ∞ÑÏùÄ Ìú¥Í≤åÏãúÍ∞ÑÏùÑ Ï†úÏô∏ÌïòÍ≥† 8ÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌï† Ïàò ÏóÜÎã§.\n",
      "‚ë¢ Ï†ú1Ìï≠ Î∞è Ï†ú2Ìï≠Ïóê ...\n",
      "Ï∂úÏ≤ò: ./data/labor_law.pdf\n",
      "--------------------------------------------------\n",
      "Î¨∏ÏÑú 2:\n",
      "ÎÇ¥Ïö©: Î≤ïÏ†úÏ≤ò                                                            9                                                       Íµ≠Í∞ÄÎ≤ïÎ†πÏ†ïÎ≥¥ÏÑºÌÑ∞\n",
      "Í∑ºÎ°úÍ∏∞Ï§ÄÎ≤ï\n",
      "[Ï†úÎ™©Í∞úÏ†ï 2021. 1. 5.]\n",
      " \n",
      "Ï†ú51Ï°∞Ïùò2(3Í∞úÏõîÏùÑ Ï¥àÍ≥ºÌïòÎäî ÌÉÑÎ†•Ï†Å Í∑ºÎ°úÏãúÍ∞ÑÏ†ú) ‚ë† ÏÇ¨Ïö©ÏûêÎäî Í∑ºÎ°úÏûêÎåÄÌëúÏôÄÏùò ÏÑúÎ©¥...\n",
      "Ï∂úÏ≤ò: ./data/labor_law.pdf\n",
      "--------------------------------------------------\n",
      "Î¨∏ÏÑú 3:\n",
      "ÎÇ¥Ïö©: ÎèôÏïàÏùò ÏÜåÏ†ïÍ∑ºÎ°úÏãúÍ∞ÑÏóê ÎπÑÌïòÏó¨ ÏßßÏùÄ Í∑ºÎ°úÏûêÎ•º ÎßêÌïúÎã§.\n",
      "‚ë° Ï†ú1Ìï≠Ï†ú6Ìò∏Ïóê Îî∞Îùº ÏÇ∞Ï∂úÎêú Í∏àÏï°Ïù¥ Í∑∏ Í∑ºÎ°úÏûêÏùò ÌÜµÏÉÅÏûÑÍ∏àÎ≥¥Îã§ Ï†ÅÏúºÎ©¥ Í∑∏ ÌÜµÏÉÅÏûÑÍ∏àÏï°ÏùÑ ÌèâÍ∑†ÏûÑÍ∏àÏúºÎ°ú ÌïúÎã§.\n",
      " \n",
      "Ï†ú3Ï°∞(Í∑ºÎ°úÏ°∞Í±¥Ïùò Í∏∞Ï§Ä) Ïù¥ Î≤ïÏóêÏÑú Ï†ïÌïòÎäî Í∑ºÎ°úÏ°∞Í±¥ÏùÄ ÏµúÏ†ÄÍ∏∞Ï§ÄÏù¥ÎØÄÎ°ú Í∑ºÎ°ú Í¥ÄÍ≥Ñ ÎãπÏÇ¨ÏûêÎäî Ïù¥ Í∏∞Ï§ÄÏùÑ Ïù¥Ïú†Î°ú Í∑ºÎ°úÏ°∞Í±¥\n",
      "ÏùÑ ÎÇÆÏ∂ú Ïàò ÏóÜÎã§.\n",
      " \n",
      "Ï†ú4Ï°∞(Í∑ºÎ°úÏ°∞Í±¥Ïùò Í≤∞Ï†ï) Í∑ºÎ°úÏ°∞Í±¥ÏùÄ Í∑ºÎ°úÏûêÏôÄ ÏÇ¨Ïö©ÏûêÍ∞Ä ÎèôÎì±Ìïú...\n",
      "Ï∂úÏ≤ò: ./data/labor_law.pdf\n",
      "--------------------------------------------------\n",
      "Î¨∏ÏÑú 4:\n",
      "ÎÇ¥Ïö©: ÌÜµÏÉÅÏ†ÅÏúºÎ°ú ÏÜåÏ†ïÍ∑ºÎ°úÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌïòÏó¨ Í∑ºÎ°úÌï† ÌïÑÏöîÍ∞Ä ÏûàÎäî Í≤ΩÏö∞ÏóêÎäî Í∑∏ ÏóÖÎ¨¥Ïùò ÏàòÌñâÏóê ÌÜµÏÉÅ ÌïÑÏöîÌïú ÏãúÍ∞ÑÏùÑ Í∑ºÎ°úÌïú\n",
      "Í≤ÉÏúºÎ°ú Î≥∏Îã§.\n",
      "‚ë° Ï†ú1Ìï≠ Îã®ÏÑúÏóêÎèÑ Î∂àÍµ¨ÌïòÍ≥† Í∑∏ ÏóÖÎ¨¥Ïóê Í¥ÄÌïòÏó¨ Í∑ºÎ°úÏûêÎåÄÌëúÏôÄÏùò ÏÑúÎ©¥ Ìï©ÏùòÎ•º Ìïú Í≤ΩÏö∞ÏóêÎäî Í∑∏ Ìï©ÏùòÏóêÏÑú Ï†ïÌïòÎäî ÏãúÍ∞Ñ\n",
      "ÏùÑ Í∑∏ ÏóÖÎ¨¥Ïùò ÏàòÌñâÏóê ÌÜµÏÉÅ ÌïÑÏöîÌïú ÏãúÍ∞ÑÏúºÎ°ú Î≥∏Îã§.\n",
      "‚ë¢ ÏóÖÎ¨¥Ïùò ÏÑ±ÏßàÏóê ÎπÑÏ∂îÏñ¥ ÏóÖÎ¨¥ ÏàòÌñâ Î∞©Î≤ïÏùÑ Í∑ºÎ°úÏûêÏùò Ïû¨ÎüâÏóê ÏúÑÏûÑÌï† ÌïÑÏöîÍ∞Ä Ïûà...\n",
      "Ï∂úÏ≤ò: ./data/labor_law.pdf\n",
      "--------------------------------------------------\n",
      "Î¨∏ÏÑú 5:\n",
      "ÎÇ¥Ïö©: ÌïúÎã§.\n",
      " \n",
      "Ï†ú21Ï°∞(Ï†ÑÏ∞®Í∏à ÏÉÅÍ≥ÑÏùò Í∏àÏßÄ) ÏÇ¨Ïö©ÏûêÎäî Ï†ÑÏ∞®Í∏à(ÂâçÂÄüÔ§ä)Ïù¥ÎÇò Í∑∏ Î∞ñÏóê Í∑ºÎ°úÌï† Í≤ÉÏùÑ Ï°∞Í±¥ÏúºÎ°ú ÌïòÎäî Ï†ÑÎåÄ(ÂâçË≤∏)Ï±ÑÍ∂åÍ≥º ÏûÑÍ∏à\n",
      "ÏùÑ ÏÉÅÍ≥ÑÌïòÏßÄ Î™ªÌïúÎã§.\n",
      " \n",
      "Ï†ú22Ï°∞(Í∞ïÏ†ú Ï†ÄÍ∏àÏùò Í∏àÏßÄ) ‚ë† ÏÇ¨Ïö©ÏûêÎäî Í∑ºÎ°úÍ≥ÑÏïΩÏóê ÎçßÎ∂ôÏó¨ Í∞ïÏ†ú Ï†ÄÏ∂ï ÎòêÎäî Ï†ÄÏ∂ïÍ∏àÏùò Í¥ÄÎ¶¨Î•º Í∑úÏ†ïÌïòÎäî Í≥ÑÏïΩÏùÑ Ï≤¥Í≤∞ÌïòÏßÄ\n",
      "Î™ªÌïúÎã§.\n",
      "‚ë° ÏÇ¨Ïö©ÏûêÍ∞Ä Í∑ºÎ°úÏûêÏùò ÏúÑÌÉÅÏúºÎ°ú Ï†ÄÏ∂ïÏùÑ Í¥ÄÎ¶¨ÌïòÎäî Í≤ΩÏö∞ÏóêÎäî Îã§Ïùå Í∞Å Ìò∏Ïùò ÏÇ¨Ìï≠ÏùÑ ÏßÄ...\n",
      "Ï∂úÏ≤ò: ./data/labor_law.pdf\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜåÎ•º RetrieverÎ°ú Î≥ÄÌôò\n",
    "retriever = chroma_db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}  # ÏÉÅÏúÑ 5Í∞ú Í≤∞Í≥º\n",
    ")\n",
    "\n",
    "# Í≤ÄÏÉâ Ïã§Ìñâ\n",
    "query = \"ÌÉÑÎ†• Í∑ºÎ°úÏóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"Í≤ÄÏÉâÎêú Î¨∏ÏÑú Ïàò: {len(retrieved_docs)}\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"Î¨∏ÏÑú {i+1}:\")\n",
    "    print(f\"ÎÇ¥Ïö©: {doc.page_content[:200]}...\")\n",
    "    print(f\"Ï∂úÏ≤ò: {doc.metadata.get('source', 'Unknown')}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3d1339",
   "metadata": {},
   "source": [
    "### 2. ÏûÑÍ≥ÑÍ∞í Í∏∞Î∞ò Í≤ÄÏÉâ\n",
    "\n",
    "#### üìè Ï†êÏàò ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6ed0f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î¨∏ÏÑú 1 (Ïú†ÏÇ¨ÎèÑ: 0.3006):\n",
      "[Ï†úÎ™©Í∞úÏ†ï 2021. 5. 18.]\n",
      " \n",
      "Ï†ú49Ï°∞(ÏûÑÍ∏àÏùò ÏãúÌö®) Ïù¥ Î≤ïÏóê Îî∞Î•∏ ÏûÑÍ∏àÏ±ÑÍ∂åÏùÄ 3ÎÖÑÍ∞Ñ ÌñâÏÇ¨ÌïòÏßÄ ÏïÑÎãàÌïòÎ©¥ ÏãúÌö®Î°ú ÏÜåÎ©∏ÌïúÎã§.\n",
      " \n",
      "       Ï†ú4Ïû• Í∑ºÎ°úÏãúÍ∞ÑÍ≥º Ìú¥Ïãù\n",
      " \n",
      "Ï†ú50...\n",
      "\n",
      "Î¨∏ÏÑú 2 (Ïú†ÏÇ¨ÎèÑ: 0.2973):\n",
      "Î≤ïÏ†úÏ≤ò                                                            9                                    ...\n",
      "\n",
      "Î¨∏ÏÑú 3 (Ïú†ÏÇ¨ÎèÑ: 0.2550):\n",
      "ÎèôÏïàÏùò ÏÜåÏ†ïÍ∑ºÎ°úÏãúÍ∞ÑÏóê ÎπÑÌïòÏó¨ ÏßßÏùÄ Í∑ºÎ°úÏûêÎ•º ÎßêÌïúÎã§.\n",
      "‚ë° Ï†ú1Ìï≠Ï†ú6Ìò∏Ïóê Îî∞Îùº ÏÇ∞Ï∂úÎêú Í∏àÏï°Ïù¥ Í∑∏ Í∑ºÎ°úÏûêÏùò ÌÜµÏÉÅÏûÑÍ∏àÎ≥¥Îã§ Ï†ÅÏúºÎ©¥ Í∑∏ ÌÜµÏÉÅÏûÑÍ∏àÏï°ÏùÑ ÌèâÍ∑†ÏûÑÍ∏àÏúºÎ°ú ÌïúÎã§.\n",
      " \n",
      "Ï†ú3Ï°∞(Í∑ºÎ°úÏ°∞Í±¥Ïùò ...\n",
      "\n",
      "Î¨∏ÏÑú 4 (Ïú†ÏÇ¨ÎèÑ: 0.2529):\n",
      "ÌÜµÏÉÅÏ†ÅÏúºÎ°ú ÏÜåÏ†ïÍ∑ºÎ°úÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌïòÏó¨ Í∑ºÎ°úÌï† ÌïÑÏöîÍ∞Ä ÏûàÎäî Í≤ΩÏö∞ÏóêÎäî Í∑∏ ÏóÖÎ¨¥Ïùò ÏàòÌñâÏóê ÌÜµÏÉÅ ÌïÑÏöîÌïú ÏãúÍ∞ÑÏùÑ Í∑ºÎ°úÌïú\n",
      "Í≤ÉÏúºÎ°ú Î≥∏Îã§.\n",
      "‚ë° Ï†ú1Ìï≠ Îã®ÏÑúÏóêÎèÑ Î∂àÍµ¨ÌïòÍ≥† Í∑∏ ÏóÖÎ¨¥Ïóê Í¥ÄÌïòÏó¨ Í∑ºÎ°úÏûêÎåÄÌëúÏôÄ...\n",
      "\n",
      "Î¨∏ÏÑú 5 (Ïú†ÏÇ¨ÎèÑ: 0.2461):\n",
      "ÌïúÎã§.\n",
      " \n",
      "Ï†ú21Ï°∞(Ï†ÑÏ∞®Í∏à ÏÉÅÍ≥ÑÏùò Í∏àÏßÄ) ÏÇ¨Ïö©ÏûêÎäî Ï†ÑÏ∞®Í∏à(ÂâçÂÄüÔ§ä)Ïù¥ÎÇò Í∑∏ Î∞ñÏóê Í∑ºÎ°úÌï† Í≤ÉÏùÑ Ï°∞Í±¥ÏúºÎ°ú ÌïòÎäî Ï†ÑÎåÄ(ÂâçË≤∏)Ï±ÑÍ∂åÍ≥º ÏûÑÍ∏à\n",
      "ÏùÑ ÏÉÅÍ≥ÑÌïòÏßÄ Î™ªÌïúÎã§.\n",
      " \n",
      "Ï†ú22Ï°∞(Í∞ïÏ†ú Ï†ÄÍ∏àÏùò Í∏àÏßÄ...\n",
      "\n",
      "Î¨∏ÏÑú 6 (Ïú†ÏÇ¨ÎèÑ: 0.2332):\n",
      "Ïâ¨Ïö¥ Ï¢ÖÎ•òÏùò Í∑ºÎ°úÎ°ú Ï†ÑÌôòÌïòÏó¨Ïïº ÌïúÎã§.<Í∞úÏ†ï 2012. 2. 1.>\n",
      "‚ë• ÏÇ¨ÏóÖÏ£ºÎäî Ï†ú1Ìï≠Ïóê Îî∞Î•∏ Ï∂úÏÇ∞Ï†ÑÌõÑÌú¥Í∞Ä Ï¢ÖÎ£å ÌõÑÏóêÎäî Ìú¥Í∞Ä Ï†ÑÍ≥º ÎèôÏùºÌïú ÏóÖÎ¨¥ ÎòêÎäî ÎèôÎì±Ìïú ÏàòÏ§ÄÏùò ÏûÑÍ∏àÏùÑ ÏßÄÍ∏âÌïòÎäî\n",
      "ÏßÅ...\n",
      "\n",
      "Î¨∏ÏÑú 7 (Ïú†ÏÇ¨ÎèÑ: 0.2273):\n",
      "[Î≥∏Ï°∞Ïã†ÏÑ§ 2021. 1. 5.]\n",
      " \n",
      "Ï†ú52Ï°∞(ÏÑ†ÌÉùÏ†Å Í∑ºÎ°úÏãúÍ∞ÑÏ†ú) ‚ë† ÏÇ¨Ïö©ÏûêÎäî Ï∑®ÏóÖÍ∑úÏπô(Ï∑®ÏóÖÍ∑úÏπôÏóê Ï§ÄÌïòÎäî Í≤ÉÏùÑ Ìè¨Ìï®ÌïúÎã§)Ïóê Îî∞Îùº ÏóÖÎ¨¥Ïùò ÏãúÏûë Î∞è Ï¢ÖÎ£å ÏãúÍ∞Å\n",
      "ÏùÑ Í∑ºÎ°úÏûêÏùò Í≤∞Ï†ïÏóê Îß°Í∏∞...\n",
      "\n",
      "Î¨∏ÏÑú 8 (Ïú†ÏÇ¨ÎèÑ: 0.2274):\n",
      "Î≤ïÏ†úÏ≤ò                                                            3                                    ...\n",
      "\n",
      "Î¨∏ÏÑú 9 (Ïú†ÏÇ¨ÎèÑ: 0.2259):\n",
      "Î≤ïÏ†úÏ≤ò                                                            1                                    ...\n",
      "\n",
      "Î¨∏ÏÑú 10 (Ïú†ÏÇ¨ÎèÑ: 0.2255):\n",
      "‚ë¢ 4Ï£º ÎèôÏïà(4Ï£º ÎØ∏ÎßåÏúºÎ°ú Í∑ºÎ°úÌïòÎäî Í≤ΩÏö∞ÏóêÎäî Í∑∏ Í∏∞Í∞Ñ)ÏùÑ ÌèâÍ∑†ÌïòÏó¨ 1Ï£º ÎèôÏïàÏùò ÏÜåÏ†ïÍ∑ºÎ°úÏãúÍ∞ÑÏù¥ 15ÏãúÍ∞Ñ ÎØ∏ÎßåÏù∏ Í∑ºÎ°ú\n",
      "ÏûêÏóê ÎåÄÌïòÏó¨Îäî Ï†ú55Ï°∞ÏôÄ Ï†ú60Ï°∞Î•º Ï†ÅÏö©ÌïòÏßÄ ÏïÑÎãàÌïúÎã§.<Í∞úÏ†ï 2...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ïú†ÏÇ¨ÎèÑ Ï†êÏàò ÏûÑÍ≥ÑÍ∞í Í∏∞Î∞ò Í≤ÄÏÉâ\n",
    "threshold_retriever = chroma_db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"score_threshold\": 0.1,  # 0.3 Ïù¥ÏÉÅÏùò Ïú†ÏÇ¨ÎèÑÎßå\n",
    "        \"k\": 10                  # ÏµúÎåÄ 10Í∞úÍπåÏßÄ\n",
    "    }\n",
    ")\n",
    "\n",
    "retrieved_docs = threshold_retriever.invoke(query)\n",
    "\n",
    "# Ïã§Ï†ú Ïú†ÏÇ¨ÎèÑ Ï†êÏàò ÌôïÏù∏\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    # Ïã§Ï†ú Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞\n",
    "    doc_embedding = embeddings.embed_query(doc.page_content)\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]\n",
    "    \n",
    "    print(f\"Î¨∏ÏÑú {i+1} (Ïú†ÏÇ¨ÎèÑ: {similarity:.4f}):\")\n",
    "    print(f\"{doc.page_content[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f5a0c",
   "metadata": {},
   "source": [
    "### 3. MMR (Maximal Marginal Relevance) Í≤ÄÏÉâ\n",
    "\n",
    "#### üéØ Îã§ÏñëÏÑ±ÏùÑ Í≥†Î†§Ìïú Í≤ÄÏÉâ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e66269d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMR Í≤ÄÏÉâ Í≤∞Í≥º:\n",
      "Î¨∏ÏÑú 1: [Ï†úÎ™©Í∞úÏ†ï 2021. 5. 18.]\n",
      " \n",
      "Ï†ú49Ï°∞(ÏûÑÍ∏àÏùò ÏãúÌö®) Ïù¥ Î≤ïÏóê Îî∞Î•∏ ÏûÑÍ∏àÏ±ÑÍ∂åÏùÄ 3ÎÖÑÍ∞Ñ ÌñâÏÇ¨ÌïòÏßÄ ÏïÑÎãàÌïòÎ©¥ ÏãúÌö®Î°ú ÏÜåÎ©∏ÌïúÎã§.\n",
      " \n",
      "       Ï†ú4Ïû• Í∑ºÎ°úÏãúÍ∞ÑÍ≥º Ìú¥Ïãù\n",
      " \n",
      "Ï†ú50Ï°∞(Í∑ºÎ°úÏãúÍ∞Ñ) ‚ë† 1Ï£º Í∞ÑÏùò Í∑ºÎ°úÏãúÍ∞ÑÏùÄ Ìú¥Í≤åÏãúÍ∞ÑÏùÑ Ï†úÏô∏ÌïòÍ≥† 40ÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌï† Ïàò ÏóÜÎã§.\n",
      "‚ë°...\n",
      "\n",
      "Î¨∏ÏÑú 2: Î≤ïÏ†úÏ≤ò                                                            1                                                       Íµ≠Í∞ÄÎ≤ïÎ†πÏ†ïÎ≥¥ÏÑºÌÑ∞\n",
      "Í∑ºÎ°úÍ∏∞Ï§ÄÎ≤ï\n",
      " \n",
      "Í∑ºÎ°úÍ∏∞Ï§ÄÎ≤ï\n",
      "[ÏãúÌñâ 2021...\n",
      "\n",
      "Î¨∏ÏÑú 3: Ï†ú7Ï°∞(Í∞ïÏ†ú Í∑ºÎ°úÏùò Í∏àÏßÄ) ÏÇ¨Ïö©ÏûêÎäî Ìè≠Ìñâ, ÌòëÎ∞ï, Í∞êÍ∏à, Í∑∏ Î∞ñÏóê Ï†ïÏã†ÏÉÅ ÎòêÎäî Ïã†Ï≤¥ÏÉÅÏùò ÏûêÏú†Î•º Î∂ÄÎãπÌïòÍ≤å Íµ¨ÏÜçÌïòÎäî ÏàòÎã®ÏúºÎ°ú\n",
      "Ïç® Í∑ºÎ°úÏûêÏùò ÏûêÏú†ÏùòÏÇ¨Ïóê Ïñ¥Í∏ãÎÇòÎäî Í∑ºÎ°úÎ•º Í∞ïÏöîÌïòÏßÄ Î™ªÌïúÎã§....\n",
      "\n",
      "Î¨∏ÏÑú 4: ÏÉàÎ°úÏö¥ ÎÇ¥Ïö©...\n",
      "\n",
      "Î¨∏ÏÑú 5: ÏÑúÎäî ÏïÑÎãà ÎêúÎã§.\n",
      "‚ë¶ Ï†ú2Ìï≠Ïóê Îî∞Îùº ÏßÅÏû• ÎÇ¥ Í¥¥Î°≠Ìûò Î∞úÏÉù ÏÇ¨Ïã§ÏùÑ Ï°∞ÏÇ¨Ìïú ÏÇ¨Îûå, Ï°∞ÏÇ¨ ÎÇ¥Ïö©ÏùÑ Î≥¥Í≥†Î∞õÏùÄ ÏÇ¨Îûå Î∞è Í∑∏ Î∞ñÏóê Ï°∞ÏÇ¨ Í≥ºÏ†ïÏóê Ï∞∏Ïó¨\n",
      "Ìïú ÏÇ¨ÎûåÏùÄ Ìï¥Îãπ Ï°∞ÏÇ¨ Í≥ºÏ†ïÏóêÏÑú ÏïåÍ≤å Îêú ÎπÑÎ∞ÄÏùÑ ÌîºÌï¥Í∑ºÎ°úÏûêÎì±Ïùò ÏùòÏÇ¨Ïóê Î∞òÌïòÏó¨ Îã§Î•∏ ÏÇ¨ÎûåÏóêÍ≤å ÎàÑÏÑ§ÌïòÏó¨ÏÑúÎäî ÏïÑÎãà Îêú\n",
      "Îã§. Îã§Îßå, Ï°∞ÏÇ¨ÏôÄ Í¥Ä...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MMR Í≤ÄÏÉâ - Í¥ÄÎ†®ÏÑ±Í≥º Îã§ÏñëÏÑ±Ïùò Í∑†Ìòï\n",
    "mmr_retriever = chroma_db.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5,                # ÏµúÏ¢Ö Î∞òÌôòÌï† Î¨∏ÏÑú Ïàò\n",
    "        \"fetch_k\": 20,         # Ï¥àÍ∏∞ ÌõÑÎ≥¥ Î¨∏ÏÑú Ïàò\n",
    "        \"lambda_mult\": 0.5     # Í¥ÄÎ†®ÏÑ± vs Îã§ÏñëÏÑ± (0=ÏµúÎåÄ Îã§ÏñëÏÑ±, 1=ÏµúÎåÄ Í¥ÄÎ†®ÏÑ±)\n",
    "    }\n",
    ")\n",
    "\n",
    "mmr_docs = mmr_retriever.invoke(query)\n",
    "\n",
    "print(\"MMR Í≤ÄÏÉâ Í≤∞Í≥º:\")\n",
    "for i, doc in enumerate(mmr_docs):\n",
    "    print(f\"Î¨∏ÏÑú {i+1}: {doc.page_content[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d898f7d",
   "metadata": {},
   "source": [
    "#### üîß lambda_mult ÌååÎùºÎØ∏ÌÑ∞ Ïã§Ìóò\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îã§ÏñëÌïú lambda_mult Í∞íÏúºÎ°ú Ïã§Ìóò\n",
    "lambda_values = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "for lambda_val in lambda_values:\n",
    "    retriever = chroma_db.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": 3,\n",
    "            \"fetch_k\": 10,\n",
    "            \"lambda_mult\": lambda_val\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    docs = retriever.invoke(query)\n",
    "    print(f\"\\nLambda {lambda_val} Í≤∞Í≥º:\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"  {i+1}. {doc.page_content[:100]}...\")\n",
    "    \n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40825374",
   "metadata": {},
   "source": [
    "### 4. Í≥†Í∏â Í≤ÄÏÉâ Í∏∞Î≤ï\n",
    "\n",
    "#### üéõÔ∏è Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌïÑÌÑ∞ÎßÅÍ≥º Í≤ÄÏÉâ Í≤∞Ìï©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4e00bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ Í≤∞Í≥º:\n",
      "Î¨∏ÏÑú 1: [Ï†úÎ™©Í∞úÏ†ï 2021. 5. 18.]\n",
      " \n",
      "Ï†ú49Ï°∞(ÏûÑÍ∏àÏùò ÏãúÌö®) Ïù¥ Î≤ïÏóê Îî∞Î•∏ ÏûÑÍ∏àÏ±ÑÍ∂åÏùÄ 3ÎÖÑÍ∞Ñ ÌñâÏÇ¨ÌïòÏßÄ ÏïÑÎãàÌïòÎ©¥ ÏãúÌö®Î°ú ÏÜåÎ©∏ÌïúÎã§.\n",
      " \n",
      "       Ï†ú4Ïû• Í∑ºÎ°úÏãúÍ∞ÑÍ≥º Ìú¥Ïãù\n",
      " \n",
      "Ï†ú50Ï°∞(Í∑ºÎ°úÏãúÍ∞Ñ) ‚ë† 1Ï£º Í∞ÑÏùò Í∑ºÎ°úÏãúÍ∞ÑÏùÄ Ìú¥Í≤åÏãúÍ∞ÑÏùÑ Ï†úÏô∏ÌïòÍ≥† 40ÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌï† Ïàò ÏóÜÎã§.\n",
      "‚ë°...\n",
      "\n",
      "Î¨∏ÏÑú 2: Î≤ïÏ†úÏ≤ò                                                            9                                                       Íµ≠Í∞ÄÎ≤ïÎ†πÏ†ïÎ≥¥ÏÑºÌÑ∞\n",
      "Í∑ºÎ°úÍ∏∞Ï§ÄÎ≤ï\n",
      "[Ï†úÎ™©Í∞úÏ†ï 2021. 1. 5...\n",
      "\n",
      "Î¨∏ÏÑú 3: ÎèôÏïàÏùò ÏÜåÏ†ïÍ∑ºÎ°úÏãúÍ∞ÑÏóê ÎπÑÌïòÏó¨ ÏßßÏùÄ Í∑ºÎ°úÏûêÎ•º ÎßêÌïúÎã§.\n",
      "‚ë° Ï†ú1Ìï≠Ï†ú6Ìò∏Ïóê Îî∞Îùº ÏÇ∞Ï∂úÎêú Í∏àÏï°Ïù¥ Í∑∏ Í∑ºÎ°úÏûêÏùò ÌÜµÏÉÅÏûÑÍ∏àÎ≥¥Îã§ Ï†ÅÏúºÎ©¥ Í∑∏ ÌÜµÏÉÅÏûÑÍ∏àÏï°ÏùÑ ÌèâÍ∑†ÏûÑÍ∏àÏúºÎ°ú ÌïúÎã§.\n",
      " \n",
      "Ï†ú3Ï°∞(Í∑ºÎ°úÏ°∞Í±¥Ïùò Í∏∞Ï§Ä) Ïù¥ Î≤ïÏóêÏÑú Ï†ïÌïòÎäî Í∑ºÎ°úÏ°∞Í±¥ÏùÄ ÏµúÏ†ÄÍ∏∞Ï§ÄÏù¥ÎØÄÎ°ú Í∑ºÎ°ú Í¥ÄÍ≥Ñ ÎãπÏÇ¨ÏûêÎäî Ïù¥ Í∏∞Ï§ÄÏùÑ Ïù¥Ïú†Î°ú Í∑º...\n",
      "\n",
      "Î¨∏ÏÑú 4: ÌÜµÏÉÅÏ†ÅÏúºÎ°ú ÏÜåÏ†ïÍ∑ºÎ°úÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌïòÏó¨ Í∑ºÎ°úÌï† ÌïÑÏöîÍ∞Ä ÏûàÎäî Í≤ΩÏö∞ÏóêÎäî Í∑∏ ÏóÖÎ¨¥Ïùò ÏàòÌñâÏóê ÌÜµÏÉÅ ÌïÑÏöîÌïú ÏãúÍ∞ÑÏùÑ Í∑ºÎ°úÌïú\n",
      "Í≤ÉÏúºÎ°ú Î≥∏Îã§.\n",
      "‚ë° Ï†ú1Ìï≠ Îã®ÏÑúÏóêÎèÑ Î∂àÍµ¨ÌïòÍ≥† Í∑∏ ÏóÖÎ¨¥Ïóê Í¥ÄÌïòÏó¨ Í∑ºÎ°úÏûêÎåÄÌëúÏôÄÏùò ÏÑúÎ©¥ Ìï©ÏùòÎ•º Ìïú Í≤ΩÏö∞ÏóêÎäî Í∑∏ Ìï©ÏùòÏóêÏÑú Ï†ïÌïòÎäî ÏãúÍ∞Ñ\n",
      "ÏùÑ Í∑∏ ÏóÖÎ¨¥Ïùò ÏàòÌñâÏóê ÌÜµÏÉÅ ÌïÑÏöîÌïú Ïãú...\n",
      "\n",
      "Î¨∏ÏÑú 5: ÌïúÎã§.\n",
      " \n",
      "Ï†ú21Ï°∞(Ï†ÑÏ∞®Í∏à ÏÉÅÍ≥ÑÏùò Í∏àÏßÄ) ÏÇ¨Ïö©ÏûêÎäî Ï†ÑÏ∞®Í∏à(ÂâçÂÄüÔ§ä)Ïù¥ÎÇò Í∑∏ Î∞ñÏóê Í∑ºÎ°úÌï† Í≤ÉÏùÑ Ï°∞Í±¥ÏúºÎ°ú ÌïòÎäî Ï†ÑÎåÄ(ÂâçË≤∏)Ï±ÑÍ∂åÍ≥º ÏûÑÍ∏à\n",
      "ÏùÑ ÏÉÅÍ≥ÑÌïòÏßÄ Î™ªÌïúÎã§.\n",
      " \n",
      "Ï†ú22Ï°∞(Í∞ïÏ†ú Ï†ÄÍ∏àÏùò Í∏àÏßÄ) ‚ë† ÏÇ¨Ïö©ÏûêÎäî Í∑ºÎ°úÍ≥ÑÏïΩÏóê ÎçßÎ∂ôÏó¨ Í∞ïÏ†ú Ï†ÄÏ∂ï ÎòêÎäî Ï†ÄÏ∂ïÍ∏àÏùò Í¥ÄÎ¶¨Î•º Í∑úÏ†ïÌïòÎäî Í≥ÑÏïΩÏùÑ Ï≤¥Í≤∞ÌïòÏßÄ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ retriever\n",
    "filtered_retriever = chroma_db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5,\n",
    "        \"filter\": {\n",
    "            \"source\": \"./data/labor_law.pdf\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "filtered_results = filtered_retriever.invoke(query)\n",
    "\n",
    "print(\"Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ Í≤∞Í≥º:\")\n",
    "for i, doc in enumerate(filtered_results):\n",
    "    print(f\"Î¨∏ÏÑú {i+1}: {doc.page_content[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656541aa",
   "metadata": {},
   "source": [
    "#### üîÑ ÎèôÏ†Å Í≤ÄÏÉâ ÌååÎùºÎØ∏ÌÑ∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739cbb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicRetriever:\n",
    "    def __init__(self, vectorstore, embeddings):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.embeddings = embeddings\n",
    "    \n",
    "    def retrieve(self, query, search_type=\"auto\", k=5):\n",
    "        \"\"\"ÏøºÎ¶¨ ÌäπÏÑ±Ïóê Îî∞Îùº ÎèôÏ†ÅÏúºÎ°ú Í≤ÄÏÉâ Ï†ÑÎûµ ÏÑ†ÌÉù\"\"\"\n",
    "        \n",
    "        # ÏøºÎ¶¨ Î≥µÏû°ÎèÑ Î∂ÑÏÑù\n",
    "        query_length = len(query.split())\n",
    "        \n",
    "        if query_length <= 3:\n",
    "            # ÏßßÏùÄ ÏøºÎ¶¨: ÎÜíÏùÄ ÏûÑÍ≥ÑÍ∞í\n",
    "            search_type = \"similarity_score_threshold\"\n",
    "            search_kwargs = {\"score_threshold\": 0.25, \"k\": k}\n",
    "        elif query_length > 10:\n",
    "            # Í∏¥ ÏøºÎ¶¨: MMRÎ°ú Îã§ÏñëÏÑ± ÌôïÎ≥¥\n",
    "            search_type = \"mmr\"\n",
    "            search_kwargs = {\"k\": k, \"fetch_k\": k*3, \"lambda_mult\": 0.3}\n",
    "        else:\n",
    "            # Ï§ëÍ∞Ñ Í∏∏Ïù¥: Í∏∞Î≥∏ Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ\n",
    "            search_type = \"similarity\"\n",
    "            search_kwargs = {\"k\": k}\n",
    "        \n",
    "        retriever = self.vectorstore.as_retriever(\n",
    "            search_type=search_type,\n",
    "            search_kwargs=search_kwargs\n",
    "        )\n",
    "        \n",
    "        return retriever.invoke(query)\n",
    "\n",
    "# ÏÇ¨Ïö© ÏòàÏãú\n",
    "dynamic_retriever = DynamicRetriever(chroma_db, embeddings)\n",
    "\n",
    "queries = [\n",
    "    \"ÌÉÑÎ†•Í∑ºÎ°ú\",  # ÏßßÏùÄ ÏøºÎ¶¨\n",
    "    \"ÌÉÑÎ†• Í∑ºÎ°úÏóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî\",  # Ï§ëÍ∞Ñ ÏøºÎ¶¨\n",
    "    \"ÌÉÑÎ†• Í∑ºÎ°ú Ï†úÎèÑÏùò Ïû•Ï†êÍ≥º Îã®Ï†ê, Í∑∏Î¶¨Í≥† Ïã§Ï†ú Ï†ÅÏö© ÏÇ¨Î°ÄÎ•º Ìè¨Ìï®ÌïòÏó¨ ÏûêÏÑ∏Ìûà ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî\"  # Í∏¥ ÏøºÎ¶¨\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nÏøºÎ¶¨: {query}\")\n",
    "    print(f\"Í∏∏Ïù¥: {len(query.split())} Îã®Ïñ¥\")\n",
    "    docs = dynamic_retriever.retrieve(query)\n",
    "    print(f\"Í≤ÄÏÉâ Í≤∞Í≥º: {len(docs)}Í∞ú Î¨∏ÏÑú\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993e667b",
   "metadata": {},
   "source": [
    "### üéØ Ïã§Ïäµ 5: Í≤ÄÏÉâ Ï†ÑÎûµ ÎπÑÍµê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Í∞ôÏùÄ ÏßàÎ¨∏Ïóê ÎåÄÌï¥ Îã§Î•∏ Í≤ÄÏÉâ Ï†ÑÎûµÎì§Ïùò Í≤∞Í≥ºÎ•º ÎπÑÍµêÌï¥Î≥¥ÏÑ∏Ïöî\n",
    "test_query = \"Í∑ºÎ°úÏãúÍ∞Ñ Îã®Ï∂ïÏóê ÎåÄÌïú Í∑úÏ†ïÏùÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?\"\n",
    "\n",
    "strategies = {\n",
    "    \"similarity\": {\"k\": 5},\n",
    "    \"similarity_score_threshold\": {\"score_threshold\": 0.3, \"k\": 10},\n",
    "    \"mmr\": {\"k\": 5, \"fetch_k\": 15, \"lambda_mult\": 0.5}\n",
    "}\n",
    "\n",
    "# Ïó¨Í∏∞Ïóê ÏΩîÎìúÎ•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5231bd3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RAG Ï≤¥Ïù∏ Íµ¨ÌòÑ\n",
    "\n",
    "### üéØ RAG Ï≤¥Ïù∏Ïù¥ÎûÄ?\n",
    "Í≤ÄÏÉâ(Retrieval)Í≥º ÏÉùÏÑ±(Generation)ÏùÑ Ïó∞Í≤∞ÌïòÏó¨ Ïô∏Î∂Ä ÏßÄÏãùÏùÑ Í∏∞Î∞òÏúºÎ°ú ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±ÌïòÎäî ÌååÏù¥ÌîÑÎùºÏù∏ÏûÖÎãàÎã§.\n",
    "\n",
    "### üîÑ RAG ÏõåÌÅ¨ÌîåÎ°úÏö∞\n",
    "```\n",
    "ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏ ‚Üí Í¥ÄÎ†® Î¨∏ÏÑú Í≤ÄÏÉâ ‚Üí Ïª®ÌÖçÏä§Ìä∏ Íµ¨ÏÑ± ‚Üí LLM ÌîÑÎ°¨ÌîÑÌä∏ ‚Üí ÎãµÎ≥Ä ÏÉùÏÑ±\n",
    "```\n",
    "\n",
    "### 1. ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø ÏÑ§Í≥Ñ\n",
    "\n",
    "#### üìù Í∏∞Î≥∏ RAG ÌîÑÎ°¨ÌîÑÌä∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "883b1a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Í∏∞Î≥∏ RAG ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø\n",
    "basic_template = \"\"\"Ï£ºÏñ¥ÏßÑ Ïª®ÌÖçÏä§Ìä∏Î•º Í∏∞Î∞òÏúºÎ°ú ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÌïòÏÑ∏Ïöî.\n",
    "\n",
    "Ïª®ÌÖçÏä§Ìä∏:\n",
    "{context}\n",
    "\n",
    "ÏßàÎ¨∏: {question}\n",
    "\n",
    "ÎãµÎ≥Ä:\"\"\"\n",
    "\n",
    "basic_prompt = ChatPromptTemplate.from_template(basic_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b3127",
   "metadata": {},
   "source": [
    "#### üé® Í≥†Í∏â RAG ÌîÑÎ°¨ÌîÑÌä∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39c836c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = \"\"\"ÎãπÏã†ÏùÄ Ï†ÑÎ¨∏Ï†ÅÏù∏ Î¨∏ÏÑú Î∂ÑÏÑù AIÏûÖÎãàÎã§. Ï£ºÏñ¥ÏßÑ **Ïª®ÌÖçÏä§Ìä∏**Î•º Î∞îÌÉïÏúºÎ°ú ÏÇ¨Ïö©ÏûêÏùò **ÏßàÎ¨∏**Ïóê Ï†ïÌôïÌïòÍ≥† Ïú†Ïö©Ìïú ÎãµÎ≥ÄÏùÑ Ï†úÍ≥µÌïòÏÑ∏Ïöî.\n",
    "\n",
    "## ÎãµÎ≥Ä ÏßÄÏπ®\n",
    "- Ïª®ÌÖçÏä§Ìä∏Ïóê ÏûàÎäî Ï†ïÎ≥¥ÎßåÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÎãµÎ≥ÄÌïòÏÑ∏Ïöî\n",
    "- ÌôïÏã§ÌïòÏßÄ ÏïäÏùÄ Ï†ïÎ≥¥Îäî \"Î™ÖÌôïÌïòÏßÄ ÏïäÏäµÎãàÎã§\"ÎùºÍ≥† Î™ÖÏãúÌïòÏÑ∏Ïöî\n",
    "- ÎãµÎ≥ÄÏùÄ ÎÖºÎ¶¨Ï†ÅÏù¥Í≥† Íµ¨Ï°∞ÌôîÎêú ÌòïÌÉúÎ°ú Ï†úÍ≥µÌïòÏÑ∏Ïöî\n",
    "- Í∞ÄÎä•Ìïú Í≤ΩÏö∞ Íµ¨Ï≤¥Ï†ÅÏù∏ ÏòàÏãúÎÇò ÏàòÏπòÎ•º Ìè¨Ìï®ÌïòÏÑ∏Ïöî\n",
    "- Ï∂úÏ≤òÎ•º ÌëúÏãúÌïòÏÑ∏Ïöî\n",
    "\n",
    "## ÎãµÎ≥Ä ÌòïÏãù\n",
    "**ÌïµÏã¨ ÎãµÎ≥Ä:** (ÏßàÎ¨∏Ïóê ÎåÄÌïú ÏßÅÏ†ëÏ†ÅÏù∏ ÎãµÎ≥Ä)\n",
    "\n",
    "**ÏÑ∏Î∂Ä ÏÑ§Î™Ö:** (Ï∂îÍ∞ÄÏ†ÅÏù∏ ÏÑ§Î™ÖÏù¥ÎÇò Î∞∞Í≤Ω Ï†ïÎ≥¥)\n",
    "\n",
    "**Í¥ÄÎ†® Ï†ïÎ≥¥:** (Ïª®ÌÖçÏä§Ìä∏ÏóêÏÑú Î∞úÍ≤¨Îêú Ïó∞Í¥Ä Ï†ïÎ≥¥)\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "## Ïª®ÌÖçÏä§Ìä∏\n",
    "{context}\n",
    "\n",
    "## ÏßàÎ¨∏\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "advanced_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", user_prompt)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cdf3ac",
   "metadata": {},
   "source": [
    "#### üåü ÎèÑÎ©îÏù∏Î≥Ñ ÌäπÌôî ÌîÑÎ°¨ÌîÑÌä∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_template = \"\"\"ÎãπÏã†ÏùÄ Î≤ïÎ•† Î¨∏ÏÑú Ï†ÑÎ¨∏ AIÏûÖÎãàÎã§. Î≤ïÎ•† Ï°∞Ìï≠ÏùÑ Ï†ïÌôïÌûà Ìï¥ÏÑùÌïòÍ≥† ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî.\n",
    "\n",
    "## Î≤ïÎ•† Ìï¥ÏÑù ÏõêÏπô\n",
    "- Ï°∞Î¨∏Ïùò Ï†ïÌôïÌïú Ïù∏Ïö©ÏùÑ Ìè¨Ìï®ÌïòÏÑ∏Ïöî\n",
    "- Î≤ïÏ†Å Ïö©Ïñ¥Îäî ÏùºÎ∞òÏù∏Ïù¥ Ïù¥Ìï¥Ìï† Ïàò ÏûàÎèÑÎ°ù ÏÑ§Î™ÖÌïòÏÑ∏Ïöî\n",
    "- ÏòàÏô∏ Ï°∞Ìï≠Ïù¥ÎÇò Îã®ÏÑúÍ∞Ä ÏûàÎã§Î©¥ Î∞òÎìúÏãú Ïñ∏Í∏âÌïòÏÑ∏Ïöî\n",
    "- Í¥ÄÎ†® Î≤ïÎ†πÏù¥ÎÇò ÏãúÌñâÎ†πÎèÑ Ìï®Íªò Ïñ∏Í∏âÌïòÏÑ∏Ïöî\n",
    "\n",
    "## Í¥ÄÎ†® Î≤ïÎ•† Ï°∞Ìï≠\n",
    "{context}\n",
    "\n",
    "## Î≤ïÎ•† ÏßàÏùò\n",
    "{question}\n",
    "\n",
    "## Î≤ïÎ•† ÎãµÎ≥Ä\n",
    "**Ìï¥Îãπ Ï°∞Ìï≠:** (Í¥ÄÎ†® Î≤ïÎ•† Ï°∞Ìï≠ Ïù∏Ïö©)\n",
    "\n",
    "**Ï°∞Ìï≠ Ìï¥ÏÑù:** (Ï°∞Ìï≠Ïùò ÏùòÎØ∏ÏôÄ Ï†ÅÏö© Î≤îÏúÑ)\n",
    "\n",
    "**Ï£ºÏùòÏÇ¨Ìï≠:** (ÏòàÏô∏ Ï°∞Ìï≠Ïù¥ÎÇò Ï†úÌïú ÏÇ¨Ìï≠)\n",
    "\n",
    "**ÎãµÎ≥Ä:**\"\"\"\n",
    "\n",
    "legal_prompt = ChatPromptTemplate.from_template(legal_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2074b",
   "metadata": {},
   "source": [
    "### 2. LLM ÏÑ§Ï†ï\n",
    "\n",
    "#### ü§ñ Í∏∞Î≥∏ Î™®Îç∏ ÏÑ§Ï†ï\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "befd3154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ÌîÑÎ°úÎçïÏÖòÏö© ÏÑ§Ï†ï\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1\",\n",
    "    temperature=0.1,        # ÏùºÍ¥ÄÏÑ± ÏûàÎäî ÎãµÎ≥Ä\n",
    "    max_tokens=1000,        # ÎãµÎ≥Ä Í∏∏Ïù¥ Ï†úÌïú\n",
    "    top_p=0.9,              # Îã§ÏñëÏÑ± Ï†úÏñ¥\n",
    "    frequency_penalty=0.1,  # Î∞òÎ≥µ Î∞©ÏßÄ\n",
    "    presence_penalty=0.1    # ÏÉàÎ°úÏö¥ Ï£ºÏ†ú Ïû•Î†§\n",
    ")\n",
    "\n",
    "# Îπ†Î•∏ ÏùëÎãµÏö© ÏÑ§Ï†ï\n",
    "fast_llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=500    # ÎãµÎ≥Ä Í∏∏Ïù¥ Ï†úÌïú\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb57167c",
   "metadata": {},
   "source": [
    "#### üîß Î™®Îç∏Î≥Ñ ÏµúÏ†Å ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1059563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï†ïÌôïÏÑ± Ï§ëÏãú (Î≤ïÎ•†, ÏùòÎ£å Îì±)\n",
    "precise_llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0,           # Í∞ÄÏû• ÌôïÏã§Ìïú ÎãµÎ≥Ä\n",
    "    top_p=0.1,              # Î≥¥ÏàòÏ†ÅÏù∏ ÏÑ†ÌÉù\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    ")\n",
    "\n",
    "# Ï∞ΩÏùòÏÑ± Ï§ëÏãú (ÎßàÏºÄÌåÖ, ÏΩòÌÖêÏ∏† Îì±)\n",
    "creative_llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0.7,        # Ï∞ΩÏùòÏ†Å ÎãµÎ≥Ä\n",
    "    top_p=0.95,            # Îã§ÏñëÌïú ÏÑ†ÌÉù\n",
    "    frequency_penalty=0.2,\n",
    "    presence_penalty=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54638f88",
   "metadata": {},
   "source": [
    "### 3. RAG Ï≤¥Ïù∏ Íµ¨ÏÑ±\n",
    "\n",
    "#### üîó Í∏∞Î≥∏ LCEL Ï≤¥Ïù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c379da04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**ÌïµÏã¨ ÎãµÎ≥Ä:**  \n",
      "ÌÉÑÎ†•Ï†Å Í∑ºÎ°úÏãúÍ∞ÑÏ†ú(ÌÉÑÎ†• Í∑ºÎ°ú)Îäî ÏùºÏ†ï Í∏∞Í∞Ñ(2Ï£º~6Í∞úÏõî) ÎèôÏïà Í∑ºÎ°úÏãúÍ∞ÑÏùÑ ÌèâÍ∑†ÌïòÏó¨ Î≤ïÏ†ï Í∑ºÎ°úÏãúÍ∞Ñ(Ï£º 40ÏãúÍ∞Ñ, Ïùº 8ÏãúÍ∞Ñ)ÏùÑ ÎßûÏ∂îÎäî Ï†úÎèÑÏûÖÎãàÎã§. ÌäπÏ†ï Ï£ºÎÇò ÌäπÏ†ïÏùºÏóê Î≤ïÏ†ï Í∑ºÎ°úÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌï¥ ÏùºÌï† Ïàò ÏûàÏßÄÎßå, Ï†ÑÏ≤¥ Îã®ÏúÑÍ∏∞Í∞ÑÏùò ÌèâÍ∑†Ïù¥ Í∏∞Ï§ÄÏùÑ ÎÑòÏßÄ ÏïäÏïÑÏïº Ìï©ÎãàÎã§.\n",
      "\n",
      "**ÏÑ∏Î∂Ä ÏÑ§Î™Ö:**  \n",
      "ÌÉÑÎ†•Ï†Å Í∑ºÎ°úÏãúÍ∞ÑÏ†úÎäî ÏóÖÎ¨¥ÎüâÏù¥ ÌäπÏ†ï ÏãúÍ∏∞Ïóê ÏßëÏ§ëÎêòÎäî Í≤ΩÏö∞ Îì±, Í∑ºÎ°úÏãúÍ∞ÑÏùÑ Ïú†Ïó∞ÌïòÍ≤å Ïö¥ÏòÅÌï† ÌïÑÏöîÍ∞Ä ÏûàÏùÑ Îïå ÌôúÏö©Îê©ÎãàÎã§.  \n",
      "- **3Í∞úÏõî Ïù¥ÎÇ¥ ÌÉÑÎ†•Ï†Å Í∑ºÎ°úÏãúÍ∞ÑÏ†ú(Ï†ú51Ï°∞):**  \n",
      "  - Ï∑®ÏóÖÍ∑úÏπô Îì±Ïóê Îî∞Îùº 2Ï£º Ïù¥ÎÇ¥Ïùò ÏùºÏ†ï Îã®ÏúÑÍ∏∞Í∞ÑÏùÑ ÌèâÍ∑†ÌïòÏó¨ 1Ï£º Í∑ºÎ°úÏãúÍ∞ÑÏù¥ 40ÏãúÍ∞ÑÏùÑ ÎÑòÏßÄ ÏïäÏúºÎ©¥, ÌäπÏ†ï Ï£º¬∑ÏùºÏóê Î≤ïÏ†ï Í∑ºÎ°úÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌï¥ Í∑ºÎ¨¥Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n",
      "- **3Í∞úÏõî Ï¥àÍ≥º~6Í∞úÏõî Ïù¥ÎÇ¥ ÌÉÑÎ†•Ï†Å Í∑ºÎ°úÏãúÍ∞ÑÏ†ú(Ï†ú51Ï°∞Ïùò2):**  \n",
      "  - Í∑ºÎ°úÏûêÎåÄÌëúÏôÄ ÏÑúÎ©¥ Ìï©ÏùòÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.\n",
      "  - Îã®ÏúÑÍ∏∞Í∞Ñ(3~6Í∞úÏõî) ÎèôÏïà Ï£º ÌèâÍ∑† 40ÏãúÍ∞ÑÏùÑ ÎÑòÏßÄ ÏïäÏïÑÏïº ÌïòÎ©∞, ÌäπÏ†ï Ï£ºÎäî 52ÏãúÍ∞Ñ, ÌäπÏ†ï ÏùºÏùÄ 12ÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌï† Ïàò ÏóÜÏäµÎãàÎã§.\n",
      "  - Í∑ºÎ°úÏùº Ï¢ÖÎ£å ÌõÑ Îã§Ïùå Í∑ºÎ°úÏùº ÏãúÏûë Ï†ÑÍπåÏßÄ Ïó∞ÏÜç 11ÏãúÍ∞Ñ Ïù¥ÏÉÅÏùò Ìú¥ÏãùÏãúÍ∞ÑÏùÑ Î≥¥Ïû•Ìï¥Ïïº Ìï©ÎãàÎã§(Î∂àÍ∞ÄÌîºÌïú Í≤ΩÏö∞ Ï†úÏô∏).\n",
      "\n",
      "**Í¥ÄÎ†® Ï†ïÎ≥¥:**  \n",
      "- Ï†ú50Ï°∞: 1Ï£º 40ÏãúÍ∞Ñ, 1Ïùº 8ÏãúÍ∞Ñ Ï¥àÍ≥º Î∂àÍ∞Ä(Ìú¥Í≤åÏãúÍ∞Ñ Ï†úÏô∏)\n",
      "- Ï†ú51Ï°∞: 3Í∞úÏõî Ïù¥ÎÇ¥ ÌÉÑÎ†•Ï†Å Í∑ºÎ°úÏãúÍ∞ÑÏ†ú\n",
      "- Ï†ú51Ï°∞Ïùò2: 3~6Í∞úÏõî ÌÉÑÎ†•Ï†Å Í∑ºÎ°úÏãúÍ∞ÑÏ†ú, Ï£º 52ÏãúÍ∞Ñ/Ïùº 12ÏãúÍ∞Ñ ÌïúÎèÑ, Ïó∞ÏÜç 11ÏãúÍ∞Ñ Ìú¥Ïãù\n",
      "- Îã®ÏúÑÍ∏∞Í∞Ñ ÎÇ¥ ÌèâÍ∑†Ïù¥ Í∏∞Ï§ÄÏùÑ ÎÑòÏßÄ ÏïäÏúºÎ©¥ ÌäπÏ†ï ÏãúÍ∏∞Ïóê ÏßëÏ§ëÍ∑ºÎ¨¥ Í∞ÄÎä•\n",
      "\n",
      "**Ï∂úÏ≤ò:**  \n",
      "- Í∑ºÎ°úÍ∏∞Ï§ÄÎ≤ï Ï†ú50Ï°∞, Ï†ú51Ï°∞, Ï†ú51Ï°∞Ïùò2 (Ïª®ÌÖçÏä§Ìä∏)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"Î¨∏ÏÑú Î¶¨Ïä§Ìä∏Î•º Î¨∏ÏûêÏó¥Î°ú Ìè¨Îß∑\"\"\"\n",
    "    return \"\\n\\n\".join([\n",
    "        f\"{doc.page_content}\" \n",
    "        for i, doc in enumerate(docs)\n",
    "    ])\n",
    "\n",
    "# Í∏∞Î≥∏ RAG Ï≤¥Ïù∏\n",
    "basic_rag_chain = (\n",
    "    RunnableParallel({\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    | advanced_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ÌÖåÏä§Ìä∏\n",
    "query = \"ÌÉÑÎ†• Í∑ºÎ°úÏóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî\"\n",
    "result = basic_rag_chain.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c118a3b",
   "metadata": {},
   "source": [
    "#### üéØ Í≥†Í∏â RAG Ï≤¥Ïù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7536552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÎãµÎ≥Ä: **ÌïµÏã¨ ÎãµÎ≥Ä:**  \n",
      "ÌÉÑÎ†•Ï†Å Í∑ºÎ°úÏãúÍ∞ÑÏ†ú(ÌÉÑÎ†• Í∑ºÎ°ú)Îäî ÏùºÏ†ï Í∏∞Í∞Ñ(2Ï£º~6Í∞úÏõî) ÎèôÏïà Í∑ºÎ°úÏãúÍ∞ÑÏùÑ ÌèâÍ∑†ÌïòÏó¨ Î≤ïÏ†ï Í∑ºÎ°úÏãúÍ∞Ñ(Ï£º 40ÏãúÍ∞Ñ, Ïùº 8ÏãúÍ∞Ñ)ÏùÑ Ï¥àÍ≥ºÌïòÏßÄ ÏïäÎäî Î≤îÏúÑ ÎÇ¥ÏóêÏÑú, ÌäπÏ†ïÌïú ÎÇ†Ïù¥ÎÇò Ï£ºÏóê Í∑ºÎ°úÏãúÍ∞ÑÏùÑ ÌÉÑÎ†•Ï†ÅÏúºÎ°ú Ï°∞Ï†ïÌï† Ïàò ÏûàÎäî Ï†úÎèÑÏûÖÎãàÎã§. 3Í∞úÏõî Ïù¥ÎÇ¥ÏôÄ 3Í∞úÏõî Ï¥àÍ≥º 6Í∞úÏõî Ïù¥ÎÇ¥Ïùò Îëê Í∞ÄÏßÄ Ïú†ÌòïÏù¥ ÏûàÏäµÎãàÎã§.\n",
      "\n",
      "**ÏÑ∏Î∂Ä ÏÑ§Î™Ö:**  \n",
      "1. **3Í∞úÏõî Ïù¥ÎÇ¥Ïùò ÌÉÑÎ†•Ï†Å Í∑ºÎ°úÏãúÍ∞ÑÏ†ú**  \n",
      "   - Ï∑®ÏóÖÍ∑úÏπô Îì±ÏóêÏÑú Ï†ïÌïú Î∞îÏóê Îî∞Îùº 2Ï£º Ïù¥ÎÇ¥Ïùò ÏùºÏ†ï Îã®ÏúÑÍ∏∞Í∞ÑÏùÑ ÌèâÍ∑†ÌïòÏó¨ 1Ï£º Í∑ºÎ°úÏãúÍ∞ÑÏù¥ 40ÏãúÍ∞Ñ(Ï†ú50Ï°∞Ï†ú1Ìï≠)ÏùÑ ÎÑòÏßÄ ÏïäÎäî Î≤îÏúÑÏóêÏÑú, ÌäπÏ†ï Ï£º ÎòêÎäî ÌäπÏ†ïÏùºÏóê Î≤ïÏ†ï Í∑ºÎ°úÏãúÍ∞ÑÏùÑ Ï¥àÍ≥ºÌïòÏó¨ Í∑ºÎ°úÌï† Ïàò ÏûàÏäµÎãàÎã§.  \n",
      "   - (Ï∂úÏ≤ò: Î¨∏ÏÑú 1, p.7)\n",
      "\n",
      "2. **3Í∞úÏõîÏùÑ Ï¥àÍ≥ºÌïòÎäî ÌÉÑÎ†•Ï†Å Í∑ºÎ°úÏãúÍ∞ÑÏ†ú(ÏµúÎåÄ 6Í∞úÏõî)**  \n",
      "   - Í∑ºÎ°úÏûêÎåÄÌëúÏôÄÏùò ÏÑúÎ©¥ Ìï©ÏùòÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.\n",
      "   - 3Í∞úÏõî Ï¥àÍ≥º 6Í∞úÏõî Ïù¥ÎÇ¥Ïùò Îã®ÏúÑÍ∏∞Í∞ÑÏùÑ ÌèâÍ∑†ÌïòÏó¨ 1Ï£º Í∑ºÎ°úÏãúÍ∞ÑÏù¥ 40ÏãúÍ∞ÑÏùÑ ÎÑòÏßÄ ÏïäÎäî Î≤îÏúÑÏóêÏÑú, ÌäπÏ†ï Ï£ºÏóêÎäî ÏµúÎåÄ 52ÏãúÍ∞Ñ, ÌäπÏ†ïÏùºÏóêÎäî ÏµúÎåÄ 12ÏãúÍ∞ÑÍπåÏßÄ Í∑ºÎ°úÌï† Ïàò ÏûàÏäµÎãàÎã§.\n",
      "   - ÏÇ¨Ïö©ÏûêÎäî Í∑ºÎ°úÏùº Ï¢ÖÎ£å ÌõÑ Îã§Ïùå Í∑ºÎ°úÏùº Í∞úÏãú Ï†ÑÍπåÏßÄ Ïó∞ÏÜçÌïòÏó¨ 11ÏãúÍ∞Ñ Ïù¥ÏÉÅÏùò Ìú¥Ïãù ÏãúÍ∞ÑÏùÑ Î∂ÄÏó¨Ìï¥Ïïº Ìï©ÎãàÎã§(Î∂àÍ∞ÄÌîºÌïú Í≤ΩÏö∞ Ï†úÏô∏).\n",
      "   - (Ï∂úÏ≤ò: Î¨∏ÏÑú 2, p.8)\n",
      "\n",
      "**Í¥ÄÎ†® Ï†ïÎ≥¥:**  \n",
      "- ÌÉÑÎ†•Ï†Å Í∑ºÎ°úÏãúÍ∞ÑÏ†úÎäî Ï∑®ÏóÖÍ∑úÏπô ÎòêÎäî Í∑ºÎ°úÏûêÎåÄÌëúÏôÄÏùò ÏÑúÎ©¥ Ìï©Ïùò Îì± ÏùºÏ†ïÌïú Ï†àÏ∞®Î•º Í±∞Ï≥êÏïº ÌïòÎ©∞, Îã®ÏúÑÍ∏∞Í∞Ñ¬∑Í∑ºÎ°úÏãúÍ∞Ñ Îì± Íµ¨Ï≤¥Ï†ÅÏù∏ ÏÇ¨Ìï≠ÏùÑ Î™ÖÌôïÌûà Ï†ïÌï¥Ïïº Ìï©ÎãàÎã§.\n",
      "- Î≤ïÏ†ïÍ∑ºÎ°úÏãúÍ∞Ñ(Ï£º 40ÏãúÍ∞Ñ, Ïùº 8ÏãúÍ∞Ñ)ÏùÄ ÌèâÍ∑† Í∏∞Ï§ÄÏù¥Î©∞, ÌäπÏ†ï ÏãúÍ∏∞Ïóê ÏßëÏ§ëÍ∑ºÎ¨¥Í∞Ä ÌïÑÏöîÌïú ÏóÖÏ¢Ö Îì±Ïóê ÌôúÏö©Îê©ÎãàÎã§.\n",
      "- (Ï∂úÏ≤ò: Î¨∏ÏÑú 1, p.7; Î¨∏ÏÑú 2, p.8)\n",
      "\n",
      "Í≤ÄÏÉâÎêú Î¨∏ÏÑú Ïàò: 5\n",
      "ÌèâÍ∑† Í¥ÄÎ†®ÎèÑ: 0.7298\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "import json\n",
    "\n",
    "class AdvancedRAGChain:\n",
    "    def __init__(self, retriever, llm, prompt_template):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.prompt_template = prompt_template\n",
    "        \n",
    "    def format_docs_with_metadata(self, docs: List) -> str:\n",
    "        \"\"\"Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î•º Ìè¨Ìï®Ìïú Î¨∏ÏÑú Ìè¨Îß∑ÌåÖ\"\"\"\n",
    "        formatted_docs = []\n",
    "        for i, doc in enumerate(docs):\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            page = doc.metadata.get('page', 'N/A')\n",
    "            \n",
    "            formatted_doc = f\"\"\"\n",
    "[Î¨∏ÏÑú {i+1}]\n",
    "Ï∂úÏ≤ò: {source}\n",
    "ÌéòÏù¥ÏßÄ: {page}\n",
    "ÎÇ¥Ïö©: {doc.page_content}\n",
    "\"\"\"\n",
    "            formatted_docs.append(formatted_doc)\n",
    "        \n",
    "        return \"\\n\".join(formatted_docs)\n",
    "    \n",
    "    def retrieve_with_scores(self, query: str, k: int = 5) -> tuple:\n",
    "        \"\"\"Ï†êÏàòÏôÄ Ìï®Íªò Î¨∏ÏÑú Í≤ÄÏÉâ\"\"\"\n",
    "        docs_with_scores = self.retriever.vectorstore.similarity_search_with_score(query, k=k)\n",
    "        docs = [doc for doc, score in docs_with_scores]\n",
    "        scores = [score for doc, score in docs_with_scores]\n",
    "        return docs, scores\n",
    "    \n",
    "    def invoke(self, query: str) -> Dict:\n",
    "        \"\"\"Ìñ•ÏÉÅÎêú RAG Ïã§Ìñâ\"\"\"\n",
    "        # Î¨∏ÏÑú Í≤ÄÏÉâ\n",
    "        docs, scores = self.retrieve_with_scores(query)\n",
    "\n",
    "        # ÏùºÏ†ï Ï†êÏàòÍ∞Ä ÌïòÌïúÏÑ† Ïù¥ÌïòÏù∏ Î¨∏ÏÑúÎäî Ï†úÏô∏\n",
    "        if not docs:\n",
    "            return {\n",
    "                \"answer\": \"Ìï¥Îãπ ÏøºÎ¶¨Ïóê ÎåÄÌïú Í¥ÄÎ†® Î¨∏ÏÑúÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\",\n",
    "                \"source_documents\": [],\n",
    "                \"relevance_scores\": [],\n",
    "                \"context\": \"\"\n",
    "            }   \n",
    "        else:\n",
    "            # Ï†êÏàòÍ∞Ä ÎÇÆÏùÄ Î¨∏ÏÑú Ï†úÏô∏\n",
    "            docs = [doc for doc, score in zip(docs, scores) if score > 0.2]\n",
    "            scores = [score for score in scores if score > 0.2]\n",
    "        \n",
    "        # Ïª®ÌÖçÏä§Ìä∏ Íµ¨ÏÑ±\n",
    "        context = self.format_docs_with_metadata(docs)\n",
    "        \n",
    "        # ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ± Î∞è LLM Ìò∏Ï∂ú\n",
    "        prompt = self.prompt_template.format(context=context, question=query)\n",
    "        response = self.llm.invoke(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"source_documents\": docs,\n",
    "            \"relevance_scores\": scores,\n",
    "            \"context\": context\n",
    "        }\n",
    "\n",
    "# Í≥†Í∏â RAG Ï≤¥Ïù∏ ÏÇ¨Ïö©\n",
    "advanced_rag = AdvancedRAGChain(retriever, llm, advanced_prompt)\n",
    "result = advanced_rag.invoke(\"ÌÉÑÎ†• Í∑ºÎ°úÏóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî\")\n",
    "\n",
    "print(\"ÎãµÎ≥Ä:\", result[\"answer\"])\n",
    "print(f\"\\nÍ≤ÄÏÉâÎêú Î¨∏ÏÑú Ïàò: {len(result['source_documents'])}\")\n",
    "print(f\"ÌèâÍ∑† Í¥ÄÎ†®ÎèÑ: {sum(result['relevance_scores'])/len(result['relevance_scores']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc6522",
   "metadata": {},
   "source": [
    "### üéØ Ïã§Ïäµ 6: ÏôÑÏ†ÑÌïú RAG ÏãúÏä§ÌÖú Íµ¨Ï∂ï      ~ 17:50Î∂ÑÍπåÏßÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac50b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ïã§Ïäµ 1\n",
    "\n",
    "\n",
    "# Îã§Ïùå Ïõπ ÌéòÏù¥ÏßÄÎì§ÏùÑ Î°úÎìúÌïòÍ≥† Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î•º Ï∂úÎ†•Ìï¥Î≥¥ÏÑ∏Ïöî\n",
    "urls = [\n",
    "    \"https://python.langchain.com/docs/tutorials/\",\n",
    "    \"https://python.langchain.com/docs/concepts/\"\n",
    "]\n",
    "\n",
    "# Ïó¨Í∏∞Ïóê ÏΩîÎìúÎ•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî\n",
    "web_loader = WebBaseLoader(web_paths=urls)\n",
    "web_docs = web_loader.load()\n",
    "print(f\"Î°úÎìúÎêú Î¨∏ÏÑú Ïàò: {len(web_docs)}\")\n",
    "print(f\"Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {web_docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b1459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686bb888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îã§Ïùå ÏöîÍµ¨ÏÇ¨Ìï≠ÏùÑ ÎßåÏ°±ÌïòÎäî RAG ÏãúÏä§ÌÖúÏùÑ Íµ¨Ï∂ïÌï¥Î≥¥ÏÑ∏Ïöî:\n",
    "# 1. Ïó¨Îü¨ Î¨∏ÏÑú ÌòïÏãù ÏßÄÏõê (PDF, Ïõπ, ÌÖçÏä§Ìä∏)\n",
    "# 2. ÎèôÏ†Å Í≤ÄÏÉâ Ï†ÑÎûµ ÏÑ†ÌÉù\n",
    "# 3. ÎèÑÎ©îÏù∏Î≥Ñ ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø\n",
    "# 4. ÏùëÎãµ ÌíàÏßà ÌèâÍ∞Ä\n",
    "\n",
    "class ComprehensiveRAGSystem:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def load_documents(self, sources):\n",
    "        \"\"\"Îã§ÏñëÌïú ÏÜåÏä§ÏóêÏÑú Î¨∏ÏÑú Î°úÎìú\"\"\"\n",
    "        # Ïó¨Í∏∞Ïóê ÏΩîÎìúÎ•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî\n",
    "        pass\n",
    "    \n",
    "    def setup_vector_store(self, documents):\n",
    "        \"\"\"Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå Íµ¨ÏÑ±\"\"\"\n",
    "        # Ïó¨Í∏∞Ïóê ÏΩîÎìúÎ•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî\n",
    "        pass\n",
    "    \n",
    "    def query(self, question, domain=\"general\"):\n",
    "        \"\"\"ÎèÑÎ©îÏù∏Î≥Ñ ÏßàÏùòÏùëÎãµ\"\"\"\n",
    "        # Ïó¨Í∏∞Ïóê ÏΩîÎìúÎ•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî\n",
    "        pass\n",
    "\n",
    "# Ïó¨Í∏∞Ïóê Íµ¨ÌòÑ ÏΩîÎìúÎ•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b5f0c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ïã§Ïäµ Î¨∏Ï†ú ÏòàÏãú ÎãµÏïà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c661407",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ïã§Ïäµ 1\n",
    "\n",
    "\n",
    "# Îã§Ïùå Ïõπ ÌéòÏù¥ÏßÄÎì§ÏùÑ Î°úÎìúÌïòÍ≥† Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î•º Ï∂úÎ†•Ìï¥Î≥¥ÏÑ∏Ïöî\n",
    "urls = [\n",
    "    \"https://python.langchain.com/docs/tutorials/\",\n",
    "    \"https://python.langchain.com/docs/concepts/\"\n",
    "]\n",
    "\n",
    "# Ïó¨Í∏∞Ïóê ÏΩîÎìúÎ•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî\n",
    "web_loader = WebBaseLoader(web_paths=urls)\n",
    "web_docs = web_loader.load()\n",
    "print(f\"Î°úÎìúÎêú Î¨∏ÏÑú Ïàò: {len(web_docs)}\")\n",
    "print(f\"Ï≤´ Î≤àÏß∏ Î¨∏ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {web_docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ca86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ïã§Ïäµ 2\n",
    "\n",
    "# Îã§Ïùå ÌÖçÏä§Ìä∏Î•º Îã§ÏñëÌïú Î∞©Î≤ïÏúºÎ°ú Î∂ÑÌï†ÌïòÍ≥† Í≤∞Í≥ºÎ•º ÎπÑÍµêÌï¥Î≥¥ÏÑ∏Ïöî\n",
    "sample_text = \"\"\"\n",
    "Ïù∏Í≥µÏßÄÎä•ÏùÄ ÌòÑÎåÄ Í∏∞Ïà†Ïùò ÌïµÏã¨ÏûÖÎãàÎã§. \n",
    "Î®∏Ïã†Îü¨ÎãùÏùÑ ÌÜµÌï¥ Ïª¥Ìì®ÌÑ∞Îäî ÌïôÏäµÌï† Ïàò ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "Îî•Îü¨ÎãùÏùÄ Ïã†Í≤ΩÎßùÏùÑ Í∏∞Î∞òÏúºÎ°ú Ìï©ÎãàÎã§.\n",
    "ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨Îäî ÌÖçÏä§Ìä∏Î•º Ïù¥Ìï¥ÌïòÎäî Í∏∞Ïà†ÏûÖÎãàÎã§.\n",
    "\n",
    "Ïª¥Ìì®ÌÑ∞ ÎπÑÏ†ÑÏùÄ Ïù¥ÎØ∏ÏßÄÎ•º Î∂ÑÏÑùÌï©ÎãàÎã§.\n",
    "Í∞ïÌôîÌïôÏäµÏùÄ ÌñâÎèôÏùÑ ÌÜµÌï¥ ÌïôÏäµÌï©ÎãàÎã§.\n",
    "\"\"\"\n",
    "\n",
    "# Ïó¨Í∏∞Ïóê ÏΩîÎìúÎ•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "# Í∏∞Î≥∏ Î¨∏Ïûê Îã®ÏúÑ Î∂ÑÌï†Í∏∞\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",  # Îëê Ï§Ñ Îã®ÏúÑÎ°ú Î∂ÑÌï†\n",
    "    chunk_size=100,    # Ï≤≠ÌÅ¨ ÌÅ¨Í∏∞\n",
    "    chunk_overlap=20,  # Ï§ëÎ≥µ ÌÅ¨Í∏∞\n",
    "    length_function=len,  # Í∏∏Ïù¥ Ï∏°Ï†ï Ìï®Ïàò\n",
    "    is_separator_regex=False  # Ï†ïÍ∑úÏãù Ïó¨Î∂Ä\n",
    ")\n",
    "# ÌÖçÏä§Ìä∏ Î∂ÑÌï†\n",
    "char_chunks = char_splitter.split_text(sample_text)\n",
    "print(f\"Î¨∏Ïûê Îã®ÏúÑ Î∂ÑÌï† Í≤∞Í≥º: {len(char_chunks)} Ï≤≠ÌÅ¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ïã§Ïäµ 3\n",
    "\n",
    "# Îã§Ïùå ÏßàÎ¨∏Îì§Ïóê ÎåÄÌï¥ Îã§Î•∏ ÏûÑÎ≤†Îî© Î™®Îç∏Îì§Ïùò Í≤ÄÏÉâ ÏÑ±Îä•ÏùÑ ÎπÑÍµêÌï¥Î≥¥ÏÑ∏Ïöî\n",
    "queries = [\n",
    "    \"Í∏∞Í≥ÑÌïôÏäµÏù¥ÎûÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?\",\n",
    "    \"Ïù¥ÎØ∏ÏßÄ Ïù∏Ïãù Í∏∞Ïà†Ïóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî\",\n",
    "    \"Ïñ∏Ïñ¥ Î™®Îç∏Ïùò ÏûëÎèô ÏõêÎ¶¨Îäî?\"\n",
    "]\n",
    "\n",
    "# Ïó¨Í∏∞Ïóê ÏΩîÎìúÎ•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî\n",
    "def compare_embedding_models(queries, doc_embeddings, documents, embeddings_models):\n",
    "    \"\"\"Ïó¨Îü¨ ÏûÑÎ≤†Îî© Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏøºÎ¶¨ÏôÄ Î¨∏ÏÑú Í∞Ñ Ïú†ÏÇ¨ÎèÑ ÎπÑÍµê\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in embeddings_models.items():\n",
    "        results[model_name] = []\n",
    "        for query in queries:\n",
    "            result = find_most_similar(query, doc_embeddings, documents, model)\n",
    "            results[model_name].append({\n",
    "                \"query\": query,\n",
    "                \"document\": result[\"document\"],\n",
    "                \"similarity\": result[\"similarity\"],\n",
    "                \"index\": result[\"index\"]\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ÏûÑÎ≤†Îî© Î™®Îç∏Îì§ Ï†ïÏùò\n",
    "embeddings_models = {\n",
    "    \"OpenAI\": embeddings_model,\n",
    "    \"BGE-M3\": embeddings_bge,\n",
    "    \"GTE\": embedding_gte,\n",
    "    \"Ollama\": embeddings_ollama\n",
    "}   \n",
    "\n",
    "# Î™®Îç∏ ÎπÑÍµê\n",
    "comparison_results = compare_embedding_models(queries, doc_embeddings, documents, embeddings_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2bc3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ïã§Ïäµ 4\n",
    "\n",
    "# 1. ÏõπÏóêÏÑú Î¨∏ÏÑú Î°úÎìú\n",
    "urls = [\n",
    "    \"https://python.langchain.com/docs/tutorials/\",\n",
    "    \"https://python.langchain.com/docs/concepts/\"\n",
    "]\n",
    "web_loader = WebBaseLoader(web_paths=urls)\n",
    "web_docs = web_loader.load()\n",
    "print(f\"Î°úÎìúÎêú Î¨∏ÏÑú Ïàò: {len(web_docs)}\")\n",
    "\n",
    "# 2. Ï†ÅÏ†àÌïú ÌÅ¨Í∏∞Î°ú Î∂ÑÌï†\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "chunks = text_splitter.split_documents(web_docs)\n",
    "print(f\"Î∂ÑÌï†Îêú Ï≤≠ÌÅ¨ Ïàò: {len(chunks)}\")\n",
    "\n",
    "# 3. ÏûÑÎ≤†Îî© Î∞è Ï†ÄÏû•\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "chroma_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"web_docs\",\n",
    "    persist_directory=\"./web_chroma_db\",\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}  # Ïú†ÏÇ¨ÎèÑ Î©îÌä∏Î¶≠\n",
    ")\n",
    "print(f\"Ï†ÄÏû•Îêú Î¨∏ÏÑú Ïàò: {chroma_db._collection.count()}\")\n",
    "\n",
    "# 4. Í≤ÄÏÉâ ÌÖåÏä§Ìä∏\n",
    "query = \"LangChain ÌäúÌÜ†Î¶¨ÏñºÏóê ÎåÄÌï¥ ÏïåÎ†§Ï£ºÏÑ∏Ïöî\"\n",
    "similar_docs = chroma_db.similarity_search(\n",
    "    query=query,\n",
    "    k=5,  # ÏÉÅÏúÑ 5Í∞ú Í≤∞Í≥º\n",
    ")\n",
    "print(f\"Í≤ÄÏÉâ Í≤∞Í≥º Ïàò: {len(similar_docs)}\")\n",
    "for i, doc in enumerate(similar_docs):\n",
    "    print(f\"Í≤∞Í≥º {i+1}: {doc.page_content[:100]}...\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a35c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ïã§Ïäµ 5\n",
    "\n",
    "# Í∞ôÏùÄ ÏßàÎ¨∏Ïóê ÎåÄÌï¥ Îã§Î•∏ Í≤ÄÏÉâ Ï†ÑÎûµÎì§Ïùò Í≤∞Í≥ºÎ•º ÎπÑÍµêÌï¥Î≥¥ÏÑ∏Ïöî\n",
    "test_query = \"Í∑ºÎ°úÏãúÍ∞Ñ Îã®Ï∂ïÏóê ÎåÄÌïú Í∑úÏ†ïÏùÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?\"\n",
    "\n",
    "strategies = {\n",
    "    \"similarity\": {\"k\": 5},\n",
    "    \"similarity_score_threshold\": {\"score_threshold\": 0.3, \"k\": 10},\n",
    "    \"mmr\": {\"k\": 5, \"fetch_k\": 15, \"lambda_mult\": 0.5}\n",
    "}\n",
    "\n",
    "# Ïó¨Í∏∞Ïóê ÏΩîÎìúÎ•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî\n",
    "for strategy, params in strategies.items():\n",
    "    print(f\"\\nÍ≤ÄÏÉâ Ï†ÑÎûµ: {strategy}\")\n",
    "    retriever = chroma_db.as_retriever(\n",
    "        search_type=strategy,\n",
    "        search_kwargs=params\n",
    "    )\n",
    "    \n",
    "    results = retriever.invoke(test_query)\n",
    "    \n",
    "    print(f\"Í≤ÄÏÉâ Í≤∞Í≥º Ïàò: {len(results)}\")\n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"Î¨∏ÏÑú {i+1}: {doc.page_content[:100]}...\")\n",
    "        print(\"-\" * 40) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7e7ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ïã§Ïäµ 6\n",
    "\n",
    "\n",
    "# Îã§Ïùå ÏöîÍµ¨ÏÇ¨Ìï≠ÏùÑ ÎßåÏ°±ÌïòÎäî RAG ÏãúÏä§ÌÖúÏùÑ Íµ¨Ï∂ïÌï¥Î≥¥ÏÑ∏Ïöî:\n",
    "# 1. Ïó¨Îü¨ Î¨∏ÏÑú ÌòïÏãù ÏßÄÏõê (PDF, Ïõπ, ÌÖçÏä§Ìä∏)\n",
    "# 2. ÎèôÏ†Å Í≤ÄÏÉâ Ï†ÑÎûµ ÏÑ†ÌÉù\n",
    "# 3. ÎèÑÎ©îÏù∏Î≥Ñ ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø\n",
    "# 4. ÏùëÎãµ ÌíàÏßà ÌèâÍ∞Ä\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader, TextLoader\n",
    "\n",
    "class ComprehensiveRAGSystem:\n",
    "    def __init__(self):\n",
    "        \"\"\"RAG ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî\"\"\"\n",
    "        self.documents = []\n",
    "        self.vector_store = None\n",
    "        self.embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        self.retriever = None\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.1)\n",
    "    \n",
    "    def load_documents(self, sources):\n",
    "        \"\"\"Îã§ÏñëÌïú ÏÜåÏä§ÏóêÏÑú Î¨∏ÏÑú Î°úÎìú\"\"\"\n",
    "        for source in sources:\n",
    "            if source.endswith('.pdf'):\n",
    "                loader = PyPDFLoader(source)\n",
    "            elif source.startswith('http'):\n",
    "                loader = WebBaseLoader(web_paths=[source])\n",
    "            else:\n",
    "                loader = TextLoader(source)\n",
    "            \n",
    "            docs = loader.load()\n",
    "            self.documents.extend(docs)\n",
    "    \n",
    "    def setup_vector_store(self, documents):\n",
    "        \"\"\"Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå Íµ¨ÏÑ±\"\"\"\n",
    "        self.vector_store = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=self.embeddings,\n",
    "            collection_name=\"rag_system\"\n",
    "        )\n",
    "    \n",
    "    def query(self, question, domain=\"legal\"):\n",
    "        \"\"\"ÎèÑÎ©îÏù∏Î≥Ñ ÏßàÏùòÏùëÎãµ\"\"\"\n",
    "        self.retriever = self.vector_store.as_retriever()\n",
    "        context = self.retriever.invoke(question)\n",
    "\n",
    "        # ÎèÑÎ©îÏù∏Î≥Ñ ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø ÏÑ†ÌÉù\n",
    "        if domain == \"legal\":\n",
    "            prompt_template = ChatPromptTemplate.from_template(\n",
    "                \"Ï£ºÏñ¥ÏßÑ Ïª®ÌÖçÏä§Ìä∏Î•º Í∏∞Î∞òÏúºÎ°ú Î≤ïÎ•†Í∞Ä Í¥ÄÏ†êÏóêÏÑú ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÌïòÏÑ∏Ïöî.\\n\\n\"\n",
    "                \"Ïª®ÌÖçÏä§Ìä∏:\\n{context}\\n\\n\"\n",
    "                \"ÏßàÎ¨∏: {question}\\n\\n\"\n",
    "                \"ÎãµÎ≥Ä:\"\n",
    "            )\n",
    "        elif domain == \"technical\":\n",
    "            prompt_template = ChatPromptTemplate.from_template(\n",
    "                \"Ï£ºÏñ¥ÏßÑ Ïª®ÌÖçÏä§Ìä∏Î•º Í∏∞Î∞òÏúºÎ°ú Í∏∞Ïà† Ï†ÑÎ¨∏Í∞Ä Í¥ÄÏ†êÏóêÏÑú ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÌïòÏÑ∏Ïöî.\\n\\n\"\n",
    "                \"Ïª®ÌÖçÏä§Ìä∏:\\n{context}\\n\\n\"\n",
    "                \"ÏßàÎ¨∏: {question}\\n\\n\"\n",
    "                \"ÎãµÎ≥Ä:\"\n",
    "            )\n",
    "        else:\n",
    "            prompt_template = ChatPromptTemplate.from_template(\n",
    "                \"Ï£ºÏñ¥ÏßÑ Ïª®ÌÖçÏä§Ìä∏Î•º Í∏∞Î∞òÏúºÎ°ú ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÌïòÏÑ∏Ïöî.\\n\\n\"\n",
    "                \"Ïª®ÌÖçÏä§Ìä∏:\\n{context}\\n\\n\"\n",
    "                \"ÏßàÎ¨∏: {question}\\n\\n\"\n",
    "                \"ÎãµÎ≥Ä:\"\n",
    "            )\n",
    "\n",
    "        # ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ± Î∞è LLM Ìò∏Ï∂ú\n",
    "        prompt = prompt_template.format(context=context, question=question)\n",
    "        response = self.llm.invoke(prompt)\n",
    "        return response\n",
    "\n",
    "# ÏÇ¨Ïö© ÏòàÏãú\n",
    "rag_system = ComprehensiveRAGSystem()\n",
    "rag_system.load_documents([\n",
    "    \"./data/labor_law.pdf\",\n",
    "    \"https://python.langchain.com/docs/tutorials/\",\n",
    "    \"https://python.langchain.com/docs/concepts/\"\n",
    "])\n",
    "rag_system.setup_vector_store(rag_system.documents)\n",
    "\n",
    "result = rag_system.query(\"ÌÉÑÎ†• Í∑ºÎ°úÏóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî\", domain=\"legal\")\n",
    "print(\"ÎãµÎ≥Ä:\", result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d03fd6c",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktds-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
