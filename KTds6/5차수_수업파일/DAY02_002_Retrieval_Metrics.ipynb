{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  정보 검색 평가지표(HitRate, MRR, NDCG)\n",
    "\n",
    "- 정보 검색 시스템의 주요 평가지표 이해\n",
    "- Hit Rate, MRR, MAP, NDCG의 계산 방법 및 의미 파악\n",
    "- 최신 Python 라이브러리를 활용한 실습\n",
    "- RAG 시스템 평가에 적용\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 1. 기본 개념 및 배경\n",
    "\n",
    "### 1.1 정보 검색 평가의 중요성\n",
    "\n",
    "정보 검색(Information Retrieval) 시스템의 성능을 평가하는 것은 다음과 같은 이유로 중요합니다:\n",
    "\n",
    "- **사용자 만족도**: 관련성 높은 문서를 상위에 배치하여 사용자 경험 향상\n",
    "- **시스템 개선**: 정량적 지표를 통한 객관적 성능 비교\n",
    "- **비즈니스 가치**: 검색 품질 향상을 통한 클릭률, 전환율 개선\n",
    "\n",
    "### 1.2 평가 지표의 분류\n",
    "\n",
    "#### 순위 인식 여부 (Rank-Aware vs Rank-Unaware)\n",
    "- **Rank-Unaware**: 검색 결과의 순서를 고려하지 않음 (Precision, Recall)\n",
    "- **Rank-Aware**: 검색 결과의 순서를 고려함 (MRR, MAP, NDCG)\n",
    "\n",
    "#### 평가 방식\n",
    "- **이진 관련성**: 관련 있음(1) 또는 없음(0)\n",
    "- **등급 관련성**: 다단계 점수 (1~5점 등)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 2. 환경 설정\n",
    "\n",
    "### 2.1 라이브러리 임포트\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 라이브러리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# LangChain (RAG 시스템용)\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 최신 평가 라이브러리 소개\n",
    "\n",
    "- **ranx**: 고속 평가 라이브러리 (2022~)\n",
    "    - Numba 기반 고성능 구현\n",
    "    - 통계적 유의성 검정 지원\n",
    "    - LaTeX 테이블 자동 생성\n",
    "\n",
    "- **pytrec_eval**: 표준 평가 도구\n",
    "    - TREC 공식 평가 도구의 Python 인터페이스\n",
    "    - 학술 연구에서 널리 사용\n",
    "\n",
    "- **ir-measures**: 정보 검색 평가 라이브러리\n",
    "    - 다양한 평가 지표 지원\n",
    "    - 사용자 정의 지표 생성 가능\n",
    "\n",
    "- **설치 방법**:\n",
    "\n",
    "    ```python\n",
    "    # 평가 전용 라이브러리\n",
    "    !pip install ranx-k \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 3. 샘플 데이터 준비\n",
    "\n",
    "### 3.1 검색 시나리오 설정\n",
    "\n",
    "고객 서비스 문의 검색 시스템을 평가하는 상황을 가정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 시나리오 설정 완료!\n",
      "총 문서 수: 10\n",
      "평가 쿼리 수: 5\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from textwrap import dedent\n",
    "\n",
    "# 문서 데이터베이스 (전체 10개 문서로 확장)\n",
    "document_pool = [\n",
    "    Document(\n",
    "        page_content=\"배송 지연 문의 - 주문 상품의 배송이 예상보다 늦어지고 있습니다.\",\n",
    "        metadata={\"id\": \"doc1\", \"category\": \"배송\", \"priority\": \"높음\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"결제 오류 - 카드 결제 시 오류가 발생하여 주문이 완료되지 않았습니다.\",\n",
    "        metadata={\"id\": \"doc2\", \"category\": \"결제\", \"priority\": \"높음\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"제품 교환 - 사이즈가 맞지 않아 다른 사이즈로 교환하고 싶습니다.\",\n",
    "        metadata={\"id\": \"doc3\", \"category\": \"교환/반품\", \"priority\": \"중간\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"주문 취소 및 환불 - 주문을 취소하고 전액 환불받고 싶습니다.\",\n",
    "        metadata={\"id\": \"doc4\", \"category\": \"취소/환불\", \"priority\": \"중간\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"포인트 적립 문의 - 구매 후 포인트가 적립되지 않았습니다.\",\n",
    "        metadata={\"id\": \"doc5\", \"category\": \"포인트\", \"priority\": \"낮음\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"재고 문의 - 원하는 상품이 품절인데 언제 재입고되나요?\",\n",
    "        metadata={\"id\": \"doc6\", \"category\": \"재고\", \"priority\": \"중간\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"회원가입 문의 - 회원가입 시 인증번호가 오지 않습니다.\",\n",
    "        metadata={\"id\": \"doc7\", \"category\": \"회원\", \"priority\": \"낮음\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"쿠폰 사용 문의 - 보유한 쿠폰이 적용되지 않습니다.\",\n",
    "        metadata={\"id\": \"doc8\", \"category\": \"쿠폰\", \"priority\": \"낮음\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"배송 주소 변경 - 배송 전에 주소를 변경하고 싶습니다.\",\n",
    "        metadata={\"id\": \"doc9\", \"category\": \"배송\", \"priority\": \"중간\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"상품 리뷰 문의 - 구매한 상품에 대한 리뷰 작성 방법을 알고 싶습니다.\",\n",
    "        metadata={\"id\": \"doc10\", \"category\": \"리뷰\", \"priority\": \"낮음\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# 검색 쿼리와 정답 (현실적인 시나리오)\n",
    "queries_and_ground_truth = [\n",
    "    {\n",
    "        \"query\": \"배송 늦어요\",\n",
    "        \"relevant_docs\": [\"doc1\", \"doc9\"]  # 배송 관련 문서들\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"결제가 안 되고 포인트도 문제가 있어요\", \n",
    "        \"relevant_docs\": [\"doc2\", \"doc5\"]  # 결제 + 포인트 관련\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"주문 취소하고 싶어요\",\n",
    "        \"relevant_docs\": [\"doc4\"]  # 취소 관련만\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"교환 환불 정책이 궁금해요\",\n",
    "        \"relevant_docs\": [\"doc3\", \"doc4\"]  # 교환 + 환불 관련\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"쿠폰이 왜 안 써져요?\",\n",
    "        \"relevant_docs\": [\"doc8\"]  # 쿠폰 관련만\n",
    "    }\n",
    "]\n",
    "\n",
    "# 시스템 검색 결과 (현실적인 성능 차이 반영)\n",
    "system_results = [\n",
    "    # Query 1: \"배송 늦어요\" - 좋은 성능 (정답 1, 2위)\n",
    "    [\"doc1\", \"doc9\", \"doc6\", \"doc2\", \"doc7\"],\n",
    "    \n",
    "    # Query 2: \"결제가 안 되고 포인트도 문제가 있어요\" - 보통 성능 (정답이 2, 4위)\n",
    "    [\"doc7\", \"doc2\", \"doc3\", \"doc5\", \"doc1\"],\n",
    "    \n",
    "    # Query 3: \"주문 취소하고 싶어요\" - 나쁜 성능 (정답이 5위)\n",
    "    [\"doc3\", \"doc6\", \"doc2\", \"doc1\", \"doc4\"],\n",
    "    \n",
    "    # Query 4: \"교환 환불 정책이 궁금해요\" - 혼재된 성능 (정답이 1, 6위)\n",
    "    [\"doc3\", \"doc7\", \"doc5\", \"doc8\", \"doc2\", \"doc4\"],\n",
    "    \n",
    "    # Query 5: \"쿠폰이 왜 안 써져요?\" - 매우 나쁜 성능 (정답이 없음!)\n",
    "    [\"doc5\", \"doc2\", \"doc7\", \"doc1\", \"doc10\"]\n",
    "]\n",
    "\n",
    "print(\"검색 시나리오 설정 완료!\")\n",
    "print(f\"총 문서 수: {len(document_pool)}\")\n",
    "print(f\"평가 쿼리 수: {len(queries_and_ground_truth)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📈 4. 평가 지표 상세 설명\n",
    "\n",
    "### 4.1 Hit Rate (적중률)\n",
    "\n",
    "#### 개념\n",
    "**Hit Rate@k**는 상위 k개 검색 결과에 관련 문서가 **하나라도** 포함되어 있는지 측정하는 지표입니다.\n",
    "\n",
    "#### 특징\n",
    "- **이진 평가**: 있음(1) 또는 없음(0)\n",
    "- **순서 무관**: 관련 문서의 위치는 고려하지 않음\n",
    "- **직관적**: 가장 이해하기 쉬운 지표\n",
    "\n",
    "#### 계산 공식\n",
    "\n",
    "```markdown\n",
    "Hit Rate@k = (적중한 쿼리 수) / (전체 쿼리 수)\n",
    "\n",
    "각 쿼리의 적중 여부 = 1 if 상위 k개에 관련 문서 존재 else 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hit Rate 계산 결과 ===\n",
      "=== Hit Rate@1 계산 과정 ===\n",
      "Query 1: GT=['doc1', 'doc9'], Pred@1=['doc1']\n",
      "         Hit: O\n",
      "Query 2: GT=['doc2', 'doc5'], Pred@1=['doc7']\n",
      "         Hit: X\n",
      "Query 3: GT=['doc4'], Pred@1=['doc3']\n",
      "         Hit: X\n",
      "Query 4: GT=['doc3', 'doc4'], Pred@1=['doc3']\n",
      "         Hit: O\n",
      "Query 5: GT=['doc8'], Pred@1=['doc5']\n",
      "         Hit: X\n",
      "\n",
      "총 적중: 2/5 = 0.400\n",
      "Hit Rate@1: 0.400\n",
      "\n",
      "=== Hit Rate@3 계산 과정 ===\n",
      "Query 1: GT=['doc1', 'doc9'], Pred@3=['doc1', 'doc9', 'doc6']\n",
      "         Hit: O\n",
      "Query 2: GT=['doc2', 'doc5'], Pred@3=['doc7', 'doc2', 'doc3']\n",
      "         Hit: O\n",
      "Query 3: GT=['doc4'], Pred@3=['doc3', 'doc6', 'doc2']\n",
      "         Hit: X\n",
      "Query 4: GT=['doc3', 'doc4'], Pred@3=['doc3', 'doc7', 'doc5']\n",
      "         Hit: O\n",
      "Query 5: GT=['doc8'], Pred@3=['doc5', 'doc2', 'doc7']\n",
      "         Hit: X\n",
      "\n",
      "총 적중: 3/5 = 0.600\n",
      "Hit Rate@3: 0.600\n",
      "\n",
      "=== Hit Rate@5 계산 과정 ===\n",
      "Query 1: GT=['doc1', 'doc9'], Pred@5=['doc1', 'doc9', 'doc6', 'doc2', 'doc7']\n",
      "         Hit: O\n",
      "Query 2: GT=['doc2', 'doc5'], Pred@5=['doc7', 'doc2', 'doc3', 'doc5', 'doc1']\n",
      "         Hit: O\n",
      "Query 3: GT=['doc4'], Pred@5=['doc3', 'doc6', 'doc2', 'doc1', 'doc4']\n",
      "         Hit: O\n",
      "Query 4: GT=['doc3', 'doc4'], Pred@5=['doc3', 'doc7', 'doc5', 'doc8', 'doc2']\n",
      "         Hit: O\n",
      "Query 5: GT=['doc8'], Pred@5=['doc5', 'doc2', 'doc7', 'doc1', 'doc10']\n",
      "         Hit: X\n",
      "\n",
      "총 적중: 4/5 = 0.800\n",
      "Hit Rate@5: 0.800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_hit_rate(ground_truth: List[List[str]], \n",
    "                      predictions: List[List[str]], \n",
    "                      k: int) -> float:\n",
    "    \"\"\"\n",
    "    Hit Rate@k 계산\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    total_queries = len(ground_truth)\n",
    "    \n",
    "    print(f\"=== Hit Rate@{k} 계산 과정 ===\")\n",
    "    \n",
    "    for i, (gt, pred) in enumerate(zip(ground_truth, predictions)):\n",
    "        # 상위 k개 예측 결과\n",
    "        top_k_pred = pred[:k]\n",
    "        \n",
    "        # 관련 문서가 하나라도 있으면 적중\n",
    "        hit = any(doc in gt for doc in top_k_pred)\n",
    "        if hit:\n",
    "            hits += 1\n",
    "        \n",
    "        print(f\"Query {i+1}: GT={gt}, Pred@{k}={top_k_pred}\")\n",
    "        print(f\"         Hit: {'O' if hit else 'X'}\")\n",
    "    \n",
    "    hit_rate = hits / total_queries\n",
    "    print(f\"\\n총 적중: {hits}/{total_queries} = {hit_rate:.3f}\")\n",
    "    return hit_rate\n",
    "\n",
    "# 실제 계산\n",
    "ground_truth = [query[\"relevant_docs\"] for query in queries_and_ground_truth]\n",
    "\n",
    "print(\"=== Hit Rate 계산 결과 ===\")\n",
    "for k in [1, 3, 5]:\n",
    "    hit_rate = calculate_hit_rate(ground_truth, system_results, k)\n",
    "    print(f\"Hit Rate@{k}: {hit_rate:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**예상 출력 분석:**\n",
    "```markdown\n",
    "Hit Rate@1: 0.400  # Query 1, 4만 1위에 정답 (5개 중 2개)\n",
    "Hit Rate@3: 0.600  # Query 1, 2, 4만 상위 3개에 정답 (5개 중 3개)  \n",
    "Hit Rate@5: 0.800  # Query 5만 상위 5개에도 정답 없음 (5개 중 4개)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 MRR (Mean Reciprocal Rank)\n",
    "\n",
    "#### 개념\n",
    "**MRR**은 **첫 번째 관련 문서의 순위**에 기반한 평가 지표입니다. 사용자가 원하는 정보를 얼마나 빨리 찾을 수 있는지를 측정합니다.\n",
    "\n",
    "#### 특징\n",
    "- **순위 고려**: 상위에 있을수록 높은 점수\n",
    "- **첫 번째만**: 첫 관련 문서 이후는 무시\n",
    "- **사용자 경험**: 실제 검색 행동과 유사\n",
    "\n",
    "#### 계산 공식\n",
    "```markdown\n",
    "MRR = (1/Q) × Σ(1/rank_i)\n",
    "\n",
    "여기서:\n",
    "- Q: 전체 쿼리 수\n",
    "- rank_i: i번째 쿼리에서 첫 번째 관련 문서의 순위\n",
    "- 관련 문서가 없으면 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MRR 계산 결과 ===\n",
      "=== MRR 계산 과정 ===\n",
      "Query 1: GT=['doc1', 'doc9']\n",
      "         Pred=['doc1', 'doc9', 'doc6', 'doc2', 'doc7']\n",
      "         첫 관련문서 순위: 1, RR: 1.000\n",
      "Query 2: GT=['doc2', 'doc5']\n",
      "         Pred=['doc7', 'doc2', 'doc3', 'doc5', 'doc1']\n",
      "         첫 관련문서 순위: 2, RR: 0.500\n",
      "Query 3: GT=['doc4']\n",
      "         Pred=['doc3', 'doc6', 'doc2', 'doc1', 'doc4']\n",
      "         첫 관련문서 순위: 5, RR: 0.200\n",
      "Query 4: GT=['doc3', 'doc4']\n",
      "         Pred=['doc3', 'doc7', 'doc5', 'doc8', 'doc2']\n",
      "         첫 관련문서 순위: 1, RR: 1.000\n",
      "Query 5: GT=['doc8']\n",
      "         Pred=['doc5', 'doc2', 'doc7', 'doc1', 'doc10']\n",
      "         첫 관련문서 순위: None, RR: 0.000\n",
      "\n",
      "평균 RR: 2.700 / 5 = 0.540\n",
      "최종 MRR: 0.540\n"
     ]
    }
   ],
   "source": [
    "def calculate_mrr(ground_truth: List[List[str]], \n",
    "                  predictions: List[List[str]], \n",
    "                  k: int = None) -> float:\n",
    "    \"\"\"\n",
    "    Mean Reciprocal Rank 계산\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    print(f\"=== MRR 계산 과정 ===\")\n",
    "    \n",
    "    for i, (gt, pred) in enumerate(zip(ground_truth, predictions)):\n",
    "        # 상위 k개만 고려 (k가 None이면 전체)\n",
    "        search_list = pred[:k] if k else pred\n",
    "        \n",
    "        # 첫 번째 관련 문서의 순위 찾기\n",
    "        first_relevant_rank = None\n",
    "        for rank, doc_id in enumerate(search_list, 1):\n",
    "            if doc_id in gt:\n",
    "                first_relevant_rank = rank\n",
    "                break\n",
    "        \n",
    "        # Reciprocal Rank 계산\n",
    "        rr = 1.0 / first_relevant_rank if first_relevant_rank else 0.0\n",
    "        reciprocal_ranks.append(rr)\n",
    "        \n",
    "        print(f\"Query {i+1}: GT={gt}\")\n",
    "        print(f\"         Pred={search_list[:5]}\")\n",
    "        print(f\"         첫 관련문서 순위: {first_relevant_rank}, RR: {rr:.3f}\")\n",
    "    \n",
    "    mrr = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "    print(f\"\\n평균 RR: {sum(reciprocal_ranks):.3f} / {len(reciprocal_ranks)} = {mrr:.3f}\")\n",
    "    return mrr\n",
    "\n",
    "# 실제 계산\n",
    "print(\"=== MRR 계산 결과 ===\")\n",
    "mrr_score = calculate_mrr(ground_truth, system_results, k=10)\n",
    "print(f\"최종 MRR: {mrr_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**예상 출력 분석:**\n",
    "```markdown\n",
    "Query 1: 첫 관련문서 순위: 1, RR: 1.000  # doc1이 1위\n",
    "Query 2: 첫 관련문서 순위: 2, RR: 0.500  # doc2가 2위  \n",
    "Query 3: 첫 관련문서 순위: 5, RR: 0.200  # doc4가 5위\n",
    "Query 4: 첫 관련문서 순위: 1, RR: 1.000  # doc3이 1위\n",
    "Query 5: 첫 관련문서 순위: None, RR: 0.000  # 관련문서 없음\n",
    "\n",
    "최종 MRR: (1.000 + 0.500 + 0.200 + 1.000 + 0.000) / 5 = 0.540\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 MAP (Mean Average Precision)\n",
    "\n",
    "#### 개념\n",
    "**MAP@k**는 상위 k개 결과에서 **모든 관련 문서의 정확도를 종합적으로 평가**하는 지표입니다.\n",
    "\n",
    "#### 특징\n",
    "- **순위 고려**: 상위에 있는 관련 문서에 더 높은 가중치\n",
    "- **전체 고려**: 모든 관련 문서의 위치를 반영\n",
    "- **정확도 중심**: 정밀도(Precision)의 평균\n",
    "\n",
    "#### 계산 공식\n",
    "```markdown\n",
    "MAP@k = (1/Q) × Σ AP@k_i\n",
    "\n",
    "AP@k = (1/R) × Σ P(j) × rel(j)\n",
    "\n",
    "여기서:\n",
    "- Q: 전체 쿼리 수\n",
    "- R: 관련 문서 수\n",
    "- P(j): j번째 위치에서의 정밀도\n",
    "- rel(j): j번째 문서가 관련 있으면 1, 아니면 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MAP@k 계산 결과 ===\n",
      "=== MAP@3 계산 과정 ===\n",
      "\n",
      "Query 1: GT=['doc1', 'doc9'], Pred=['doc1', 'doc9', 'doc6']\n",
      "  위치 1: doc1 (관련O) - P@1 = 1.000\n",
      "  위치 2: doc9 (관련O) - P@2 = 1.000\n",
      "  위치 3: doc6 (관련X)\n",
      "  AP@3 = 2.000 / 2 = 1.000\n",
      "\n",
      "Query 2: GT=['doc2', 'doc5'], Pred=['doc7', 'doc2', 'doc3']\n",
      "  위치 1: doc7 (관련X)\n",
      "  위치 2: doc2 (관련O) - P@2 = 0.500\n",
      "  위치 3: doc3 (관련X)\n",
      "  AP@3 = 0.500 / 2 = 0.250\n",
      "\n",
      "Query 3: GT=['doc4'], Pred=['doc3', 'doc6', 'doc2']\n",
      "  위치 1: doc3 (관련X)\n",
      "  위치 2: doc6 (관련X)\n",
      "  위치 3: doc2 (관련X)\n",
      "  AP@3 = 0.000 / 1 = 0.000\n",
      "\n",
      "Query 4: GT=['doc3', 'doc4'], Pred=['doc3', 'doc7', 'doc5']\n",
      "  위치 1: doc3 (관련O) - P@1 = 1.000\n",
      "  위치 2: doc7 (관련X)\n",
      "  위치 3: doc5 (관련X)\n",
      "  AP@3 = 1.000 / 2 = 0.500\n",
      "\n",
      "Query 5: GT=['doc8'], Pred=['doc5', 'doc2', 'doc7']\n",
      "  위치 1: doc5 (관련X)\n",
      "  위치 2: doc2 (관련X)\n",
      "  위치 3: doc7 (관련X)\n",
      "  AP@3 = 0.000 / 1 = 0.000\n",
      "\n",
      "최종 MAP@3: 1.750 / 5 = 0.350\n",
      "MAP@3: 0.350\n",
      "\n",
      "=== MAP@5 계산 과정 ===\n",
      "\n",
      "Query 1: GT=['doc1', 'doc9'], Pred=['doc1', 'doc9', 'doc6', 'doc2', 'doc7']\n",
      "  위치 1: doc1 (관련O) - P@1 = 1.000\n",
      "  위치 2: doc9 (관련O) - P@2 = 1.000\n",
      "  위치 3: doc6 (관련X)\n",
      "  위치 4: doc2 (관련X)\n",
      "  위치 5: doc7 (관련X)\n",
      "  AP@5 = 2.000 / 2 = 1.000\n",
      "\n",
      "Query 2: GT=['doc2', 'doc5'], Pred=['doc7', 'doc2', 'doc3', 'doc5', 'doc1']\n",
      "  위치 1: doc7 (관련X)\n",
      "  위치 2: doc2 (관련O) - P@2 = 0.500\n",
      "  위치 3: doc3 (관련X)\n",
      "  위치 4: doc5 (관련O) - P@4 = 0.500\n",
      "  위치 5: doc1 (관련X)\n",
      "  AP@5 = 1.000 / 2 = 0.500\n",
      "\n",
      "Query 3: GT=['doc4'], Pred=['doc3', 'doc6', 'doc2', 'doc1', 'doc4']\n",
      "  위치 1: doc3 (관련X)\n",
      "  위치 2: doc6 (관련X)\n",
      "  위치 3: doc2 (관련X)\n",
      "  위치 4: doc1 (관련X)\n",
      "  위치 5: doc4 (관련O) - P@5 = 0.200\n",
      "  AP@5 = 0.200 / 1 = 0.200\n",
      "\n",
      "Query 4: GT=['doc3', 'doc4'], Pred=['doc3', 'doc7', 'doc5', 'doc8', 'doc2']\n",
      "  위치 1: doc3 (관련O) - P@1 = 1.000\n",
      "  위치 2: doc7 (관련X)\n",
      "  위치 3: doc5 (관련X)\n",
      "  위치 4: doc8 (관련X)\n",
      "  위치 5: doc2 (관련X)\n",
      "  AP@5 = 1.000 / 2 = 0.500\n",
      "\n",
      "Query 5: GT=['doc8'], Pred=['doc5', 'doc2', 'doc7', 'doc1', 'doc10']\n",
      "  위치 1: doc5 (관련X)\n",
      "  위치 2: doc2 (관련X)\n",
      "  위치 3: doc7 (관련X)\n",
      "  위치 4: doc1 (관련X)\n",
      "  위치 5: doc10 (관련X)\n",
      "  AP@5 = 0.000 / 1 = 0.000\n",
      "\n",
      "최종 MAP@5: 2.200 / 5 = 0.440\n",
      "MAP@5: 0.440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_map_at_k(ground_truth: List[List[str]], \n",
    "                       predictions: List[List[str]], \n",
    "                       k: int) -> float:\n",
    "    \"\"\"\n",
    "    Mean Average Precision@k 계산\n",
    "    \"\"\"\n",
    "    average_precisions = []\n",
    "    \n",
    "    print(f\"=== MAP@{k} 계산 과정 ===\")\n",
    "    \n",
    "    for i, (gt, pred) in enumerate(zip(ground_truth, predictions)):\n",
    "        # 상위 k개만 고려\n",
    "        top_k_pred = pred[:k]\n",
    "        \n",
    "        # Average Precision 계산\n",
    "        relevant_count = 0\n",
    "        precision_sum = 0.0\n",
    "        \n",
    "        print(f\"\\nQuery {i+1}: GT={gt}, Pred={top_k_pred}\")\n",
    "        \n",
    "        for rank, doc_id in enumerate(top_k_pred, 1):\n",
    "            if doc_id in gt:\n",
    "                relevant_count += 1\n",
    "                precision_at_rank = relevant_count / rank\n",
    "                precision_sum += precision_at_rank\n",
    "                print(f\"  위치 {rank}: {doc_id} (관련O) - P@{rank} = {precision_at_rank:.3f}\")\n",
    "            else:\n",
    "                print(f\"  위치 {rank}: {doc_id} (관련X)\")\n",
    "        \n",
    "        # AP 계산: 관련 문서 수로 나누어 정규화\n",
    "        ap = precision_sum / len(gt) if len(gt) > 0 else 0.0\n",
    "        average_precisions.append(ap)\n",
    "        print(f\"  AP@{k} = {precision_sum:.3f} / {len(gt)} = {ap:.3f}\")\n",
    "    \n",
    "    map_score = sum(average_precisions) / len(average_precisions)\n",
    "    print(f\"\\n최종 MAP@{k}: {sum(average_precisions):.3f} / {len(average_precisions)} = {map_score:.3f}\")\n",
    "    return map_score\n",
    "\n",
    "# 실제 계산\n",
    "print(\"=== MAP@k 계산 결과 ===\")\n",
    "for k in [3, 5]:\n",
    "    map_score = calculate_map_at_k(ground_truth, system_results, k)\n",
    "    print(f\"MAP@{k}: {map_score:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**예상 출력 분석:**\n",
    "```markdown\n",
    "Query 1: AP@3 = (1.000 + 1.000) / 2 = 1.000  # 1,2위 모두 정답\n",
    "Query 2: AP@3 = (0.500) / 2 = 0.250           # 2위만 정답\n",
    "Query 3: AP@3 = 0.000 / 1 = 0.000             # 3위 안에 정답 없음  \n",
    "Query 4: AP@3 = (1.000) / 2 = 0.500           # 1위만 정답\n",
    "Query 5: AP@3 = 0.000 / 1 = 0.000             # 3위 안에 정답 없음\n",
    "\n",
    "MAP@3: (1.000 + 0.250 + 0.000 + 0.500 + 0.000) / 5 = 0.350\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 NDCG (Normalized Discounted Cumulative Gain)\n",
    "\n",
    "#### 개념\n",
    "**NDCG@k**는 관련성 점수와 순위를 모두 고려한 **가장 정교한 평가 지표**입니다.\n",
    "\n",
    "#### 특징\n",
    "- **등급 관련성**: 다단계 관련성 점수 지원\n",
    "- **위치 할인**: 하위 순위일수록 가중치 감소\n",
    "- **정규화**: 이상적인 순위와 비교하여 0~1 범위\n",
    "\n",
    "#### 계산 공식\n",
    "```markdown\n",
    "NDCG@k = DCG@k / IDCG@k\n",
    "\n",
    "DCG@k = Σ(i=1 to k) (2^rel_i - 1) / log₂(i + 1)\n",
    "\n",
    "여기서:\n",
    "- rel_i: i번째 문서의 관련성 점수\n",
    "- IDCG@k: 이상적인 순위에서의 DCG   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NDCG@k 계산 결과 ===\n",
      "=== NDCG@3 계산 과정 ===\n",
      "\n",
      "Query 1: GT=['doc1', 'doc9'], Pred=['doc1', 'doc9', 'doc6']\n",
      "  위치 1: doc1 (관련성=1) - Gain: 1.000\n",
      "  위치 2: doc9 (관련성=1) - Gain: 0.631\n",
      "  위치 3: doc6 (관련성=0)\n",
      "  DCG: 1.631, IDCG: 1.631, NDCG: 1.000\n",
      "\n",
      "Query 2: GT=['doc2', 'doc5'], Pred=['doc7', 'doc2', 'doc3']\n",
      "  위치 1: doc7 (관련성=0)\n",
      "  위치 2: doc2 (관련성=1) - Gain: 0.631\n",
      "  위치 3: doc3 (관련성=0)\n",
      "  DCG: 0.631, IDCG: 1.631, NDCG: 0.387\n",
      "\n",
      "Query 3: GT=['doc4'], Pred=['doc3', 'doc6', 'doc2']\n",
      "  위치 1: doc3 (관련성=0)\n",
      "  위치 2: doc6 (관련성=0)\n",
      "  위치 3: doc2 (관련성=0)\n",
      "  DCG: 0.000, IDCG: 1.000, NDCG: 0.000\n",
      "\n",
      "Query 4: GT=['doc3', 'doc4'], Pred=['doc3', 'doc7', 'doc5']\n",
      "  위치 1: doc3 (관련성=1) - Gain: 1.000\n",
      "  위치 2: doc7 (관련성=0)\n",
      "  위치 3: doc5 (관련성=0)\n",
      "  DCG: 1.000, IDCG: 1.631, NDCG: 0.613\n",
      "\n",
      "Query 5: GT=['doc8'], Pred=['doc5', 'doc2', 'doc7']\n",
      "  위치 1: doc5 (관련성=0)\n",
      "  위치 2: doc2 (관련성=0)\n",
      "  위치 3: doc7 (관련성=0)\n",
      "  DCG: 0.000, IDCG: 1.000, NDCG: 0.000\n",
      "\n",
      "최종 NDCG@3: 2.000 / 5 = 0.400\n",
      "NDCG@3: 0.400\n",
      "\n",
      "=== NDCG@5 계산 과정 ===\n",
      "\n",
      "Query 1: GT=['doc1', 'doc9'], Pred=['doc1', 'doc9', 'doc6', 'doc2', 'doc7']\n",
      "  위치 1: doc1 (관련성=1) - Gain: 1.000\n",
      "  위치 2: doc9 (관련성=1) - Gain: 0.631\n",
      "  위치 3: doc6 (관련성=0)\n",
      "  위치 4: doc2 (관련성=0)\n",
      "  위치 5: doc7 (관련성=0)\n",
      "  DCG: 1.631, IDCG: 1.631, NDCG: 1.000\n",
      "\n",
      "Query 2: GT=['doc2', 'doc5'], Pred=['doc7', 'doc2', 'doc3', 'doc5', 'doc1']\n",
      "  위치 1: doc7 (관련성=0)\n",
      "  위치 2: doc2 (관련성=1) - Gain: 0.631\n",
      "  위치 3: doc3 (관련성=0)\n",
      "  위치 4: doc5 (관련성=1) - Gain: 0.431\n",
      "  위치 5: doc1 (관련성=0)\n",
      "  DCG: 1.062, IDCG: 1.631, NDCG: 0.651\n",
      "\n",
      "Query 3: GT=['doc4'], Pred=['doc3', 'doc6', 'doc2', 'doc1', 'doc4']\n",
      "  위치 1: doc3 (관련성=0)\n",
      "  위치 2: doc6 (관련성=0)\n",
      "  위치 3: doc2 (관련성=0)\n",
      "  위치 4: doc1 (관련성=0)\n",
      "  위치 5: doc4 (관련성=1) - Gain: 0.387\n",
      "  DCG: 0.387, IDCG: 1.000, NDCG: 0.387\n",
      "\n",
      "Query 4: GT=['doc3', 'doc4'], Pred=['doc3', 'doc7', 'doc5', 'doc8', 'doc2']\n",
      "  위치 1: doc3 (관련성=1) - Gain: 1.000\n",
      "  위치 2: doc7 (관련성=0)\n",
      "  위치 3: doc5 (관련성=0)\n",
      "  위치 4: doc8 (관련성=0)\n",
      "  위치 5: doc2 (관련성=0)\n",
      "  DCG: 1.000, IDCG: 1.631, NDCG: 0.613\n",
      "\n",
      "Query 5: GT=['doc8'], Pred=['doc5', 'doc2', 'doc7', 'doc1', 'doc10']\n",
      "  위치 1: doc5 (관련성=0)\n",
      "  위치 2: doc2 (관련성=0)\n",
      "  위치 3: doc7 (관련성=0)\n",
      "  위치 4: doc1 (관련성=0)\n",
      "  위치 5: doc10 (관련성=0)\n",
      "  DCG: 0.000, IDCG: 1.000, NDCG: 0.000\n",
      "\n",
      "최종 NDCG@5: 2.651 / 5 = 0.530\n",
      "NDCG@5: 0.530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_ndcg_at_k(ground_truth: List[List[str]], \n",
    "                        predictions: List[List[str]], \n",
    "                        k: int,\n",
    "                        relevance_scores: Dict[str, Dict[str, int]] = None) -> float:\n",
    "    \"\"\"\n",
    "    NDCG@k 계산 (이진 관련성 또는 등급 관련성)\n",
    "    \"\"\"\n",
    "    if relevance_scores is None:\n",
    "        # 이진 관련성: 관련 있으면 1, 없으면 0\n",
    "        relevance_scores = {}\n",
    "        for i, gt in enumerate(ground_truth):\n",
    "            query_id = f\"query_{i}\"\n",
    "            relevance_scores[query_id] = {doc: 1 for doc in gt}\n",
    "    \n",
    "    ndcg_scores = []\n",
    "    \n",
    "    print(f\"=== NDCG@{k} 계산 과정 ===\")\n",
    "    \n",
    "    for i, (gt, pred) in enumerate(zip(ground_truth, predictions)):\n",
    "        query_id = f\"query_{i}\"\n",
    "        top_k_pred = pred[:k]\n",
    "        \n",
    "        print(f\"\\nQuery {i+1}: GT={gt}, Pred={top_k_pred}\")\n",
    "        \n",
    "        # DCG 계산\n",
    "        dcg = 0.0\n",
    "        for rank, doc_id in enumerate(top_k_pred, 1):\n",
    "            rel_score = relevance_scores.get(query_id, {}).get(doc_id, 0)\n",
    "            if rel_score > 0:\n",
    "                gain = (2**rel_score - 1) / math.log2(rank + 1)\n",
    "                dcg += gain\n",
    "                print(f\"  위치 {rank}: {doc_id} (관련성={rel_score}) - Gain: {gain:.3f}\")\n",
    "            else:\n",
    "                print(f\"  위치 {rank}: {doc_id} (관련성=0)\")\n",
    "        \n",
    "        # IDCG 계산 (이상적인 순위)\n",
    "        ideal_scores = sorted(relevance_scores.get(query_id, {}).values(), \n",
    "                             reverse=True)[:k]\n",
    "        idcg = 0.0\n",
    "        for rank, rel_score in enumerate(ideal_scores, 1):\n",
    "            if rel_score > 0:\n",
    "                idcg += (2**rel_score - 1) / math.log2(rank + 1)\n",
    "        \n",
    "        # NDCG 계산\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "        ndcg_scores.append(ndcg)\n",
    "        \n",
    "        print(f\"  DCG: {dcg:.3f}, IDCG: {idcg:.3f}, NDCG: {ndcg:.3f}\")\n",
    "    \n",
    "    final_ndcg = sum(ndcg_scores) / len(ndcg_scores)\n",
    "    print(f\"\\n최종 NDCG@{k}: {sum(ndcg_scores):.3f} / {len(ndcg_scores)} = {final_ndcg:.3f}\")\n",
    "    return final_ndcg\n",
    "\n",
    "# 실제 계산 (이진 관련성)\n",
    "print(\"=== NDCG@k 계산 결과 ===\")\n",
    "for k in [3, 5]:\n",
    "    ndcg_score = calculate_ndcg_at_k(ground_truth, system_results, k)\n",
    "    print(f\"NDCG@{k}: {ndcg_score:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 5. 지표별 특성 비교 분석\n",
    "\n",
    "- **Hit Rate**: 검색 시스템의 기본 성능 확인\n",
    "- **MRR**: 사용자가 빠르게 답을 찾는 것이 중요한 경우\n",
    "- **MAP**: 모든 관련 문서의 순위가 중요한 경우\n",
    "- **NDCG**: 등급별 관련성과 순위 모두 고려해야 하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "        종합 평가 결과 비교\n",
      "============================================================\n",
      "=== Hit Rate@1 계산 과정 ===\n",
      "Query 1: GT=['doc1', 'doc9'], Pred@1=['doc1']\n",
      "         Hit: O\n",
      "Query 2: GT=['doc2', 'doc5'], Pred@1=['doc7']\n",
      "         Hit: X\n",
      "Query 3: GT=['doc4'], Pred@1=['doc3']\n",
      "         Hit: X\n",
      "Query 4: GT=['doc3', 'doc4'], Pred@1=['doc3']\n",
      "         Hit: O\n",
      "Query 5: GT=['doc8'], Pred@1=['doc5']\n",
      "         Hit: X\n",
      "\n",
      "총 적중: 2/5 = 0.400\n",
      "=== Hit Rate@3 계산 과정 ===\n",
      "Query 1: GT=['doc1', 'doc9'], Pred@3=['doc1', 'doc9', 'doc6']\n",
      "         Hit: O\n",
      "Query 2: GT=['doc2', 'doc5'], Pred@3=['doc7', 'doc2', 'doc3']\n",
      "         Hit: O\n",
      "Query 3: GT=['doc4'], Pred@3=['doc3', 'doc6', 'doc2']\n",
      "         Hit: X\n",
      "Query 4: GT=['doc3', 'doc4'], Pred@3=['doc3', 'doc7', 'doc5']\n",
      "         Hit: O\n",
      "Query 5: GT=['doc8'], Pred@3=['doc5', 'doc2', 'doc7']\n",
      "         Hit: X\n",
      "\n",
      "총 적중: 3/5 = 0.600\n",
      "=== Hit Rate@5 계산 과정 ===\n",
      "Query 1: GT=['doc1', 'doc9'], Pred@5=['doc1', 'doc9', 'doc6', 'doc2', 'doc7']\n",
      "         Hit: O\n",
      "Query 2: GT=['doc2', 'doc5'], Pred@5=['doc7', 'doc2', 'doc3', 'doc5', 'doc1']\n",
      "         Hit: O\n",
      "Query 3: GT=['doc4'], Pred@5=['doc3', 'doc6', 'doc2', 'doc1', 'doc4']\n",
      "         Hit: O\n",
      "Query 4: GT=['doc3', 'doc4'], Pred@5=['doc3', 'doc7', 'doc5', 'doc8', 'doc2']\n",
      "         Hit: O\n",
      "Query 5: GT=['doc8'], Pred@5=['doc5', 'doc2', 'doc7', 'doc1', 'doc10']\n",
      "         Hit: X\n",
      "\n",
      "총 적중: 4/5 = 0.800\n",
      "\n",
      "=== MRR 계산 과정 ===\n",
      "Query 1: GT=['doc1', 'doc9']\n",
      "         Pred=['doc1', 'doc9', 'doc6', 'doc2', 'doc7']\n",
      "         첫 관련문서 순위: 1, RR: 1.000\n",
      "Query 2: GT=['doc2', 'doc5']\n",
      "         Pred=['doc7', 'doc2', 'doc3', 'doc5', 'doc1']\n",
      "         첫 관련문서 순위: 2, RR: 0.500\n",
      "Query 3: GT=['doc4']\n",
      "         Pred=['doc3', 'doc6', 'doc2', 'doc1', 'doc4']\n",
      "         첫 관련문서 순위: 5, RR: 0.200\n",
      "Query 4: GT=['doc3', 'doc4']\n",
      "         Pred=['doc3', 'doc7', 'doc5', 'doc8', 'doc2']\n",
      "         첫 관련문서 순위: 1, RR: 1.000\n",
      "Query 5: GT=['doc8']\n",
      "         Pred=['doc5', 'doc2', 'doc7', 'doc1', 'doc10']\n",
      "         첫 관련문서 순위: None, RR: 0.000\n",
      "\n",
      "평균 RR: 2.700 / 5 = 0.540\n",
      "\n",
      "=== MAP@3 계산 과정 ===\n",
      "\n",
      "Query 1: GT=['doc1', 'doc9'], Pred=['doc1', 'doc9', 'doc6']\n",
      "  위치 1: doc1 (관련O) - P@1 = 1.000\n",
      "  위치 2: doc9 (관련O) - P@2 = 1.000\n",
      "  위치 3: doc6 (관련X)\n",
      "  AP@3 = 2.000 / 2 = 1.000\n",
      "\n",
      "Query 2: GT=['doc2', 'doc5'], Pred=['doc7', 'doc2', 'doc3']\n",
      "  위치 1: doc7 (관련X)\n",
      "  위치 2: doc2 (관련O) - P@2 = 0.500\n",
      "  위치 3: doc3 (관련X)\n",
      "  AP@3 = 0.500 / 2 = 0.250\n",
      "\n",
      "Query 3: GT=['doc4'], Pred=['doc3', 'doc6', 'doc2']\n",
      "  위치 1: doc3 (관련X)\n",
      "  위치 2: doc6 (관련X)\n",
      "  위치 3: doc2 (관련X)\n",
      "  AP@3 = 0.000 / 1 = 0.000\n",
      "\n",
      "Query 4: GT=['doc3', 'doc4'], Pred=['doc3', 'doc7', 'doc5']\n",
      "  위치 1: doc3 (관련O) - P@1 = 1.000\n",
      "  위치 2: doc7 (관련X)\n",
      "  위치 3: doc5 (관련X)\n",
      "  AP@3 = 1.000 / 2 = 0.500\n",
      "\n",
      "Query 5: GT=['doc8'], Pred=['doc5', 'doc2', 'doc7']\n",
      "  위치 1: doc5 (관련X)\n",
      "  위치 2: doc2 (관련X)\n",
      "  위치 3: doc7 (관련X)\n",
      "  AP@3 = 0.000 / 1 = 0.000\n",
      "\n",
      "최종 MAP@3: 1.750 / 5 = 0.350\n",
      "=== MAP@5 계산 과정 ===\n",
      "\n",
      "Query 1: GT=['doc1', 'doc9'], Pred=['doc1', 'doc9', 'doc6', 'doc2', 'doc7']\n",
      "  위치 1: doc1 (관련O) - P@1 = 1.000\n",
      "  위치 2: doc9 (관련O) - P@2 = 1.000\n",
      "  위치 3: doc6 (관련X)\n",
      "  위치 4: doc2 (관련X)\n",
      "  위치 5: doc7 (관련X)\n",
      "  AP@5 = 2.000 / 2 = 1.000\n",
      "\n",
      "Query 2: GT=['doc2', 'doc5'], Pred=['doc7', 'doc2', 'doc3', 'doc5', 'doc1']\n",
      "  위치 1: doc7 (관련X)\n",
      "  위치 2: doc2 (관련O) - P@2 = 0.500\n",
      "  위치 3: doc3 (관련X)\n",
      "  위치 4: doc5 (관련O) - P@4 = 0.500\n",
      "  위치 5: doc1 (관련X)\n",
      "  AP@5 = 1.000 / 2 = 0.500\n",
      "\n",
      "Query 3: GT=['doc4'], Pred=['doc3', 'doc6', 'doc2', 'doc1', 'doc4']\n",
      "  위치 1: doc3 (관련X)\n",
      "  위치 2: doc6 (관련X)\n",
      "  위치 3: doc2 (관련X)\n",
      "  위치 4: doc1 (관련X)\n",
      "  위치 5: doc4 (관련O) - P@5 = 0.200\n",
      "  AP@5 = 0.200 / 1 = 0.200\n",
      "\n",
      "Query 4: GT=['doc3', 'doc4'], Pred=['doc3', 'doc7', 'doc5', 'doc8', 'doc2']\n",
      "  위치 1: doc3 (관련O) - P@1 = 1.000\n",
      "  위치 2: doc7 (관련X)\n",
      "  위치 3: doc5 (관련X)\n",
      "  위치 4: doc8 (관련X)\n",
      "  위치 5: doc2 (관련X)\n",
      "  AP@5 = 1.000 / 2 = 0.500\n",
      "\n",
      "Query 5: GT=['doc8'], Pred=['doc5', 'doc2', 'doc7', 'doc1', 'doc10']\n",
      "  위치 1: doc5 (관련X)\n",
      "  위치 2: doc2 (관련X)\n",
      "  위치 3: doc7 (관련X)\n",
      "  위치 4: doc1 (관련X)\n",
      "  위치 5: doc10 (관련X)\n",
      "  AP@5 = 0.000 / 1 = 0.000\n",
      "\n",
      "최종 MAP@5: 2.200 / 5 = 0.440\n",
      "\n",
      "=== NDCG@3 계산 과정 ===\n",
      "\n",
      "Query 1: GT=['doc1', 'doc9'], Pred=['doc1', 'doc9', 'doc6']\n",
      "  위치 1: doc1 (관련성=1) - Gain: 1.000\n",
      "  위치 2: doc9 (관련성=1) - Gain: 0.631\n",
      "  위치 3: doc6 (관련성=0)\n",
      "  DCG: 1.631, IDCG: 1.631, NDCG: 1.000\n",
      "\n",
      "Query 2: GT=['doc2', 'doc5'], Pred=['doc7', 'doc2', 'doc3']\n",
      "  위치 1: doc7 (관련성=0)\n",
      "  위치 2: doc2 (관련성=1) - Gain: 0.631\n",
      "  위치 3: doc3 (관련성=0)\n",
      "  DCG: 0.631, IDCG: 1.631, NDCG: 0.387\n",
      "\n",
      "Query 3: GT=['doc4'], Pred=['doc3', 'doc6', 'doc2']\n",
      "  위치 1: doc3 (관련성=0)\n",
      "  위치 2: doc6 (관련성=0)\n",
      "  위치 3: doc2 (관련성=0)\n",
      "  DCG: 0.000, IDCG: 1.000, NDCG: 0.000\n",
      "\n",
      "Query 4: GT=['doc3', 'doc4'], Pred=['doc3', 'doc7', 'doc5']\n",
      "  위치 1: doc3 (관련성=1) - Gain: 1.000\n",
      "  위치 2: doc7 (관련성=0)\n",
      "  위치 3: doc5 (관련성=0)\n",
      "  DCG: 1.000, IDCG: 1.631, NDCG: 0.613\n",
      "\n",
      "Query 5: GT=['doc8'], Pred=['doc5', 'doc2', 'doc7']\n",
      "  위치 1: doc5 (관련성=0)\n",
      "  위치 2: doc2 (관련성=0)\n",
      "  위치 3: doc7 (관련성=0)\n",
      "  DCG: 0.000, IDCG: 1.000, NDCG: 0.000\n",
      "\n",
      "최종 NDCG@3: 2.000 / 5 = 0.400\n",
      "=== NDCG@5 계산 과정 ===\n",
      "\n",
      "Query 1: GT=['doc1', 'doc9'], Pred=['doc1', 'doc9', 'doc6', 'doc2', 'doc7']\n",
      "  위치 1: doc1 (관련성=1) - Gain: 1.000\n",
      "  위치 2: doc9 (관련성=1) - Gain: 0.631\n",
      "  위치 3: doc6 (관련성=0)\n",
      "  위치 4: doc2 (관련성=0)\n",
      "  위치 5: doc7 (관련성=0)\n",
      "  DCG: 1.631, IDCG: 1.631, NDCG: 1.000\n",
      "\n",
      "Query 2: GT=['doc2', 'doc5'], Pred=['doc7', 'doc2', 'doc3', 'doc5', 'doc1']\n",
      "  위치 1: doc7 (관련성=0)\n",
      "  위치 2: doc2 (관련성=1) - Gain: 0.631\n",
      "  위치 3: doc3 (관련성=0)\n",
      "  위치 4: doc5 (관련성=1) - Gain: 0.431\n",
      "  위치 5: doc1 (관련성=0)\n",
      "  DCG: 1.062, IDCG: 1.631, NDCG: 0.651\n",
      "\n",
      "Query 3: GT=['doc4'], Pred=['doc3', 'doc6', 'doc2', 'doc1', 'doc4']\n",
      "  위치 1: doc3 (관련성=0)\n",
      "  위치 2: doc6 (관련성=0)\n",
      "  위치 3: doc2 (관련성=0)\n",
      "  위치 4: doc1 (관련성=0)\n",
      "  위치 5: doc4 (관련성=1) - Gain: 0.387\n",
      "  DCG: 0.387, IDCG: 1.000, NDCG: 0.387\n",
      "\n",
      "Query 4: GT=['doc3', 'doc4'], Pred=['doc3', 'doc7', 'doc5', 'doc8', 'doc2']\n",
      "  위치 1: doc3 (관련성=1) - Gain: 1.000\n",
      "  위치 2: doc7 (관련성=0)\n",
      "  위치 3: doc5 (관련성=0)\n",
      "  위치 4: doc8 (관련성=0)\n",
      "  위치 5: doc2 (관련성=0)\n",
      "  DCG: 1.000, IDCG: 1.631, NDCG: 0.613\n",
      "\n",
      "Query 5: GT=['doc8'], Pred=['doc5', 'doc2', 'doc7', 'doc1', 'doc10']\n",
      "  위치 1: doc5 (관련성=0)\n",
      "  위치 2: doc2 (관련성=0)\n",
      "  위치 3: doc7 (관련성=0)\n",
      "  위치 4: doc1 (관련성=0)\n",
      "  위치 5: doc10 (관련성=0)\n",
      "  DCG: 0.000, IDCG: 1.000, NDCG: 0.000\n",
      "\n",
      "최종 NDCG@5: 2.651 / 5 = 0.530\n",
      "\n",
      "============================================================\n",
      "            최종 결과 요약\n",
      "============================================================\n",
      "지표           점수       해석\n",
      "--------------------------------------------------\n",
      "Hit_Rate@1   0.400    첫 번째 결과의 유용성\n",
      "Hit_Rate@3   0.600    상위 3개 결과의 포괄성\n",
      "Hit_Rate@5   0.800    상위 5개 결과의 포괄성\n",
      "MRR          0.540    첫 관련문서 발견 속도\n",
      "MAP@3        0.350    상위 3개의 정확도\n",
      "MAP@5        0.440    상위 5개의 정확도\n",
      "NDCG@3       0.400    상위 3개의 종합 품질\n",
      "NDCG@5       0.530    상위 5개의 종합 품질\n"
     ]
    }
   ],
   "source": [
    "def comprehensive_evaluation():\n",
    "    \"\"\"모든 지표를 한번에 계산하여 특성 비교\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"        종합 평가 결과 비교\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Hit Rate 계산\n",
    "    for k in [1, 3, 5]:\n",
    "        hr = calculate_hit_rate(ground_truth, system_results, k)\n",
    "        results[f\"Hit_Rate@{k}\"] = hr\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # MRR 계산\n",
    "    mrr = calculate_mrr(ground_truth, system_results)\n",
    "    results[\"MRR\"] = mrr\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # MAP 계산  \n",
    "    for k in [3, 5]:\n",
    "        map_score = calculate_map_at_k(ground_truth, system_results, k)\n",
    "        results[f\"MAP@{k}\"] = map_score\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # NDCG 계산\n",
    "    for k in [3, 5]:\n",
    "        ndcg = calculate_ndcg_at_k(ground_truth, system_results, k)\n",
    "        results[f\"NDCG@{k}\"] = ndcg\n",
    "    \n",
    "    # 결과 요약 테이블\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"            최종 결과 요약\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'지표':<12} {'점수':<8} {'해석'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    interpretations = {\n",
    "        \"Hit_Rate@1\": \"첫 번째 결과의 유용성\",\n",
    "        \"Hit_Rate@3\": \"상위 3개 결과의 포괄성\", \n",
    "        \"Hit_Rate@5\": \"상위 5개 결과의 포괄성\",\n",
    "        \"MRR\": \"첫 관련문서 발견 속도\",\n",
    "        \"MAP@3\": \"상위 3개의 정확도\", \n",
    "        \"MAP@5\": \"상위 5개의 정확도\",\n",
    "        \"NDCG@3\": \"상위 3개의 종합 품질\",\n",
    "        \"NDCG@5\": \"상위 5개의 종합 품질\"\n",
    "    }\n",
    "    \n",
    "    for metric, score in results.items():\n",
    "        interp = interpretations.get(metric, \"\")\n",
    "        print(f\"{metric:<12} {score:<8.3f} {interp}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 종합 평가 실행\n",
    "evaluation_results = comprehensive_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🛠️ 6. 최신 라이브러리 활용\n",
    "\n",
    "### 6.1 ranx 라이브러리 사용법\n",
    "```python\n",
    "# ranx 설치 및 임포트\n",
    "!pip install ranx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ranx를 사용한 평가 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\ktds-llm\\.venv\\Lib\\site-packages\\ranx\\metrics\\hit_rate.py:38: NumbaTypeSafetyWarning: \u001b[1m\u001b[1m\u001b[1munsafe cast from uint64 to int64. Precision may be lost.\u001b[0m\u001b[0m\u001b[0m\n",
      "  scores[i] = _hit_rate(qrels[i], run[i], k, rel_lvl)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranx 라이브러리 계산 결과:\n",
      "hit_rate@1: 0.400\n",
      "hit_rate@3: 0.600\n",
      "hit_rate@5: 0.800\n",
      "mrr: 0.540\n",
      "map@3: 0.350\n",
      "map@5: 0.440\n",
      "ndcg@3: 0.400\n",
      "ndcg@5: 0.530\n"
     ]
    }
   ],
   "source": [
    "from ranx import Qrels, Run, evaluate, compare\n",
    "\n",
    "# 데이터 변환\n",
    "def convert_to_ranx_format(ground_truth, predictions):\n",
    "    \"\"\"데이터를 ranx 형식으로 변환\"\"\"\n",
    "    \n",
    "    # Qrels (Ground Truth) 형식: {query_id: {doc_id: relevance_score}}\n",
    "    qrels_dict = {}\n",
    "    for i, gt_docs in enumerate(ground_truth):\n",
    "        query_id = f\"q_{i+1}\"\n",
    "        qrels_dict[query_id] = {doc_id: 1 for doc_id in gt_docs}\n",
    "    \n",
    "    # Run (Predictions) 형식: {query_id: {doc_id: score}}\n",
    "    run_dict = {}\n",
    "    for i, pred_docs in enumerate(predictions):\n",
    "        query_id = f\"q_{i+1}\"\n",
    "        # 순위에 따라 점수 부여 (높은 순위 = 높은 점수)\n",
    "        run_dict[query_id] = {\n",
    "            doc_id: len(pred_docs) - rank \n",
    "            for rank, doc_id in enumerate(pred_docs)\n",
    "        }\n",
    "    \n",
    "    return qrels_dict, run_dict\n",
    "\n",
    "\n",
    "qrels_dict, run_dict = convert_to_ranx_format(ground_truth, system_results)\n",
    "\n",
    "# ranx 객체 생성\n",
    "qrels = Qrels(qrels_dict, name=\"Customer_Service_Queries\")\n",
    "run = Run(run_dict, name=\"Search_System_v1\")\n",
    "\n",
    "print(\"=== ranx를 사용한 평가 ===\")\n",
    "\n",
    "# 여러 지표 동시 계산\n",
    "metrics = [\"hit_rate@1\", \"hit_rate@3\", \"hit_rate@5\", \"mrr\", \"map@3\", \"map@5\", \"ndcg@3\", \"ndcg@5\"]\n",
    "results = evaluate(qrels, run, metrics)\n",
    "\n",
    "print(\"ranx 라이브러리 계산 결과:\")\n",
    "for metric, score in results.items():\n",
    "    print(f\"{metric}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 pytrec_eval 사용법\n",
    "```python\n",
    "# pytrec_eval 설치 \n",
    "!pip install pytrec-eval\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytrec_eval\n",
    "import json\n",
    "\n",
    "# pytrec_eval 형식으로 변환\n",
    "qrel_trec = {}\n",
    "run_trec = {}\n",
    "\n",
    "for i, (gt, pred) in enumerate(zip(ground_truth, system_results)):\n",
    "    query_id = f\"q{i+1}\"\n",
    "    \n",
    "    # qrel 형식: {query_id: {doc_id: relevance}}\n",
    "    qrel_trec[query_id] = {doc_id: 1 for doc_id in gt}\n",
    "    \n",
    "    # run 형식: {query_id: {doc_id: score}}\n",
    "    run_trec[query_id] = {\n",
    "        doc_id: 1.0 - (rank * 0.1) \n",
    "        for rank, doc_id in enumerate(pred)\n",
    "    }\n",
    "\n",
    "# 평가자 생성\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "    qrel_trec, \n",
    "    {'map', 'ndcg', 'recip_rank', 'P_1', 'P_3'} # 평가 지표 설정 (예: MAP, NDCG, Reciprocal Rank, Precision@1, Precision@3)\n",
    ")\n",
    "\n",
    "# 평가 실행\n",
    "results = evaluator.evaluate(run_trec)\n",
    "\n",
    "print(\"=== pytrec_eval 결과 ===\")\n",
    "print(json.dumps(results, indent=2))\n",
    "\n",
    "# 평균 점수 계산 (k=5)\n",
    "metrics_avg = {}\n",
    "for metric in ['map', 'ndcg', 'recip_rank', 'P_1', 'P_3']:\n",
    "    scores = [results[qid][metric] for qid in results.keys()]\n",
    "    metrics_avg[metric] = sum(scores) / len(scores)\n",
    "    print(f\"{metric}: {metrics_avg[metric]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📝 7. 실습 문제\n",
    "\n",
    "**문제 1**: 다음 검색 결과에 대해 Hit Rate@3, MRR, MAP@3을 계산하세요.\n",
    "\n",
    "\n",
    "- **예상 결과:**\n",
    "    ```markdown\n",
    "    Hit Rate@3: 0.750 (4개 중 3개 쿼리에서 관련 문서 발견)\n",
    "    MRR: 0.458 ((1/1 + 1/2 + 0/1 + 1/3) / 4)\n",
    "    MAP@3: 0.417 (정답 위치와 개수에 따라 계산)\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연습 데이터 (다양한 성능 수준)\n",
    "practice_ground_truth = [\n",
    "    [\"A\", \"B\"],         # Query 1의 정답\n",
    "    [\"C\"],              # Query 2의 정답  \n",
    "    [\"D\", \"E\"],         # Query 3의 정답\n",
    "    [\"F\"]               # Query 4의 정답\n",
    "]\n",
    "\n",
    "practice_predictions = [\n",
    "    [\"A\", \"X\", \"B\"],      # Query 1: 좋은 성능 (1,3위)\n",
    "    [\"Y\", \"C\", \"Z\"],      # Query 2: 보통 성능 (2위)\n",
    "    [\"W\", \"V\", \"U\"],      # Query 3: 나쁜 성능 (정답 없음)\n",
    "    [\"T\", \"S\", \"F\"]       # Query 4: 나쁜 성능 (3위)\n",
    "]\n",
    "\n",
    "# 여기에 코드를 작성하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_practice_problem():\n",
    "    \"\"\"연습 문제 해답 함수\"\"\"\n",
    "    print(\"=== 연습 문제 풀이 ===\")\n",
    "    \n",
    "    # Hit Rate@3 계산\n",
    "    hit_rate = calculate_hit_rate(practice_ground_truth, practice_predictions, 3)\n",
    "    print(f\"Hit Rate@3: {hit_rate:.3f}\")\n",
    "    \n",
    "    # MRR 계산  \n",
    "    mrr = calculate_mrr(practice_ground_truth, practice_predictions)\n",
    "    print(f\"MRR: {mrr:.3f}\")\n",
    "    \n",
    "    # MAP@3 계산\n",
    "    map_score = calculate_map_at_k(practice_ground_truth, practice_predictions, 3)\n",
    "    print(f\"MAP@3: {map_score:.3f}\")\n",
    "\n",
    "# 정답 확인\n",
    "solve_practice_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**문제 2**: 검색 시스템을 개선했을 때의 성능 변화를 측정해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개선 전 vs 개선 후 시스템 비교\n",
    "system_v1_results = system_results  # 기존 시스템\n",
    "\n",
    "# 개선된 시스템 (v2) - 더 나은 성능\n",
    "system_v2_results = [\n",
    "    [\"doc1\", \"doc9\", \"doc2\", \"doc6\", \"doc7\"],  # Query 1: 더 좋아짐\n",
    "    [\"doc2\", \"doc5\", \"doc3\", \"doc7\", \"doc1\"],  # Query 2: 개선됨  \n",
    "    [\"doc4\", \"doc3\", \"doc6\", \"doc2\", \"doc1\"],  # Query 3: 크게 개선됨\n",
    "    [\"doc3\", \"doc4\", \"doc7\", \"doc5\", \"doc8\"],  # Query 4: 개선됨\n",
    "    [\"doc8\", \"doc5\", \"doc2\", \"doc7\", \"doc1\"]   # Query 5: 개선됨\n",
    "]\n",
    "\n",
    "# 여기에 코드를 작성하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_systems():\n",
    "    \"\"\"두 시스템의 성능 비교\"\"\"\n",
    "    print(\"=== 시스템 성능 비교 ===\")\n",
    "    \n",
    "    metrics = [\"Hit Rate@3\", \"MRR\", \"MAP@3\", \"NDCG@3\"]\n",
    "    \n",
    "    # v1 성능\n",
    "    v1_hr3 = calculate_hit_rate(ground_truth, system_v1_results, 3)\n",
    "    v1_mrr = calculate_mrr(ground_truth, system_v1_results)\n",
    "    v1_map3 = calculate_map_at_k(ground_truth, system_v1_results, 3)\n",
    "    v1_ndcg3 = calculate_ndcg_at_k(ground_truth, system_v1_results, 3)\n",
    "    \n",
    "    # v2 성능  \n",
    "    v2_hr3 = calculate_hit_rate(ground_truth, system_v2_results, 3)\n",
    "    v2_mrr = calculate_mrr(ground_truth, system_v2_results)\n",
    "    v2_map3 = calculate_map_at_k(ground_truth, system_v2_results, 3)\n",
    "    v2_ndcg3 = calculate_ndcg_at_k(ground_truth, system_v2_results, 3)\n",
    "    \n",
    "    # 결과 비교\n",
    "    v1_scores = [v1_hr3, v1_mrr, v1_map3, v1_ndcg3]\n",
    "    v2_scores = [v2_hr3, v2_mrr, v2_map3, v2_ndcg3]\n",
    "    \n",
    "    print(\"\\n시스템 성능 비교:\")\n",
    "    print(f\"{'지표':<12} {'v1':<8} {'v2':<8} {'개선율':<8}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for metric, v1, v2 in zip(metrics, v1_scores, v2_scores):\n",
    "        improvement = ((v2 - v1) / v1 * 100) if v1 > 0 else float('inf')\n",
    "        print(f\"{metric:<12} {v1:<8.3f} {v2:<8.3f} {improvement:<8.1f}%\")\n",
    "\n",
    "# 시스템 비교 실행\n",
    "compare_systems()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎓 8. 핵심 개념 정리\n",
    "\n",
    "### 8.1 지표별 특성 요약\n",
    "\n",
    "| 지표 | 강점 | 약점 | 적합한 상황 |\n",
    "|------|------|------|------------|\n",
    "| **Hit Rate** | 직관적, 계산 간단 | 순위 무시, 이진적 | 기본 성능 확인 |\n",
    "| **MRR** | 사용자 경험 반영 | 첫 번째만 고려 | QA, 단일 정답 검색 |\n",
    "| **MAP** | 모든 관련문서 고려 | 등급 관련성 미지원 | 포괄적 검색 평가 |\n",
    "| **NDCG** | 가장 정교한 평가 | 복잡함, 등급 필요 | 추천 시스템, 웹검색 |\n",
    "\n",
    "### 8.2 시스템별 추천 지표\n",
    "\n",
    "| 시스템 유형 | 주요 지표 | 이유 |\n",
    "|------------|-----------|------|\n",
    "| **웹 검색 엔진** | NDCG@10, MAP@10, MRR | 다양한 관련성 수준과 순위가 중요 |\n",
    "| **상품 추천** | Hit Rate@5, NDCG@5, MAP@5 | 상위 추천의 정확성이 핵심 |\n",
    "| **문서 검색** | MRR, MAP@10, Hit Rate@3 | 정확한 문서 발견이 우선 |\n",
    "| **FAQ 검색** | MRR, Hit Rate@1, MAP@3 | 빠른 정답 찾기가 중요 |\n",
    "| **이커머스 검색** | NDCG@3, Hit Rate@10, MRR | 상품 관련성과 순위 모두 중요 |\n",
    "\n",
    "### 8.3 성능 해석 가이드라인\n",
    "\n",
    "| 지표 | 우수 | 양호 | 보통 | 개선 필요 |\n",
    "|------|------|------|------|-----------|\n",
    "| **Hit Rate** | ≥ 0.800 | 0.600 - 0.799 | 0.400 - 0.599 | < 0.400 |\n",
    "| **MRR** | ≥ 0.700 | 0.500 - 0.699 | 0.300 - 0.499 | < 0.300 |\n",
    "| **MAP** | ≥ 0.600 | 0.400 - 0.599 | 0.200 - 0.399 | < 0.200 |\n",
    "| **NDCG** | ≥ 0.700 | 0.500 - 0.699 | 0.300 - 0.499 | < 0.300 |\n",
    "\n",
    "\n",
    "### 8.4 성능 개선 가이드\n",
    "\n",
    "- 🔴 개선 필요 (성능 향상 우선순위)\n",
    "    1. **데이터 품질 개선**: 문서 전처리, 중복 제거\n",
    "    2. **임베딩 모델 변경**: 더 적합한 도메인 특화 모델 사용\n",
    "    3. **검색 파라미터 조정**: top-k, 임계값 등\n",
    "\n",
    "- 🟡 보통 (최적화 고려)\n",
    "    1. **하이브리드 검색 도입**: 키워드 + 벡터 검색 결합\n",
    "    2. **리랭킹 시스템 추가**: 재순위화 알고리즘 적용\n",
    "    3. **쿼리 확장**: 동의어, 관련어 추가\n",
    "\n",
    "- 🟢 양호/우수 (미세 조정)\n",
    "    1. **파인튜닝**: 도메인 특화 데이터로 모델 조정\n",
    "    2. **앙상블 방법**: 여러 검색 전략 조합\n",
    "    3. **사용자 피드백 반영**: 클릭률, 만족도 데이터 활용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 9. 추가 학습 자료\n",
    "\n",
    "### 9.1 관련 논문\n",
    "1. **\"Evaluation Measures for Information Retrieval\"** - Christopher Manning\n",
    "2. **\"Normalized Discounted Cumulative Gain\"** - Kalervo Järvelin\n",
    "3. **\"A Comparison of Statistical Significance Tests\"** - Mark Smucker\n",
    "\n",
    "### 9.2 라이브러리\n",
    "- **ranx**: 고성능 평가 라이브러리\n",
    "- **pytrec_eval**: 표준 TREC 평가 도구\n",
    "- **ir_measures**: 통합 평가 메트릭 라이브러리\n",
    "- **PyTerrier**: 정보 검색 실험 플랫폼\n",
    "\n",
    "### 9.3 실무 적용\n",
    "1. **A/B 테스트와 연계**: 오프라인 지표와 온라인 성과의 상관관계 분석\n",
    "2. **비즈니스 지표 연결**: 클릭률, 전환율과 IR 지표의 관계 파악\n",
    "3. **지속적 모니터링**: 실시간 성능 추적 시스템 구축\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktds-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
