{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  쿼리 확장 (Query Expansion)\n",
    "\n",
    "--- \n",
    "\n",
    "## 1. 개념 이해\n",
    "\n",
    "### 1.1 쿼리 확장이란?\n",
    "\n",
    "- 쿼리 확장(Query Expansion)은 사용자의 원본 질문을 보다 효과적인 검색을 위해 변형하거나 확장하는 기법입니다. \n",
    "- RAG 시스템에서 검색의 품질은 최종 답변의 품질을 좌우하는 핵심 요소이므로, 적절한 쿼리 확장 기법을 통해 검색 성능을 크게 향상시킬 수 있습니다.\n",
    "\n",
    "### 1.2 쿼리 확장의 필요성\n",
    "\n",
    "#### 🔍 **검색의 한계**\n",
    "- **어휘 불일치 문제**: 사용자 질문과 문서 간 용어 차이\n",
    "- **모호한 질문**: 불명확하거나 맥락이 부족한 쿼리\n",
    "- **복잡한 질문**: 여러 하위 질문을 포함하는 복합 질문\n",
    "- **의도 파악 어려움**: 사용자의 실제 의도와 표면적 질문의 차이\n",
    "\n",
    "#### 🎯 **해결 방향**\n",
    "- **의미론적 확장**: 동의어, 관련 개념 추가\n",
    "- **구조적 분해**: 복잡한 질문을 단순한 하위 질문으로 분해\n",
    "- **맥락적 확장**: 배경 지식과 일반적 개념 추가\n",
    "- **가상 문서 생성**: 이상적 답변을 통한 검색 개선\n",
    "\n",
    "### 1.3 쿼리 확장 방법론 분류\n",
    "\n",
    "| 방법론 | 핵심 아이디어 | 장점 | 단점 | 적용 상황 |\n",
    "|--------|---------------|------|------|-----------|\n",
    "| **Query Reformulation** | LLM으로 질문 재작성 | 구현 간단, 즉시 적용 | 단일 변형만 생성 | 일반적인 질문 개선 |\n",
    "| **Multi Query** | 다양한 관점의 질문 생성 | 검색 다양성 증가 | 계산 비용 증가 | 모호한 질문 처리 |\n",
    "| **Decomposition** | 복잡한 질문을 하위 질문으로 분해 | 체계적 접근 | 분해 정확도 의존 | 복합적 질문 |\n",
    "| **Step-Back Prompting** | 일반적 맥락에서 구체적 답변으로 | 포괄적 이해 | 추가 검색 필요 | 전문적/복잡한 질문 |\n",
    "| **HyDE** | 가상 답변 문서 생성 | 의미적 정렬 우수 | 환각 위험 | Zero-shot 상황 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정 및 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# LangChain 핵심\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, BaseOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# 검색 및 벡터 저장\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) Langsmith tracing 설정`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langsmith tracing 여부를 확인 (true: langsmith 추적 활성화, false: langsmith 추적 비활성화)\n",
    "import os\n",
    "print(os.getenv('LANGSMITH_TRACING'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(4) 벡터스토어 로드`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vector_store(embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\"), collection_name=\"hybrid_search_db\", persist_directory = \"./local_chroma_db\"):\n",
    "    \"\"\"\n",
    "    기존 벡터 저장소를 로드하거나 새로 생성\n",
    "    \n",
    "    Returns:\n",
    "        Chroma: 벡터 저장소 객체\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        # 기존 벡터 저장소 로드 시도\n",
    "        vector_store = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=embeddings,\n",
    "            persist_directory=persist_directory,\n",
    "        )\n",
    "        \n",
    "        doc_count = vector_store._collection.count()\n",
    "        if doc_count > 0:\n",
    "            print(f\"✅ 기존 벡터 저장소 로드: {doc_count}개 문서\")\n",
    "            return vector_store\n",
    "        else:\n",
    "            print(\"⚠️ 빈 벡터 저장소입니다. 데이터를 추가해주세요.\")\n",
    "            return vector_store\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 벡터 저장소 로드 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "# 벡터 저장소 초기화\n",
    "chroma_db = initialize_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(5) 백터 검색기 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 retriever 초기화\n",
    "chroma_k_retriever = chroma_db.as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "retrieved_docs = chroma_k_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) **Query Reformulation** \n",
    "\n",
    "- **Query Reformulation**은 **LLM**을 활용해 원본 질문을 다양한 형태로 재구성\n",
    "- **동의어 확장**과 **키워드 추가**를 통해 검색 쿼리의 범위를 확장\n",
    "- 모호한 질문을 **명확하게 구체화**하여 검색 정확도 향상\n",
    "- 하나의 질문에 대해 **다양한 변형 쿼리**를 생성하여 검색 커버리지 확대\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/query_rewrite.png\" alt=\"rag\" align=\"center\" border=\"0\"  width=\"800\" height=auto>\n",
    "</center>\n",
    "\n",
    "\n",
    "[출처] https://arxiv.org/abs/2305.14283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 쿼리 리포뮬레이션을 위한 프롬프트 템플릿 정의\n",
    "reformulation_template = \"\"\"다음 질문을 검색 성능을 향상시키기 위해 다시 작성해주세요:\n",
    "[질문]\n",
    "{question}\n",
    "\n",
    "다음 방식으로 질문을 재작성하세요:\n",
    "1. 동의어 추가\n",
    "2. 더 구체적인 키워드 포함\n",
    "3. 관련된 개념 확장\n",
    "\n",
    "[재작성된 질문]\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = ChatPromptTemplate.from_template(reformulation_template)\n",
    "\n",
    "# LLM 모델 초기화\n",
    "llm = ChatOpenAI(model='gpt-4.1-mini', temperature=0)\n",
    "\n",
    "# 쿼리 리포뮬레이션 체인 생성\n",
    "reformulation_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 체인 실행\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "reformulated_query = reformulation_chain.invoke({\"question\": query})\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "pprint(f\"리포뮬레이션된 쿼리: \\n{reformulated_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리포뮬레이션된 쿼리로 검색\n",
    "retrieved_docs = chroma_k_retriever.invoke(reformulated_query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runnable 객체로 변환하여 검색기 생성 (LCEL)\n",
    "reformulation_retriever = reformulation_chain | chroma_k_retriever\n",
    "\n",
    "# 쿼리 리포뮬레이션 검색기 실행\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "retrieved_docs = reformulation_retriever.invoke({\"question\": query})\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_dataset(file_path):\n",
    "    \"\"\"\n",
    "    평가 데이터셋 로드\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 평가 데이터 파일 경로\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: 평가 데이터셋\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(file_path)\n",
    "        elif file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"지원하지 않는 파일 형식\")\n",
    "        \n",
    "        print(f\"✅ 평가 데이터셋 로드: {len(df)}개 질문\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 데이터셋 로드 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "# 평가 데이터셋 로드\n",
    "\n",
    "eval_df = load_evaluation_dataset(\"./data/synthetic_testset.csv\")\n",
    "eval_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_evaluation_data(df):\n",
    "    \"\"\"\n",
    "    평가 데이터 전처리\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): 원본 데이터프레임\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (질문 리스트, 정답 문서 리스트)\n",
    "    \"\"\"\n",
    "    questions = df['user_input'].tolist()\n",
    "    \n",
    "    # 정답 문서 파싱\n",
    "    reference_contexts = []\n",
    "    for contexts in df['reference_contexts']:\n",
    "        if isinstance(contexts, str):\n",
    "            # 문자열을 리스트로 변환\n",
    "            context_list = eval(contexts)\n",
    "        else:\n",
    "            context_list = contexts\n",
    "        \n",
    "        # Document 객체로 변환\n",
    "        docs = [Document(page_content=ctx) for ctx in context_list]\n",
    "        reference_contexts.append(docs)\n",
    "    \n",
    "    return questions, reference_contexts\n",
    "\n",
    "# 평가 데이터 전처리\n",
    "questions, reference_contexts = prepare_evaluation_data(eval_df)\n",
    "\n",
    "# 평가 데이터 확인\n",
    "for i, (q, refs) in enumerate(zip(questions[:3], reference_contexts[:3])):\n",
    "    print(f\"\\n[질문 {i+1}]\")\n",
    "    print(f\"질문: {q}\")\n",
    "    print(f\"정답 문서: {len(refs)}개\")\n",
    "    for j, ref in enumerate(refs):\n",
    "        print(f\"  [{j+1}] 내용: {ref.page_content[:50]}...\")  # 내용 일부만 출력\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 평가 데이터 확인\n",
    "for i, (q, refs) in enumerate(zip(questions[-3:], reference_contexts[-3:])):\n",
    "    print(f\"\\n[질문 {i+1}]\")\n",
    "    print(f\"질문: {q}\")\n",
    "    print(f\"정답 문서: {len(refs)}개\")\n",
    "    for j, ref in enumerate(refs):\n",
    "        print(f\"  [{j+1}] 내용: {ref.page_content[:50]}...\")  # 내용 일부만 출력\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranx-k 라이브러리 사용해서 검색 결과 평가\n",
    "from ranx_k.evaluation import evaluate_with_ranx_similarity\n",
    "\n",
    "# ranx-k 평가 실행 (rouge 점수가 높은 경우) -> 문자열 유사도 기반 평가\n",
    "chroma_results = evaluate_with_ranx_similarity(\n",
    "    retriever=chroma_k_retriever,\n",
    "    questions=questions, \n",
    "    reference_contexts=reference_contexts,\n",
    "    k=5,\n",
    "    method='kiwi_rouge',  \n",
    "    similarity_threshold=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformulation_results = evaluate_with_ranx_similarity(\n",
    "    retriever=reformulation_retriever,\n",
    "    questions=questions, \n",
    "    reference_contexts=reference_contexts,\n",
    "    k=5,\n",
    "    method='kiwi_rouge',  \n",
    "    similarity_threshold=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) **Multi Query** \n",
    "\n",
    "- **Multi Query**는 **Retriever의 LLM**을 사용해 단일 질문을 다수의 쿼리로 확장\n",
    "- 원본 질문에 대해 **다양한 관점**과 **표현 방식**으로 쿼리 자동 생성\n",
    "- **LLM의 생성 능력**을 활용해 검색 범위를 자연스럽게 확장\n",
    "- 검색의 **다양성**과 **포괄성**이 향상되어 관련 문서 검색 확률 증가\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/multi-query.png\" alt=\"rag\" align=\"center\" border=\"0\"  width=\"800\" height=auto>\n",
    "</center>\n",
    "\n",
    "[출처] https://arxiv.org/abs/2411.13154"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) MultiQueryRetriever 활용`\n",
    "\n",
    "- https://python.langchain.com/docs/how_to/MultiQueryRetriever/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티 쿼리 생성\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM 모델 초기화 (멀티 쿼리 생성용)\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4.1-mini',\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# 기본 retriever를 이용한 멀티 쿼리 생성 \n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=chroma_k_retriever, llm=llm\n",
    ")\n",
    "\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "retrieved_docs = multi_query_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) Custom Prompt 활용`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "\n",
    "# 출력 파서: LLM 결과를 질문 리스트로 변환\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Split the text into lines and remove empty lines.\"\"\"\n",
    "        return [line.strip() for line in text.strip().split(\"\\n\") if line.strip()]\n",
    "    \n",
    "\n",
    "# 쿼리 생성 프롬프트\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"Generate three different versions of the given user question to retrieve relevant documents from a vector database. The goal is to reframe the question from various perspectives to overcome limitations of distance-based similarity search.\n",
    "\n",
    "    The generated questions should have the following characteristics:\n",
    "    1. Maintain the core intent of the original question but use different expressions or viewpoints.\n",
    "    2. Include synonyms or related concepts where possible.\n",
    "    3. Slightly broaden or narrow the scope of the question to potentially include diverse relevant information.\n",
    "\n",
    "    Write each question on a new line and include only the questions.\n",
    "\n",
    "    [Original question]\n",
    "    {question}\n",
    "    \n",
    "    [Alternative questions]\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# 멀티쿼리 체인 구성\n",
    "multiquery_chain = QUERY_PROMPT | llm | LineListOutputParser()\n",
    "\n",
    "# 테스트 쿼리 실행\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "result = multiquery_chain.invoke({\"question\": query})\n",
    "\n",
    "print(\"생성된 대안 질문들:\")\n",
    "for i, q in enumerate(result, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 쿼리 검색기 생성\n",
    "multi_query_custom_retriever = MultiQueryRetriever(\n",
    "    retriever=chroma_k_retriever, # 기본 retriever\n",
    "    llm_chain=multiquery_chain,   # 멀티쿼리 체인\n",
    "    parser_key=\"lines\"            # \"lines\": 출력 파서의 키\n",
    ")  \n",
    "\n",
    "retrieved_docs = multi_query_custom_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_query_results = evaluate_with_ranx_similarity(\n",
    "    retriever=multi_query_custom_retriever,\n",
    "    questions=questions, \n",
    "    reference_contexts=reference_contexts,\n",
    "    k=5,\n",
    "    method='kiwi_rouge',  \n",
    "    similarity_threshold=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) **Decomposition** \n",
    "\n",
    "- **단계별 분해 전략**을 통해 복잡한 질문을 작은 단위로 나누어 처리함\n",
    "- 각 하위 질문마다 **독립적인 검색 프로세스**를 진행하여 정확도를 향상시킴\n",
    "- **LEAST-TO-MOST PROMPTING**을 활용하여 체계적인 문제 해결 방식을 구현함\n",
    "- 복잡한 문제를 단순화하여 검색 효율성을 극대화하는 방법론\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/query_decomposition.png\" alt=\"rag\" align=\"center\" border=\"0\"  width=\"800\" height=auto>\n",
    "</center>\n",
    "\n",
    "\n",
    "[출처] https://arxiv.org/pdf/2205.10625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to decompose the given input question into multiple sub-questions. \n",
    "    The goal is to break down the input into a set of sub-problems/sub-questions that can be answered independently.\n",
    "\n",
    "    Follow these guidelines to generate the sub-questions:\n",
    "    1. Cover various aspects related to the core topic of the original question.\n",
    "    2. Each sub-question should be specific, clear, and answerable independently.\n",
    "    3. Ensure that the sub-questions collectively address all important aspects of the original question.\n",
    "    4. Consider temporal aspects (past, present, future) where applicable.\n",
    "    5. Formulate the questions in a direct and concise manner.\n",
    "\n",
    "    [Input question] \n",
    "    {question}\n",
    "\n",
    "    [Sub-questions (5)]\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# 쿼리 생성 체인\n",
    "decomposition_chain = QUERY_PROMPT | llm | LineListOutputParser()\n",
    "\n",
    "# 테스트 쿼리 실행\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "result = decomposition_chain.invoke({\"question\": query})\n",
    "\n",
    "print(\"생성된 서브 질문들:\")\n",
    "for i, q in enumerate(result, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 쿼리 검색기 생성\n",
    "multi_query_decompostion_retriever = MultiQueryRetriever(\n",
    "    retriever=chroma_k_retriever,    # 기본 retriever\n",
    "    llm_chain=decomposition_chain,   # 서브 질문 생성 체인\n",
    "    parser_key=\"lines\"               # \"lines\": 출력 파서의 키\n",
    ")  \n",
    "\n",
    "retrieved_docs = multi_query_decompostion_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_results = evaluate_with_ranx_similarity(\n",
    "    retriever=multi_query_decompostion_retriever,\n",
    "    questions=questions, \n",
    "    reference_contexts=reference_contexts,\n",
    "    k=5,\n",
    "    method='kiwi_rouge',  \n",
    "    similarity_threshold=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) **Step-Back Prompting**\n",
    "\n",
    "- **단계적 후퇴 방식**을 통해 구체적 질문을 일반적 맥락에서 접근함\n",
    "- **맥락 기반 검색**으로 넓은 관점에서 구체적 답변으로 좁혀나감\n",
    "- **포괄적 접근법**을 활용하여 복잡한 질문에 대한 이해도를 높임\n",
    "- 일반적 맥락에서 시작하여 구체적 해답을 찾아가는 체계적 접근 방식\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/query_stepback.png\" alt=\"rag\" align=\"center\" border=\"0\"  width=\"800\" height=auto>\n",
    "</center>\n",
    "\n",
    "\n",
    "[출처] https://arxiv.org/pdf/2310.06117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Step-Back 질문 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# Few Shot 예제 - (구체적 질문, 포괄적 질문) 쌍\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"애플의 M1 칩 개발이 기업 가치에 미친 영향은?\",\n",
    "        \"output\": \"기업의 핵심 기술 내재화가 경쟁우위에 미치는 영향은 무엇인가?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"아마존의 AWS가 수익성에 기여하는 방식은?\",\n",
    "        \"output\": \"기업의 새로운 사업 영역 확장이 수익 구조에 미치는 영향은 무엇인가?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"토요타의 하이브리드 기술 전략의 핵심은?\",\n",
    "        \"output\": \"자동차 산업에서 친환경 기술 혁신이 기업 성장에 미치는 영향은 무엇인가?\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# 프롬프트 템플릿 초기화\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "])\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Step-Back 생성을 위한 프롬프트\n",
    "step_back_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"당신은 기업 분석 전문가입니다. 특정 기업에 대한 구체적인 질문을 해당 산업이나 비즈니스 전반의 일반적인 관점에서 \n",
    "                재해석하는 것이 임무입니다. 산업 동향, 경쟁 구도, 기술 혁신, 사업 모델 등의 관점에서 더 포괄적인 질문으로 \n",
    "                바꾸어 주세요. 다음은 예시입니다:\"\"\"\n",
    "            ),\n",
    "            few_shot_prompt,\n",
    "            (\"user\", \"{question}\"),\n",
    "        ])\n",
    "\n",
    "# Step-Back 체인 생성\n",
    "step_back_chain = step_back_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step-Back 질문 생성\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "step_back_question = step_back_chain.invoke({\"question\": query})\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(f\"Step-Back 질문: {step_back_question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-Back 검색기 생성\n",
    "step_back_retriever = step_back_chain | chroma_k_retriever\n",
    "\n",
    "# Step-Back 검색 실행\n",
    "retrieved_docs = step_back_retriever.invoke({\"question\": query})\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepback_results = evaluate_with_ranx_similarity(\n",
    "    retriever=step_back_retriever,\n",
    "    questions=questions, \n",
    "    reference_contexts=reference_contexts,\n",
    "    k=5,\n",
    "    method='kiwi_rouge',  \n",
    "    similarity_threshold=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 최종 답변 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# 프롬프트 템플릿 초기화\n",
    "response_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"당신은 전문가입니다. 다음 컨텍스트와 질문을 바탕으로 포괄적인 답변을 제공해주세요.\n",
    "\n",
    "            일반 컨텍스트:\n",
    "            {normal_context}\n",
    "            \n",
    "            기본 개념 컨텍스트:\n",
    "            {step_back_context}\n",
    "            \n",
    "            원래 질문: {question}\n",
    "            \n",
    "            답변:\"\"\"\n",
    "        )\n",
    "\n",
    "# 문서 포맷팅 함수\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "\n",
    "# 답변 생성 체인\n",
    "answer_chain = (\n",
    "            {\n",
    "                \"normal_context\": chroma_k_retriever,\n",
    "                \"step_back_context\": step_back_retriever,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | response_prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "# 답변 생성\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "answer = answer_chain.invoke(query)\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(f\"답변: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) **HyDE** (Hypothetical Document Embedding)\n",
    "\n",
    "- **가상 문서 생성**을 통해 주어진 질문에 대해 가상의 이상적인 답변 문서를 LLM으로 생성함\n",
    "- 생성된 문서의 **임베딩 기반 검색**으로 실제 문서와 매칭을 수행함\n",
    "- **맥락 기반 검색 방식**으로 질문의 의도를 더 정확하게 반영함\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/query_HyDE.png\" alt=\"rag\" align=\"center\" border=\"0\"  width=\"800\" height=auto>\n",
    "</center>\n",
    "\n",
    "\n",
    "[출처] https://arxiv.org/abs/2212.10496"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 가상 문서 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# HyDE를 위한 프롬프트 템플릿 생성\n",
    "template = \"\"\"주어진 질문에 대한 이상적인 문서 내용을 생성해주세요.\n",
    "문서는 학술적이고 전문적인 톤으로 작성되어야 합니다.\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "문서 내용:\"\"\"\n",
    "\n",
    "hyde_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM 모델 초기화\n",
    "hyde_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# 문서 생성 체인 생성\n",
    "hyde_chain = hyde_prompt | hyde_llm | StrOutputParser()\n",
    "\n",
    "# 문서 생성 실행\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "hypothetical_doc = hyde_chain.invoke({\"question\": query})\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(f\"문서 내용: {hypothetical_doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 유사 문서 검색`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상 문서를 기반으로 실제 문서 검색\n",
    "    \n",
    "retrieved_docs = chroma_k_retriever.invoke(hypothetical_doc)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) HyDE 검색기 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_retriever = hyde_chain | chroma_k_retriever\n",
    "\n",
    "retrieved_docs = hyde_retriever.invoke({\"question\": query})\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) 최종 답변 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 RAG를 위한 프롬프트 템플릿 생성\n",
    "template = \"\"\"다음 컨텍스트를 바탕으로 질문에 답변해주세요:\n",
    "\n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\"\"\"\n",
    "\n",
    "rag_prompt =  ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# RAG 체인 생성\n",
    "rag_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "rag_chain = rag_prompt | rag_llm | StrOutputParser()\n",
    "    \n",
    "# RAG 실행\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "context = format_docs(retrieved_docs)\n",
    "\n",
    "answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(f\"답변: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(4) HyDE 체인 종합`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. 가상 문서 생성 및 검색\n",
    "query = \"테슬라의 경영진을 분석해주세요.\"\n",
    "\n",
    "# Step 2. 유사 문서 검색\n",
    "retrieved_docs = hyde_retriever.invoke({\"question\": query})\n",
    "\n",
    "# Step 3. 최종 답변 생성\n",
    "final_answer = rag_chain.invoke(\n",
    "    {\n",
    "        \"context\": format_docs(retrieved_docs), \n",
    "        \"question\": query\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(f\"가상 문서 내용: {hypothetical_doc}\")\n",
    "print(f\"답변: {final_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kt-ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
