{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  정보 검색 평가지표(HitRate, MRR, NDCG)\n",
    "\n",
    "- 정보 검색 시스템의 주요 평가지표 이해\n",
    "- Hit Rate, MRR, MAP, NDCG의 계산 방법 및 의미 파악\n",
    "- 최신 Python 라이브러리를 활용한 실습\n",
    "- RAG 시스템 평가에 적용\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 1. 기본 개념 및 배경\n",
    "\n",
    "### 1.1 정보 검색 평가의 중요성\n",
    "\n",
    "정보 검색(Information Retrieval) 시스템의 성능을 평가하는 것은 다음과 같은 이유로 중요합니다:\n",
    "\n",
    "- **사용자 만족도**: 관련성 높은 문서를 상위에 배치하여 사용자 경험 향상\n",
    "- **시스템 개선**: 정량적 지표를 통한 객관적 성능 비교\n",
    "- **비즈니스 가치**: 검색 품질 향상을 통한 클릭률, 전환율 개선\n",
    "\n",
    "### 1.2 평가 지표의 분류\n",
    "\n",
    "#### 순위 인식 여부 (Rank-Aware vs Rank-Unaware)\n",
    "- **Rank-Unaware**: 검색 결과의 순서를 고려하지 않음 (Precision, Recall)\n",
    "- **Rank-Aware**: 검색 결과의 순서를 고려함 (MRR, MAP, NDCG)\n",
    "\n",
    "#### 평가 방식\n",
    "- **이진 관련성**: 관련 있음(1) 또는 없음(0)\n",
    "- **등급 관련성**: 다단계 점수 (1~5점 등)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 2. 환경 설정\n",
    "\n",
    "### 2.1 라이브러리 임포트\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 라이브러리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 정보검색(IR) 평가 라이브러리 소개\n",
    "\n",
    "- **ranx**: 고속 평가 라이브러리 (2022~)\n",
    "    - Numba 기반 고성능 구현\n",
    "    - 통계적 유의성 검정 지원\n",
    "    - LaTeX 테이블 자동 생성\n",
    "\n",
    "- **pytrec_eval**: 표준 평가 도구\n",
    "    - TREC 공식 평가 도구의 Python 인터페이스\n",
    "    - 학술 연구에서 널리 사용\n",
    "\n",
    "- **ir-measures**: 정보 검색 평가 라이브러리\n",
    "    - 다양한 평가 지표 지원\n",
    "    - 사용자 정의 지표 생성 가능\n",
    "\n",
    "- **ranx 설치**:\n",
    "\n",
    "    ```python\n",
    "    # 평가 전용 라이브러리\n",
    "    !pip install ranx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 3. 샘플 데이터 준비\n",
    "\n",
    "### 3.1 검색 시나리오 설정\n",
    "\n",
    "고객 서비스 문의 검색 시스템을 평가하는 상황을 가정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from textwrap import dedent\n",
    "\n",
    "# 문서 데이터베이스 (전체 10개 문서로 확장)\n",
    "document_pool = [\n",
    "    Document(\n",
    "        page_content=\"배송 지연 문의 - 주문 상품의 배송이 예상보다 늦어지고 있습니다.\",\n",
    "        metadata={\"id\": \"doc1\", \"category\": \"배송\", \"priority\": \"높음\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"결제 오류 - 카드 결제 시 오류가 발생하여 주문이 완료되지 않았습니다.\",\n",
    "        metadata={\"id\": \"doc2\", \"category\": \"결제\", \"priority\": \"높음\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"제품 교환 - 사이즈가 맞지 않아 다른 사이즈로 교환하고 싶습니다.\",\n",
    "        metadata={\"id\": \"doc3\", \"category\": \"교환/반품\", \"priority\": \"중간\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"주문 취소 및 환불 - 주문을 취소하고 전액 환불받고 싶습니다.\",\n",
    "        metadata={\"id\": \"doc4\", \"category\": \"취소/환불\", \"priority\": \"중간\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"포인트 적립 문의 - 구매 후 포인트가 적립되지 않았습니다.\",\n",
    "        metadata={\"id\": \"doc5\", \"category\": \"포인트\", \"priority\": \"낮음\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"재고 문의 - 원하는 상품이 품절인데 언제 재입고되나요?\",\n",
    "        metadata={\"id\": \"doc6\", \"category\": \"재고\", \"priority\": \"중간\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"회원가입 문의 - 회원가입 시 인증번호가 오지 않습니다.\",\n",
    "        metadata={\"id\": \"doc7\", \"category\": \"회원\", \"priority\": \"낮음\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"쿠폰 사용 문의 - 보유한 쿠폰이 적용되지 않습니다.\",\n",
    "        metadata={\"id\": \"doc8\", \"category\": \"쿠폰\", \"priority\": \"낮음\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"배송 주소 변경 - 배송 전에 주소를 변경하고 싶습니다.\",\n",
    "        metadata={\"id\": \"doc9\", \"category\": \"배송\", \"priority\": \"중간\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"상품 리뷰 문의 - 구매한 상품에 대한 리뷰 작성 방법을 알고 싶습니다.\",\n",
    "        metadata={\"id\": \"doc10\", \"category\": \"리뷰\", \"priority\": \"낮음\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# 검색 쿼리와 정답 (현실적인 시나리오)\n",
    "queries_and_ground_truth = [\n",
    "    {\n",
    "        \"query\": \"배송 늦어요\",\n",
    "        \"relevant_docs\": [\"doc1\", \"doc9\"]  # 배송 관련 문서들\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"결제가 안 되고 포인트도 문제가 있어요\", \n",
    "        \"relevant_docs\": [\"doc2\", \"doc5\"]  # 결제 + 포인트 관련\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"주문 취소하고 싶어요\",\n",
    "        \"relevant_docs\": [\"doc4\"]  # 취소 관련만\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"교환 환불 정책이 궁금해요\",\n",
    "        \"relevant_docs\": [\"doc3\", \"doc4\"]  # 교환 + 환불 관련\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"쿠폰이 왜 안 써져요?\",\n",
    "        \"relevant_docs\": [\"doc8\"]  # 쿠폰 관련만\n",
    "    }\n",
    "]\n",
    "\n",
    "# 시스템 검색 결과 (현실적인 성능 차이 반영)\n",
    "system_results = [\n",
    "    # Query 1: \"배송 늦어요\" - 좋은 성능 (정답 1, 2위)\n",
    "    [\"doc1\", \"doc9\", \"doc6\", \"doc2\", \"doc7\"],\n",
    "    \n",
    "    # Query 2: \"결제가 안 되고 포인트도 문제가 있어요\" - 보통 성능 (정답이 2, 4위)\n",
    "    [\"doc7\", \"doc2\", \"doc3\", \"doc5\", \"doc1\"],\n",
    "    \n",
    "    # Query 3: \"주문 취소하고 싶어요\" - 나쁜 성능 (정답이 5위)\n",
    "    [\"doc3\", \"doc6\", \"doc2\", \"doc1\", \"doc4\"],\n",
    "    \n",
    "    # Query 4: \"교환 환불 정책이 궁금해요\" - 혼재된 성능 (정답이 1, 6위)\n",
    "    [\"doc3\", \"doc7\", \"doc5\", \"doc8\", \"doc2\", \"doc4\"],\n",
    "    \n",
    "    # Query 5: \"쿠폰이 왜 안 써져요?\" - 매우 나쁜 성능 (정답이 없음!)\n",
    "    [\"doc5\", \"doc2\", \"doc7\", \"doc1\", \"doc10\"]\n",
    "]\n",
    "\n",
    "print(\"검색 시나리오 설정 완료!\")\n",
    "print(f\"총 문서 수: {len(document_pool)}\")\n",
    "print(f\"평가 쿼리 수: {len(queries_and_ground_truth)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📈 4. 평가 지표 상세 설명\n",
    "\n",
    "### 4.1 Hit Rate (적중률)\n",
    "\n",
    "#### 개념\n",
    "**Hit Rate@k**는 상위 k개 검색 결과에 관련 문서가 **하나라도** 포함되어 있는지 측정하는 지표입니다.\n",
    "\n",
    "#### 특징\n",
    "- **이진 평가**: 있음(1) 또는 없음(0)\n",
    "- **순서 무관**: 관련 문서의 위치는 고려하지 않음\n",
    "- **직관적**: 가장 이해하기 쉬운 지표\n",
    "\n",
    "#### 계산 공식\n",
    "\n",
    "```markdown\n",
    "Hit Rate@k = (적중한 쿼리 수) / (전체 쿼리 수)\n",
    "\n",
    "각 쿼리의 적중 여부 = 1 if 상위 k개에 관련 문서 존재 else 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hit_rate(ground_truth: List[List[str]], \n",
    "                      predictions: List[List[str]], \n",
    "                      k: int) -> float:\n",
    "    \"\"\"\n",
    "    Hit Rate@k 계산\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    total_queries = len(ground_truth)\n",
    "    \n",
    "    print(f\"=== Hit Rate@{k} 계산 과정 ===\")\n",
    "    \n",
    "    for i, (gt, pred) in enumerate(zip(ground_truth, predictions)):\n",
    "        # 상위 k개 예측 결과\n",
    "        top_k_pred = pred[:k]\n",
    "        \n",
    "        # 관련 문서가 하나라도 있으면 적중\n",
    "        hit = any(doc in gt for doc in top_k_pred)\n",
    "        if hit:\n",
    "            hits += 1\n",
    "        \n",
    "        print(f\"Query {i+1}: GT={gt}, Pred@{k}={top_k_pred}\")\n",
    "        print(f\"         Hit: {'O' if hit else 'X'}\")\n",
    "    \n",
    "    hit_rate = hits / total_queries\n",
    "    print(f\"\\n총 적중: {hits}/{total_queries} = {hit_rate:.3f}\")\n",
    "    return hit_rate\n",
    "\n",
    "# 실제 계산\n",
    "ground_truth = [query[\"relevant_docs\"] for query in queries_and_ground_truth]\n",
    "\n",
    "print(\"=== Hit Rate 계산 결과 ===\")\n",
    "for k in [1, 3, 5]:\n",
    "    hit_rate = calculate_hit_rate(ground_truth, system_results, k)\n",
    "    print(f\"Hit Rate@{k}: {hit_rate:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**예상 출력 분석:**\n",
    "```markdown\n",
    "Hit Rate@1: 0.400  # Query 1, 4만 1위에 정답 (5개 중 2개)\n",
    "Hit Rate@3: 0.600  # Query 1, 2, 4만 상위 3개에 정답 (5개 중 3개)  \n",
    "Hit Rate@5: 0.800  # Query 5만 상위 5개에도 정답 없음 (5개 중 4개)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 MRR (Mean Reciprocal Rank)\n",
    "\n",
    "#### 개념\n",
    "**MRR**은 **첫 번째 관련 문서의 순위**에 기반한 평가 지표입니다. 사용자가 원하는 정보를 얼마나 빨리 찾을 수 있는지를 측정합니다.\n",
    "\n",
    "#### 특징\n",
    "- **순위 고려**: 상위에 있을수록 높은 점수\n",
    "- **첫 번째만**: 첫 관련 문서 이후는 무시\n",
    "- **사용자 경험**: 실제 검색 행동과 유사\n",
    "\n",
    "#### 계산 공식\n",
    "```markdown\n",
    "MRR = (1/Q) × Σ(1/rank_i)\n",
    "\n",
    "여기서:\n",
    "- Q: 전체 쿼리 수\n",
    "- rank_i: i번째 쿼리에서 첫 번째 관련 문서의 순위\n",
    "- 관련 문서가 없으면 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(ground_truth: List[List[str]], \n",
    "                  predictions: List[List[str]], \n",
    "                  k: int = None) -> float:\n",
    "    \"\"\"\n",
    "    Mean Reciprocal Rank 계산\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    print(f\"=== MRR 계산 과정 ===\")\n",
    "    \n",
    "    for i, (gt, pred) in enumerate(zip(ground_truth, predictions)):\n",
    "        # 상위 k개만 고려 (k가 None이면 전체)\n",
    "        search_list = pred[:k] if k else pred\n",
    "        \n",
    "        # 첫 번째 관련 문서의 순위 찾기\n",
    "        first_relevant_rank = None\n",
    "        for rank, doc_id in enumerate(search_list, 1):\n",
    "            if doc_id in gt:\n",
    "                first_relevant_rank = rank\n",
    "                break\n",
    "        \n",
    "        # Reciprocal Rank 계산\n",
    "        rr = 1.0 / first_relevant_rank if first_relevant_rank else 0.0\n",
    "        reciprocal_ranks.append(rr)\n",
    "        \n",
    "        print(f\"Query {i+1}: GT={gt}\")\n",
    "        print(f\"         Pred={search_list[:5]}\")\n",
    "        print(f\"         첫 관련문서 순위: {first_relevant_rank}, RR: {rr:.3f}\")\n",
    "    \n",
    "    mrr = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "    print(f\"\\n평균 RR: {sum(reciprocal_ranks):.3f} / {len(reciprocal_ranks)} = {mrr:.3f}\")\n",
    "    return mrr\n",
    "\n",
    "# 실제 계산\n",
    "print(\"=== MRR 계산 결과 ===\")\n",
    "mrr_score = calculate_mrr(ground_truth, system_results, k=10)\n",
    "print(f\"최종 MRR: {mrr_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**예상 출력 분석:**\n",
    "```markdown\n",
    "Query 1: 첫 관련문서 순위: 1, RR: 1.000  # doc1이 1위\n",
    "Query 2: 첫 관련문서 순위: 2, RR: 0.500  # doc2가 2위  \n",
    "Query 3: 첫 관련문서 순위: 5, RR: 0.200  # doc4가 5위\n",
    "Query 4: 첫 관련문서 순위: 1, RR: 1.000  # doc3이 1위\n",
    "Query 5: 첫 관련문서 순위: None, RR: 0.000  # 관련문서 없음\n",
    "\n",
    "최종 MRR: (1.000 + 0.500 + 0.200 + 1.000 + 0.000) / 5 = 0.540\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 MAP (Mean Average Precision)\n",
    "\n",
    "#### 개념\n",
    "**MAP@k**는 상위 k개 결과에서 **모든 관련 문서의 정확도를 종합적으로 평가**하는 지표입니다.\n",
    "\n",
    "#### 특징\n",
    "- **순위 고려**: 상위에 있는 관련 문서에 더 높은 가중치\n",
    "- **전체 고려**: 모든 관련 문서의 위치를 반영\n",
    "- **정확도 중심**: 정밀도(Precision)의 평균\n",
    "\n",
    "#### 계산 공식\n",
    "```markdown\n",
    "MAP@k = (1/Q) × Σ AP@k_i\n",
    "\n",
    "AP@k = (1/R) × Σ P(j) × rel(j)\n",
    "\n",
    "여기서:\n",
    "- Q: 전체 쿼리 수\n",
    "- R: 관련 문서 수\n",
    "- P(j): j번째 위치에서의 정밀도\n",
    "- rel(j): j번째 문서가 관련 있으면 1, 아니면 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map_at_k(ground_truth: List[List[str]], \n",
    "                       predictions: List[List[str]], \n",
    "                       k: int) -> float:\n",
    "    \"\"\"\n",
    "    Mean Average Precision@k 계산\n",
    "    \"\"\"\n",
    "    average_precisions = []\n",
    "    \n",
    "    print(f\"=== MAP@{k} 계산 과정 ===\")\n",
    "    \n",
    "    for i, (gt, pred) in enumerate(zip(ground_truth, predictions)):\n",
    "        # 상위 k개만 고려\n",
    "        top_k_pred = pred[:k]\n",
    "        \n",
    "        # Average Precision 계산\n",
    "        relevant_count = 0\n",
    "        precision_sum = 0.0\n",
    "        \n",
    "        print(f\"\\nQuery {i+1}: GT={gt}, Pred={top_k_pred}\")\n",
    "        \n",
    "        for rank, doc_id in enumerate(top_k_pred, 1):\n",
    "            if doc_id in gt:\n",
    "                relevant_count += 1\n",
    "                precision_at_rank = relevant_count / rank\n",
    "                precision_sum += precision_at_rank\n",
    "                print(f\"  위치 {rank}: {doc_id} (관련O) - P@{rank} = {precision_at_rank:.3f}\")\n",
    "            else:\n",
    "                print(f\"  위치 {rank}: {doc_id} (관련X)\")\n",
    "        \n",
    "        # AP 계산: 관련 문서 수로 나누어 정규화\n",
    "        ap = precision_sum / len(gt) if len(gt) > 0 else 0.0\n",
    "        average_precisions.append(ap)\n",
    "        print(f\"  AP@{k} = {precision_sum:.3f} / {len(gt)} = {ap:.3f}\")\n",
    "    \n",
    "    map_score = sum(average_precisions) / len(average_precisions)\n",
    "    print(f\"\\n최종 MAP@{k}: {sum(average_precisions):.3f} / {len(average_precisions)} = {map_score:.3f}\")\n",
    "    return map_score\n",
    "\n",
    "# 실제 계산\n",
    "print(\"=== MAP@k 계산 결과 ===\")\n",
    "for k in [3, 5]:\n",
    "    map_score = calculate_map_at_k(ground_truth, system_results, k)\n",
    "    print(f\"MAP@{k}: {map_score:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**예상 출력 분석:**\n",
    "```markdown\n",
    "Query 1: AP@3 = (1.000 + 1.000) / 2 = 1.000  # 1,2위 모두 정답\n",
    "Query 2: AP@3 = (0.500) / 2 = 0.250           # 2위만 정답\n",
    "Query 3: AP@3 = 0.000 / 1 = 0.000             # 3위 안에 정답 없음  \n",
    "Query 4: AP@3 = (1.000) / 2 = 0.500           # 1위만 정답\n",
    "Query 5: AP@3 = 0.000 / 1 = 0.000             # 3위 안에 정답 없음\n",
    "\n",
    "MAP@3: (1.000 + 0.250 + 0.000 + 0.500 + 0.000) / 5 = 0.350\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 NDCG (Normalized Discounted Cumulative Gain)\n",
    "\n",
    "#### 개념\n",
    "**NDCG@k**는 관련성 점수와 순위를 모두 고려한 **가장 정교한 평가 지표**입니다.\n",
    "\n",
    "#### 특징\n",
    "- **등급 관련성**: 다단계 관련성 점수 지원\n",
    "- **위치 할인**: 하위 순위일수록 가중치 감소\n",
    "- **정규화**: 이상적인 순위와 비교하여 0~1 범위\n",
    "\n",
    "#### 계산 공식\n",
    "```markdown\n",
    "NDCG@k = DCG@k / IDCG@k\n",
    "\n",
    "DCG@k = Σ(i=1 to k) (2^rel_i - 1) / log₂(i + 1)\n",
    "\n",
    "여기서:\n",
    "- rel_i: i번째 문서의 관련성 점수\n",
    "- IDCG@k: 이상적인 순위에서의 DCG   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_ndcg_at_k(ground_truth: List[List[str]], \n",
    "                        predictions: List[List[str]], \n",
    "                        k: int,\n",
    "                        relevance_scores: Dict[str, Dict[str, int]] = None) -> float:\n",
    "    \"\"\"\n",
    "    NDCG@k 계산 (이진 관련성 또는 등급 관련성)\n",
    "    \"\"\"\n",
    "    if relevance_scores is None:\n",
    "        # 이진 관련성: 관련 있으면 1, 없으면 0\n",
    "        relevance_scores = {}\n",
    "        for i, gt in enumerate(ground_truth):\n",
    "            query_id = f\"query_{i}\"\n",
    "            relevance_scores[query_id] = {doc: 1 for doc in gt}\n",
    "    \n",
    "    ndcg_scores = []\n",
    "    \n",
    "    print(f\"=== NDCG@{k} 계산 과정 ===\")\n",
    "    \n",
    "    for i, (gt, pred) in enumerate(zip(ground_truth, predictions)):\n",
    "        query_id = f\"query_{i}\"\n",
    "        top_k_pred = pred[:k]\n",
    "        \n",
    "        print(f\"\\nQuery {i+1}: GT={gt}, Pred={top_k_pred}\")\n",
    "        \n",
    "        # DCG 계산\n",
    "        dcg = 0.0\n",
    "        for rank, doc_id in enumerate(top_k_pred, 1):\n",
    "            rel_score = relevance_scores.get(query_id, {}).get(doc_id, 0)\n",
    "            if rel_score > 0:\n",
    "                gain = (2**rel_score - 1) / math.log2(rank + 1)\n",
    "                dcg += gain\n",
    "                print(f\"  위치 {rank}: {doc_id} (관련성={rel_score}) - Gain: {gain:.3f}\")\n",
    "            else:\n",
    "                print(f\"  위치 {rank}: {doc_id} (관련성=0)\")\n",
    "        \n",
    "        # IDCG 계산 (이상적인 순위)\n",
    "        ideal_scores = sorted(relevance_scores.get(query_id, {}).values(), \n",
    "                             reverse=True)[:k]\n",
    "        idcg = 0.0\n",
    "        for rank, rel_score in enumerate(ideal_scores, 1):\n",
    "            if rel_score > 0:\n",
    "                idcg += (2**rel_score - 1) / math.log2(rank + 1)\n",
    "        \n",
    "        # NDCG 계산\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "        ndcg_scores.append(ndcg)\n",
    "        \n",
    "        print(f\"  DCG: {dcg:.3f}, IDCG: {idcg:.3f}, NDCG: {ndcg:.3f}\")\n",
    "    \n",
    "    final_ndcg = sum(ndcg_scores) / len(ndcg_scores)\n",
    "    print(f\"\\n최종 NDCG@{k}: {sum(ndcg_scores):.3f} / {len(ndcg_scores)} = {final_ndcg:.3f}\")\n",
    "    return final_ndcg\n",
    "\n",
    "# 실제 계산 (이진 관련성)\n",
    "print(\"=== NDCG@k 계산 결과 ===\")\n",
    "for k in [3, 5]:\n",
    "    ndcg_score = calculate_ndcg_at_k(ground_truth, system_results, k)\n",
    "    print(f\"NDCG@{k}: {ndcg_score:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 5. 지표별 특성 비교 분석\n",
    "\n",
    "- **Hit Rate**: 검색 시스템의 기본 성능 확인\n",
    "- **MRR**: 사용자가 빠르게 답을 찾는 것이 중요한 경우\n",
    "- **MAP**: 모든 관련 문서의 순위가 중요한 경우\n",
    "- **NDCG**: 등급별 관련성과 순위 모두 고려해야 하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_evaluation():\n",
    "    \"\"\"모든 지표를 한번에 계산하여 특성 비교\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"        종합 평가 결과 비교\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Hit Rate 계산\n",
    "    for k in [1, 3, 5]:\n",
    "        hr = calculate_hit_rate(ground_truth, system_results, k)\n",
    "        results[f\"Hit_Rate@{k}\"] = hr\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # MRR 계산\n",
    "    mrr = calculate_mrr(ground_truth, system_results)\n",
    "    results[\"MRR\"] = mrr\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # MAP 계산  \n",
    "    for k in [3, 5]:\n",
    "        map_score = calculate_map_at_k(ground_truth, system_results, k)\n",
    "        results[f\"MAP@{k}\"] = map_score\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # NDCG 계산\n",
    "    for k in [3, 5]:\n",
    "        ndcg = calculate_ndcg_at_k(ground_truth, system_results, k)\n",
    "        results[f\"NDCG@{k}\"] = ndcg\n",
    "    \n",
    "    # 결과 요약 테이블\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"            최종 결과 요약\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'지표':<12} {'점수':<8} {'해석'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    interpretations = {\n",
    "        \"Hit_Rate@1\": \"첫 번째 결과의 유용성\",\n",
    "        \"Hit_Rate@3\": \"상위 3개 결과의 포괄성\", \n",
    "        \"Hit_Rate@5\": \"상위 5개 결과의 포괄성\",\n",
    "        \"MRR\": \"첫 관련문서 발견 속도\",\n",
    "        \"MAP@3\": \"상위 3개의 정확도\", \n",
    "        \"MAP@5\": \"상위 5개의 정확도\",\n",
    "        \"NDCG@3\": \"상위 3개의 종합 품질\",\n",
    "        \"NDCG@5\": \"상위 5개의 종합 품질\"\n",
    "    }\n",
    "    \n",
    "    for metric, score in results.items():\n",
    "        interp = interpretations.get(metric, \"\")\n",
    "        print(f\"{metric:<12} {score:<8.3f} {interp}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 종합 평가 실행\n",
    "evaluation_results = comprehensive_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🛠️ 6. 정보검색(IR) 라이브러리 활용\n",
    "\n",
    "- ranx 라이브러리 사용 (pip install ranx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranx import Qrels, Run, evaluate, compare\n",
    "import logging\n",
    "\n",
    "def convert_to_ranx_format(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    데이터를 ranx 형식으로 변환 \n",
    "    \n",
    "    Args:\n",
    "        ground_truth: 각 쿼리에 대한 관련 정답 문서 ID의 리스트로 구성된 리스트\n",
    "        predictions: 각 쿼리에 대한 예측된 문서 ID의 리스트로 구성된 리스트 (순위 기반)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (qrels_dict, run_dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    qrels_dict = {}\n",
    "    run_dict = {}\n",
    "    \n",
    "    # 입력 데이터 길이 검증\n",
    "    if len(ground_truth) != len(predictions):\n",
    "        raise ValueError(f\"Ground truth length ({len(ground_truth)}) != predictions length ({len(predictions)})\")\n",
    "    \n",
    "    for i, (gt_docs, pred_docs) in enumerate(zip(ground_truth, predictions)):\n",
    "        query_id = f\"q_{i+1}\"\n",
    "        \n",
    "        # Ground truth 처리\n",
    "        if gt_docs:\n",
    "            # 문서 ID를 문자열로 통일하고 중복 제거\n",
    "            qrels_dict[query_id] = {str(doc_id): 1 for doc_id in set(gt_docs)}\n",
    "        else:\n",
    "            # 빈 ground truth - 빈 딕셔너리로 유지\n",
    "            qrels_dict[query_id] = {}\n",
    "            logging.warning(f\"Query {query_id} has empty ground truth\")\n",
    "        \n",
    "        # Predictions 처리\n",
    "        if pred_docs:\n",
    "            # 문서 ID를 문자열로 통일하고 순위 기반 점수 부여\n",
    "            # 중복 제거 및 순서 유지\n",
    "            seen = set()\n",
    "            unique_pred_docs = []\n",
    "            for doc_id in pred_docs:\n",
    "                if doc_id not in seen:\n",
    "                    unique_pred_docs.append(doc_id)\n",
    "                    seen.add(doc_id)\n",
    "            \n",
    "            run_dict[query_id] = {\n",
    "                str(doc_id): len(unique_pred_docs) - rank \n",
    "                for rank, doc_id in enumerate(unique_pred_docs)\n",
    "            }\n",
    "        else:\n",
    "            # 빈 예측 결과\n",
    "            run_dict[query_id] = {}\n",
    "            logging.warning(f\"Query {query_id} has empty predictions\")\n",
    "    \n",
    "    return qrels_dict, run_dict\n",
    "\n",
    "def validate_data_consistency(qrels_dict, run_dict):\n",
    "    \"\"\"데이터 일관성 검증\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # 쿼리 ID 일치 확인\n",
    "    qrels_queries = set(qrels_dict.keys())\n",
    "    run_queries = set(run_dict.keys())\n",
    "    \n",
    "    if qrels_queries != run_queries:\n",
    "        missing_in_run = qrels_queries - run_queries\n",
    "        missing_in_qrels = run_queries - qrels_queries\n",
    "        \n",
    "        if missing_in_run:\n",
    "            issues.append(f\"Queries missing in run: {missing_in_run}\")\n",
    "        if missing_in_qrels:\n",
    "            issues.append(f\"Queries missing in qrels: {missing_in_qrels}\")\n",
    "    \n",
    "    # 빈 결과 확인\n",
    "    empty_qrels = [q for q, docs in qrels_dict.items() if not docs]\n",
    "    empty_runs = [q for q, docs in run_dict.items() if not docs]\n",
    "    \n",
    "    if empty_qrels:\n",
    "        issues.append(f\"Queries with empty ground truth: {empty_qrels}\")\n",
    "    if empty_runs:\n",
    "        issues.append(f\"Queries with empty predictions: {empty_runs}\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "def evaluate_with_ranx(ground_truth, system_results, custom_metrics=None):\n",
    "    \"\"\"\n",
    "    ranx를 사용한 검색 시스템 평가 \n",
    "    \n",
    "    Args:\n",
    "        ground_truth: 각 쿼리에 대한 관련 정답 문서 ID의 리스트로 구성된 리스트\n",
    "        system_results: 각 쿼리에 대한 예측된 문서 ID의 리스트로 구성된 리스트 (순위 기반)\n",
    "        custom_metrics: 평가할 사용자 정의 메트릭의 리스트 (선택 사항)\n",
    "\n",
    "    Returns:\n",
    "        dict: 평가 결과\n",
    "    \"\"\"\n",
    "    \n",
    "    # 데이터 변환\n",
    "    try:\n",
    "        qrels_dict, run_dict = convert_to_ranx_format(ground_truth, system_results)\n",
    "    except ValueError as e:\n",
    "        print(f\"데이터 변환 오류: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 데이터 일관성 검증\n",
    "    issues = validate_data_consistency(qrels_dict, run_dict)\n",
    "    if issues:\n",
    "        print(\"⚠️  데이터 일관성 문제 발견:\")\n",
    "        for issue in issues:\n",
    "            print(f\"   - {issue}\")\n",
    "        print()\n",
    "    \n",
    "    # ranx 객체 생성\n",
    "    try:\n",
    "        qrels = Qrels(qrels_dict, name=\"Customer_Service_Queries\")\n",
    "        run = Run(run_dict, name=\"Search_System_v1\")\n",
    "    except Exception as e:\n",
    "        print(f\"ranx 객체 생성 오류: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 메트릭 정의 (ranx에서 지원하는 정확한 메트릭 이름 사용)\n",
    "    if custom_metrics is None:\n",
    "        metrics = [\n",
    "            \"hit_rate@1\", \"hit_rate@3\", \"hit_rate@5\", \n",
    "            \"mrr\", \n",
    "            \"map@3\", \"map@5\", \n",
    "            \"ndcg@3\", \"ndcg@5\",\n",
    "            \"precision@1\", \"precision@3\", \"precision@5\",\n",
    "            \"recall@3\", \"recall@5\"\n",
    "        ]\n",
    "    else:\n",
    "        metrics = custom_metrics\n",
    "    \n",
    "    # 평가 실행\n",
    "    try:\n",
    "        results = evaluate(qrels, run, metrics)\n",
    "        \n",
    "        print(\"ranx 라이브러리 계산 결과:\")\n",
    "        print(\"-\" * 40)\n",
    "        for metric, score in results.items():\n",
    "            print(f\"{metric:<15}: {score:.4f}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"평가 중 오류 발생: {e}\")\n",
    "        print(\"지원되지 않는 메트릭이 포함되어 있을 수 있습니다.\")\n",
    "        \n",
    "        # 개별 메트릭으로 재시도\n",
    "        results = {}\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                score = evaluate(qrels, run, metric)\n",
    "                results[metric] = score\n",
    "                print(f\"{metric:<15}: {score:.4f}\")\n",
    "            except Exception as metric_error:\n",
    "                print(f\"{metric:<15}: ERROR - {metric_error}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# 평가 실행\n",
    "print(f\"관련 정답 문서: {ground_truth}\")\n",
    "print(f\"시스템 결과: {system_results}\")\n",
    "\n",
    "results = evaluate_with_ranx(\n",
    "    ground_truth, \n",
    "    system_results,\n",
    "    custom_metrics=[\"hit_rate@1\", \"hit_rate@3\", \"hit_rate@5\", \"mrr\", \"map@3\",  \"map@5\", \"ndcg@3\", \"ndcg@5\"]\n",
    ")\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n평가 완료: {len(results)}개 메트릭 계산됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎓 7. 핵심 개념 정리\n",
    "\n",
    "### 7.1 지표별 특성 요약\n",
    "\n",
    "| 지표 | 강점 | 약점 | 적합한 상황 |\n",
    "|------|------|------|------------|\n",
    "| **Hit Rate** | 직관적, 계산 간단 | 순위 무시, 이진적 | 기본 성능 확인 |\n",
    "| **MRR** | 사용자 경험 반영 | 첫 번째만 고려 | QA, 단일 정답 검색 |\n",
    "| **MAP** | 모든 관련문서 고려 | 등급 관련성 미지원 | 포괄적 검색 평가 |\n",
    "| **NDCG** | 가장 정교한 평가 | 복잡함, 등급 필요 | 추천 시스템, 웹검색 |\n",
    "\n",
    "### 7.2 시스템별 추천 지표\n",
    "\n",
    "| 시스템 유형 | 주요 지표 | 이유 |\n",
    "|------------|-----------|------|\n",
    "| **웹 검색 엔진** | NDCG@10, MAP@10, MRR | 다양한 관련성 수준과 순위가 중요 |\n",
    "| **상품 추천** | Hit Rate@5, NDCG@5, MAP@5 | 상위 추천의 정확성이 핵심 |\n",
    "| **문서 검색** | MRR, MAP@10, Hit Rate@3 | 정확한 문서 발견이 우선 |\n",
    "| **FAQ 검색** | MRR, Hit Rate@1, MAP@3 | 빠른 정답 찾기가 중요 |\n",
    "| **이커머스 검색** | NDCG@3, Hit Rate@10, MRR | 상품 관련성과 순위 모두 중요 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kt-ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
