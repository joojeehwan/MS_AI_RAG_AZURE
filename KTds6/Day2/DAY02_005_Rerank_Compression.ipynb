{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  재순위화(Re-rank) 기법, 맥락 압축(Contextual Compression) 기법\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 개념 이해\n",
    "\n",
    "### 1.1 RAG 파이프라인의 한계점\n",
    "\n",
    "기본적인 RAG 시스템에서는 벡터 유사도만으로 문서를 검색하고 반환하는데, 이 과정에서 다음과 같은 문제점들이 발생합니다:\n",
    "\n",
    "#### 🚨 **주요 문제점들**\n",
    "\n",
    "1. **의미적 유사도의 한계**: 벡터 유사도가 항상 실제 관련성과 일치하지 않음\n",
    "2. **순위 정확도 문제**: 가장 관련성 높은 문서가 상위에 오지 않을 수 있음\n",
    "3. **노이즈 정보 포함**: 검색된 문서에 불필요한 정보가 많이 포함됨\n",
    "4. **컨텍스트 길이 제한**: LLM의 컨텍스트 윈도우 한계로 인한 정보 손실\n",
    "5. **비용 효율성**: 불필요한 토큰으로 인한 API 비용 증가\n",
    "\n",
    "### 1.2 해결책: 이중 단계 처리\n",
    "\n",
    "#### 🔄 **Two-Stage Retrieval Process**\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Query] --> B[1차 검색<br/>벡터 유사도]\n",
    "    B --> C[대량의 후보 문서<br/>k=50~100개]\n",
    "    C --> D[2차 처리<br/>Re-ranking/Compression]\n",
    "    D --> E[정제된 결과<br/>k=3~10개]\n",
    "    E --> F[LLM]\n",
    "```\n",
    "\n",
    "### 1.3 재순위화 vs 맥락 압축\n",
    "\n",
    "| 특징 | 재순위화 (Re-ranking) | 맥락 압축 (Contextual Compression) |\n",
    "|------|----------------------|-----------------------------------|\n",
    "| **목적** | 검색 결과의 순서 최적화 | 관련 정보만 선별적 추출 |\n",
    "| **처리 방식** | 문서 전체 순위 재정렬 | 문서 내용 필터링/압축 |\n",
    "| **출력** | 순서가 바뀐 동일한 문서들 | 압축되거나 필터링된 문서들 |\n",
    "| **주요 장점** | 정확도 향상 | 비용 절감, 노이즈 제거 |\n",
    "| **사용 시점** | 검색 직후 | 검색 후 또는 재순위화 후 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 환경 설정\n",
    "\n",
    "### 2.1 필수 라이브러리 설치\n",
    "\n",
    "```bash\n",
    "# 기본 LangChain 및 RAG 구성 요소\n",
    "pip install langchain langchain-community langchain-openai\n",
    "pip install langchain-chroma\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 환경 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 경고 메시지 숨기기\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Langsmith tracing 여부를 확인 (true: langsmith 추적 활성화, false: langsmith 추적 비활성화)\n",
    "print(os.getenv('LANGSMITH_TRACING'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 기본 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pprint import pprint\n",
    "\n",
    "# LangChain 핵심\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# 검색 및 압축\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import (\n",
    "    CrossEncoderReranker,\n",
    "    LLMListwiseRerank,\n",
    "    LLMChainFilter,\n",
    "    LLMChainExtractor,\n",
    "    EmbeddingsFilter,\n",
    "    DocumentCompressorPipeline\n",
    ")\n",
    "\n",
    "# 외부 모델\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "\n",
    "# 벡터 저장소\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# 데이터 분석 및 시각화\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# # 한글 폰트 인식 - Windows\n",
    "# import matplotlib \n",
    "# font_name = matplotlib.font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "# matplotlib.rc('font', family=font_name)\n",
    "\n",
    "# 한글 폰트 인식 - Mac\n",
    "import matplotlib\n",
    "matplotlib.rc('font', family='AppleGothic')\n",
    "\n",
    "# 마이너스 부호 인식\n",
    "matplotlib.rc(\"axes\", unicode_minus = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 벡터 저장소 및 기본 검색기 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vector_store(embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\"), collection_name=\"hybrid_search_db\", persist_directory = \"./local_chroma_db\"):\n",
    "    \"\"\"\n",
    "    기존 벡터 저장소를 로드하거나 새로 생성\n",
    "    \n",
    "    Returns:\n",
    "        Chroma: 벡터 저장소 객체\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        # 기존 벡터 저장소 로드 시도\n",
    "        vector_store = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=embeddings,\n",
    "            persist_directory=persist_directory,\n",
    "        )\n",
    "        \n",
    "        doc_count = vector_store._collection.count()\n",
    "        if doc_count > 0:\n",
    "            print(f\"✅ 기존 벡터 저장소 로드: {doc_count}개 문서\")\n",
    "            return vector_store\n",
    "        else:\n",
    "            print(\"⚠️ 빈 벡터 저장소입니다. 데이터를 추가해주세요.\")\n",
    "            return vector_store\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 벡터 저장소 로드 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "# 벡터 저장소 초기화\n",
    "vector_store = initialize_vector_store()\n",
    "\n",
    "if vector_store:\n",
    "    # 기본 검색기 생성\n",
    "    base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "    print(\"✅ 기본 검색기 생성 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 retriever 테스트 \n",
    "\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "retrieved_docs = base_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 재순위화(Re-ranking) 기법\n",
    "\n",
    "### 3.1 재순위화의 핵심 개념\n",
    "\n",
    "재순위화는 1차 검색으로 얻은 문서들을 더 정교한 기준으로 재평가하여 순서를 재정렬하는 기법입니다.\n",
    "\n",
    "#### 🎯 **재순위화의 작동 원리**\n",
    "\n",
    "1. **1차 검색**: 벡터 유사도로 많은 후보 문서 검색 (k=50~100)\n",
    "2. **정밀 분석**: 각 문서와 쿼리 간의 세밀한 관련성 평가\n",
    "3. **순위 재정렬**: 관련성 점수에 따라 문서 순서 재배치\n",
    "4. **상위 선택**: 가장 관련성 높은 상위 k개 문서 반환\n",
    "\n",
    "### 3.2 Cross-Encoder 재순위화\n",
    "\n",
    "Cross-Encoder는 쿼리와 문서를 함께 입력받아 직접적인 관련성 점수를 계산하는 모델입니다.\n",
    "\n",
    "- **Cross-Encoder** 모델을 활용하여 검색 결과의 정밀한 재정렬을 수행함\n",
    "- 검색 쿼리와 검색된 문서 간 유사도를 더 정확하게 계산함\n",
    "\n",
    "- 참고: https://www.sbert.net/examples/applications/cross-encoder/README.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_encoder_reranker(model_name=\"BAAI/bge-reranker-v2-m3\", top_n=5):\n",
    "    \"\"\"\n",
    "    Cross-Encoder 재순위화 시스템 생성\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): 사용할 Cross-Encoder 모델\n",
    "        top_n (int): 반환할 상위 문서 수\n",
    "    \n",
    "    Returns:\n",
    "        ContextualCompressionRetriever: 재순위화 검색기\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Cross-Encoder 재순위화 시스템 초기화\")\n",
    "    print(f\"   모델: {model_name}\")\n",
    "    print(f\"   상위 {top_n}개 문서 반환\")\n",
    "    \n",
    "    try:\n",
    "        # Cross-Encoder 모델 로드\n",
    "        cross_encoder_model = HuggingFaceCrossEncoder(model_name=model_name)\n",
    "        \n",
    "        # 재순위화 컴프레서 생성\n",
    "        reranker = CrossEncoderReranker(\n",
    "            model=cross_encoder_model, \n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "        # 컨텍스트 압축 검색기 생성\n",
    "        compression_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=reranker,\n",
    "            base_retriever=base_retriever,\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Cross-Encoder 재순위화 시스템 생성 완료\")\n",
    "        return compression_retriever\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Cross-Encoder 초기화 실패: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재순위화 시스템 생성\n",
    "cross_encoder_retriever = create_cross_encoder_reranker(top_n=5)\n",
    "\n",
    "# CrossEncoderReranker를 사용한 retriever를 사용하여 검색\n",
    "query = \"테슬라 트럭 모델이 있나요?\"\n",
    "retrieved_docs = cross_encoder_retriever.invoke(query) \n",
    "\n",
    "# 검색 결과 출력\n",
    "print(\"\\n🔄 Cross-Encoder 재순위화 검색 결과:\")\n",
    "print(\"-\"*200)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_dataset(file_path):\n",
    "    \"\"\"\n",
    "    평가 데이터셋 로드\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 평가 데이터 파일 경로\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: 평가 데이터셋\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(file_path)\n",
    "        elif file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"지원하지 않는 파일 형식\")\n",
    "        \n",
    "        print(f\"✅ 평가 데이터셋 로드: {len(df)}개 질문\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 데이터셋 로드 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "# 평가 데이터셋 로드\n",
    "\n",
    "eval_df = load_evaluation_dataset(\"./data/synthetic_testset.csv\")\n",
    "eval_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_evaluation_data(df):\n",
    "    \"\"\"\n",
    "    평가 데이터 전처리\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): 원본 데이터프레임\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (질문 리스트, 정답 문서 리스트)\n",
    "    \"\"\"\n",
    "    questions = df['user_input'].tolist()\n",
    "    \n",
    "    # 정답 문서 파싱\n",
    "    reference_contexts = []\n",
    "    for contexts in df['reference_contexts']:\n",
    "        if isinstance(contexts, str):\n",
    "            # 문자열을 리스트로 변환\n",
    "            context_list = eval(contexts)\n",
    "        else:\n",
    "            context_list = contexts\n",
    "        \n",
    "        # Document 객체로 변환\n",
    "        docs = [Document(page_content=ctx) for ctx in context_list]\n",
    "        reference_contexts.append(docs)\n",
    "    \n",
    "    return questions, reference_contexts\n",
    "\n",
    "# 평가 데이터 전처리\n",
    "questions, reference_contexts = prepare_evaluation_data(eval_df)\n",
    "\n",
    "# 평가 데이터 확인\n",
    "for i, (q, refs) in enumerate(zip(questions[:3], reference_contexts[:3])):\n",
    "    print(f\"\\n[질문 {i+1}]\")\n",
    "    print(f\"질문: {q}\")\n",
    "    print(f\"정답 문서: {len(refs)}개\")\n",
    "    for j, ref in enumerate(refs):\n",
    "        print(f\"  [{j+1}] 내용: {ref.page_content[:50]}...\")  # 내용 일부만 출력\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 평가 데이터 확인\n",
    "for i, (q, refs) in enumerate(zip(questions[-3:], reference_contexts[-3:])):\n",
    "    print(f\"\\n[질문 {i+1}]\")\n",
    "    print(f\"질문: {q}\")\n",
    "    print(f\"정답 문서: {len(refs)}개\")\n",
    "    for j, ref in enumerate(refs):\n",
    "        print(f\"  [{j+1}] 내용: {ref.page_content[:50]}...\")  # 내용 일부만 출력\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranx-k 라이브러리 사용해서 검색 결과 평가\n",
    "from ranx_k.evaluation import evaluate_with_ranx_similarity\n",
    "\n",
    "# ranx-k 평가 실행 (rouge 점수가 높은 경우) -> 문자열 유사도 기반 평가\n",
    "base_results = evaluate_with_ranx_similarity(\n",
    "    retriever=base_retriever,\n",
    "    questions=questions, \n",
    "    reference_contexts=reference_contexts,\n",
    "    k=5,\n",
    "    method='kiwi_rouge',  \n",
    "    similarity_threshold=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_results = evaluate_with_ranx_similarity(\n",
    "    retriever=cross_encoder_retriever,\n",
    "    questions=questions, \n",
    "    reference_contexts=reference_contexts,\n",
    "    k=5,\n",
    "    method='kiwi_rouge',  \n",
    "    similarity_threshold=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 LLM 기반 재순위화\n",
    "\n",
    "- **대규모 언어 모델**을 활용하여 검색 결과의 재순위화를 수행함\n",
    "- 쿼리와 문서 간의 **관련성 분석**을 통해 최적의 순서를 도출함\n",
    "- **LLMListwiseRerank**와 같은 전문화된 재순위화 모델을 적용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_reranker(model_name=\"gpt-4.1-mini\", top_n=5):\n",
    "    \"\"\"\n",
    "    LLM 기반 재순위화 시스템 생성\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): 사용할 LLM 모델\n",
    "        top_n (int): 반환할 상위 문서 수\n",
    "    \n",
    "    Returns:\n",
    "        ContextualCompressionRetriever: LLM 재순위화 검색기\n",
    "    \"\"\"\n",
    "    print(f\"🤖 LLM 재순위화 시스템 초기화\")\n",
    "    print(f\"   모델: {model_name}\")\n",
    "    print(f\"   상위 {top_n}개 문서 반환\")\n",
    "    \n",
    "    try:\n",
    "        # LLM 초기화\n",
    "        llm = ChatOpenAI(model=model_name, temperature=0)\n",
    "        \n",
    "        # LLM 기반 재순위화 컴프레서 생성\n",
    "        llm_reranker = LLMListwiseRerank.from_llm(llm, top_n=top_n)\n",
    "        \n",
    "        # 컨텍스트 압축 검색기 생성\n",
    "        llm_compression_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=llm_reranker,\n",
    "            base_retriever=base_retriever,\n",
    "        )\n",
    "        \n",
    "        print(\"✅ LLM 재순위화 시스템 생성 완료\")\n",
    "        return llm_compression_retriever\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ LLM 재순위화 초기화 실패: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 재순위화 시스템 생성\n",
    "llm_retriever = create_llm_reranker(top_n=3)\n",
    "\n",
    "# LLM 재순위화 검색기 사용하여 검색\n",
    "query = \"테슬라 트럭 모델이 있나요?\"\n",
    "retrieved_docs = llm_retriever.invoke(query)\n",
    "\n",
    "# 검색 결과 출력\n",
    "print(\"\\n🤖 LLM 재순위화 검색 결과:\")\n",
    "print(\"-\"*200)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 맥락 압축(Contextual Compression) 기법\n",
    "\n",
    "### 4.1 맥락 압축의 핵심 개념\n",
    "\n",
    "맥락 압축은 검색된 문서에서 쿼리와 관련된 핵심 정보만을 선별적으로 추출하는 기법입니다.\n",
    "\n",
    "#### 🎯 **맥락 압축의 이점**\n",
    "\n",
    "1. **비용 절감**: 불필요한 토큰 제거로 LLM API 비용 절약\n",
    "2. **성능 향상**: 핵심 정보만 제공하여 답변 품질 향상\n",
    "3. **노이즈 제거**: 관련 없는 정보 필터링\n",
    "4. **컨텍스트 효율성**: 제한된 컨텍스트 윈도우 최적 활용\n",
    "\n",
    "### 4.2 LLM 기반 필터링\n",
    "\n",
    "#### 4.2.1 LLMChainFilter 구현\n",
    "\n",
    "\n",
    "- **LLM 기반 필터링**으로 검색된 문서의 포함 여부를 결정함\n",
    "- **원본 유지 방식**으로 문서 내용의 변경 없이 선별 작업을 수행함\n",
    "- **선택적 필터링**을 통해 관련성 높은 문서만을 최종 반환함\n",
    "- 문서 원본을 보존하면서 관련성 기반의 스마트한 선별을 수행하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_filter(model_name=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    LLM 기반 문서 필터링 시스템 생성\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): 사용할 LLM 모델\n",
    "    \n",
    "    Returns:\n",
    "        ContextualCompressionRetriever: LLM 필터링 검색기\n",
    "    \"\"\"\n",
    "    print(f\"🔍 LLM 문서 필터링 시스템 초기화\")\n",
    "    print(f\"   모델: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        # LLM 초기화\n",
    "        llm = ChatOpenAI(model=model_name, temperature=0)\n",
    "        \n",
    "        # LLM 체인 필터 생성\n",
    "        llm_filter = LLMChainFilter.from_llm(llm)\n",
    "        \n",
    "        # 컨텍스트 압축 검색기 생성\n",
    "        filter_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=llm_filter,\n",
    "            base_retriever=base_retriever,\n",
    "        )\n",
    "        \n",
    "        print(\"✅ LLM 필터링 시스템 생성 완료\")\n",
    "        return filter_retriever\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ LLM 필터링 초기화 실패: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 필터링 시스템 생성\n",
    "filter_retriever = create_llm_filter()\n",
    "\n",
    "# LLM 필터링 검색기 사용하여 검색\n",
    "query = \"테슬라 트럭 모델이 있나요?\"\n",
    "retrieved_docs = filter_retriever.invoke(query)\n",
    "\n",
    "# 검색 결과 출력\n",
    "print(\"\\n🔍 LLM 필터링 검색 결과:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 LLMChainExtractor 구현\n",
    "\n",
    "- **LLM 기반 추출**로 문서에서 쿼리 관련 핵심 내용만을 선별함\n",
    "- **순차적 처리 방식**으로 각 문서를 검토하여 관련 정보를 추출함\n",
    "- **맞춤형 요약**을 통해 쿼리에 최적화된 압축 결과를 생성함\n",
    "- 쿼리 맥락에 따른 선별적 정보 추출로 효율적인 문서 압축을 실현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_extractor(model_name=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    LLM 기반 정보 추출 시스템 생성\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): 사용할 LLM 모델\n",
    "    \n",
    "    Returns:\n",
    "        ContextualCompressionRetriever: LLM 추출 검색기\n",
    "    \"\"\"\n",
    "    print(f\"📝 LLM 정보 추출 시스템 초기화\")\n",
    "    print(f\"   모델: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        # LLM 초기화\n",
    "        llm = ChatOpenAI(model=model_name, temperature=0)\n",
    "        \n",
    "        # LLM 체인 추출기 생성\n",
    "        llm_extractor = LLMChainExtractor.from_llm(llm)\n",
    "        \n",
    "        # 컨텍스트 압축 검색기 생성\n",
    "        extractor_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=llm_extractor,\n",
    "            base_retriever=base_retriever,\n",
    "        )\n",
    "        \n",
    "        print(\"✅ LLM 추출 시스템 생성 완료\")\n",
    "        return extractor_retriever\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ LLM 추출 초기화 실패: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 추출 시스템 생성\n",
    "extractor_retriever = create_llm_extractor()\n",
    "\n",
    "# LLM 추출 검색기 사용하여 검색\n",
    "query = \"테슬라 트럭 모델이 있나요?\"\n",
    "retrieved_docs = extractor_retriever.invoke(query)\n",
    "\n",
    "# 검색 결과 출력\n",
    "print(\"\\n📝 LLM 추출 검색 결과:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 임베딩 기반 필터링 (EmbeddingsFilter)\n",
    "\n",
    "- **임베딩 기반 필터링**으로 문서와 쿼리 간 유사도를 계산함\n",
    "- **LLM 미사용 방식**으로 빠른 처리 속도와 비용 효율성을 확보함 (LLM 호출보다 저렴하고 빠른 옵션)\n",
    "- **유사도 기준 선별**을 통해 관련성 높은 문서만을 효과적으로 추출함\n",
    "- 경제적이고 신속한 임베딩 기반의 문서 필터링 기법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_filter(similarity_threshold=0.1):\n",
    "    \"\"\"\n",
    "    임베딩 기반 유사도 필터링 시스템 생성\n",
    "    \n",
    "    Args:\n",
    "        similarity_threshold (float): 유사도 임계값 (0~1)\n",
    "    \n",
    "    Returns:\n",
    "        ContextualCompressionRetriever: 임베딩 필터링 검색기\n",
    "    \"\"\"\n",
    "    print(f\"🧮 임베딩 기반 필터링 시스템 초기화\")\n",
    "    print(f\"   유사도 임계값: {similarity_threshold}\")\n",
    "    \n",
    "    try:\n",
    "        # 임베딩 모델 초기화\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        \n",
    "        # 임베딩 필터 생성\n",
    "        embeddings_filter = EmbeddingsFilter(\n",
    "            embeddings=embeddings, \n",
    "            similarity_threshold=similarity_threshold\n",
    "        )\n",
    "        \n",
    "        # 컨텍스트 압축 검색기 생성\n",
    "        embedding_filter_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=embeddings_filter,\n",
    "            base_retriever=base_retriever,\n",
    "        )\n",
    "        \n",
    "        print(\"✅ 임베딩 필터링 시스템 생성 완료\")\n",
    "        return embedding_filter_retriever\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 임베딩 필터링 초기화 실패: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_embeddings_filter_thresholds():\n",
    "    \"\"\"다양한 임계값에서 임베딩 필터 성능 테스트\"\"\"\n",
    "    if not base_retriever:\n",
    "        print(\"❌ 기본 검색기가 초기화되지 않았습니다.\")\n",
    "        return\n",
    "    \n",
    "    print(\"🧮 임베딩 필터 임계값별 성능 테스트\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 다양한 임계값 설정\n",
    "    thresholds = [0.2, 0.4, 0.6, 0.8]\n",
    "    test_query = \"테슬라 트럭 모델이 있나요?\"\n",
    "    \n",
    "    print(f\"테스트 질문: {test_query}\\n\")\n",
    "    \n",
    "    threshold_results = {}\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        print(f\"🔍 임계값 {threshold} 테스트:\")\n",
    "        \n",
    "        try:\n",
    "            # 임베딩 필터 생성\n",
    "            filter_retriever = create_embeddings_filter(\n",
    "                similarity_threshold=threshold\n",
    "            )\n",
    "            \n",
    "            if filter_retriever:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # 필터링 실행\n",
    "                filtered_docs = filter_retriever.invoke(test_query)\n",
    "                \n",
    "                end_time = time.time()\n",
    "                processing_time = end_time - start_time\n",
    "                \n",
    "                threshold_results[threshold] = {\n",
    "                    \"filtered_count\": len(filtered_docs),\n",
    "                    \"processing_time\": processing_time\n",
    "                }\n",
    "                \n",
    "                print(f\"   ✅ 필터링된 문서 수: {len(filtered_docs)}개\")\n",
    "                print(f\"   ⏱️ 처리 시간: {processing_time:.3f}초\")\n",
    "                \n",
    "                # 상위 결과 미리보기\n",
    "                if filtered_docs:\n",
    "                    print(f\"   📄 상위 결과: {filtered_docs[0].page_content[:80]}...\")\n",
    "                else:\n",
    "                    print(\"   ❌ 임계값 조건을 만족하는 문서 없음\")\n",
    "            else:\n",
    "                print(\"   ❌ 필터 생성 실패\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 오류 발생: {e}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # 결과 시각화\n",
    "    if threshold_results:\n",
    "        print(\"\\n📊 임계값별 결과 요약:\")\n",
    "        \n",
    "        # DataFrame 생성\n",
    "        df = pd.DataFrame(threshold_results).T\n",
    "        df.index.name = 'Threshold'\n",
    "        df = df.round(3)\n",
    "        print(df.to_string())\n",
    "        \n",
    "        # 시각화\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # 필터링된 문서 수\n",
    "        ax1.bar(df.index, df['filtered_count'], color='skyblue')\n",
    "        ax1.set_title('임계값별 필터링된 문서 수')\n",
    "        ax1.set_xlabel('유사도 임계값')\n",
    "        ax1.set_ylabel('문서 수')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 처리 시간\n",
    "        ax2.bar(df.index, df['processing_time'], color='lightcoral')\n",
    "        ax2.set_title('임계값별 처리 시간')\n",
    "        ax2.set_xlabel('유사도 임계값')\n",
    "        ax2.set_ylabel('처리 시간 (초)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return threshold_results\n",
    "\n",
    "# 임베딩 필터 임계값 테스트\n",
    "threshold_test_results = test_embeddings_filter_thresholds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 파이프라인 최적화 (DocumentCompressorPipeline)\n",
    "\n",
    "여러 압축 기법을 순차적으로 연결하여 최적의 결과를 얻는 방법입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "\n",
    "\n",
    "def create_comprehensive_pipeline():\n",
    "    \"\"\"\n",
    "    포괄적인 문서 압축 파이프라인 생성\n",
    "    \n",
    "    Returns:\n",
    "        ContextualCompressionRetriever: 다단계 압축 검색기\n",
    "    \"\"\"\n",
    "    print(\"🔧 포괄적인 문서 압축 파이프라인 구성\")\n",
    "    \n",
    "    try:\n",
    "        # 임베딩 모델 초기화\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        \n",
    "        # LLM 초기화\n",
    "        llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "        \n",
    "        # 파이프라인 구성 요소들\n",
    "        print(\"   1️⃣ 중복 제거 필터 추가\")\n",
    "        redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "        \n",
    "        print(\"   2️⃣ 유사도 기반 필터 추가 (임계값: 0.4)\")\n",
    "        similarity_filter = EmbeddingsFilter(\n",
    "            embeddings=embeddings, \n",
    "            similarity_threshold=0.4\n",
    "        )\n",
    "        \n",
    "        print(\"   3️⃣ LLM 기반 재순위화 추가\")\n",
    "        llm_reranker = LLMListwiseRerank.from_llm(llm, top_n=5)\n",
    "        \n",
    "        # 파이프라인 생성\n",
    "        pipeline_compressor = DocumentCompressorPipeline(\n",
    "            transformers=[\n",
    "                redundant_filter,    # 1단계: 중복 제거\n",
    "                similarity_filter,   # 2단계: 유사도 필터링\n",
    "                llm_reranker        # 3단계: LLM 재순위화\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # 최종 압축 검색기 생성\n",
    "        pipeline_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=pipeline_compressor,\n",
    "            base_retriever=base_retriever,\n",
    "        )\n",
    "        \n",
    "        print(\"✅ 포괄적인 압축 파이프라인 생성 완료\")\n",
    "        return pipeline_retriever\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 파이프라인 생성 실패: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 생성\n",
    "pipeline_retriever = create_comprehensive_pipeline()\n",
    "\n",
    "# 파이프라인 검색기 사용하여 검색\n",
    "query = \"테슬라 트럭 모델이 있나요?\"\n",
    "retrieved_docs = pipeline_retriever.invoke(query)\n",
    "\n",
    "# 검색 결과 출력\n",
    "print(\"\\n🔄 포괄적인 문서 압축 파이프라인 검색 결과:\")\n",
    "print(f\"   필터링된 문서 수: {len(retrieved_docs)}개\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kt-ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
