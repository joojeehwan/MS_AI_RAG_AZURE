{"cells":[{"cell_type":"markdown","metadata":{"id":"Ntj4ZcMrriJ1"},"source":["#  RAG 답변 평가 (정량적 지표 활용)\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"yIh4veMlriJ2"},"source":["## 환경 설정 및 준비"]},{"cell_type":"markdown","metadata":{"id":"J8q0H1F5riJ3"},"source":["`(1) Env 환경변수`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdRE9MzyriJ3"},"outputs":[],"source":["from dotenv import load_dotenv\n","load_dotenv()"]},{"cell_type":"markdown","metadata":{"id":"1zxZwKDlriJ3"},"source":["`(2) 기본 라이브러리`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tf45uHMgriJ3"},"outputs":[],"source":["import os\n","import json\n","import time\n","from typing import List, Dict, Any, Optional\n","from pprint import pprint\n","\n","# LangChain 핵심\n","from langchain_core.documents import Document\n","from langchain_core.retrievers import BaseRetriever\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","\n","# 외부 모델\n","from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n","from langchain_community.document_transformers import EmbeddingsRedundantFilter\n","\n","# 벡터 저장소\n","from langchain_chroma import Chroma\n","\n","# 데이터 분석 및 시각화\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# # 한글 폰트 인식 - Windows\n","# import matplotlib\n","# font_name = matplotlib.font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n","# matplotlib.rc('font', family=font_name)\n","\n","# 한글 폰트 인식 - Mac\n","import matplotlib\n","matplotlib.rc('font', family='AppleGothic')\n","\n","# 마이너스 부호 인식\n","matplotlib.rc(\"axes\", unicode_minus = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KkFlOgo-riJ4"},"outputs":[],"source":["from langfuse.langchain import CallbackHandler\n","\n","# LangChain 콜백 핸들러 생성\n","langfuse_handler = CallbackHandler()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KIcC0mT2riJ4"},"outputs":[],"source":["# Langsmith tracing 여부를 확인 (true: langsmith 추적 활성화, false: langsmith 추적 비활성화)\n","print(\"langsmith 추적 여부: \", os.getenv('LANGSMITH_TRACING'))"]},{"cell_type":"markdown","metadata":{"id":"q9c82lSoriJ4"},"source":["`(3) Test Data`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JqSKAThSriJ4"},"outputs":[],"source":["# Test 데이터셋에 대한 QA 생성 결과를 리뷰한 후 다시 로드\n","import pandas as pd\n","df_qa_test = pd.read_csv(\"data/synthetic_testset.csv\")\n","\n","print(f\"테스트셋: {df_qa_test.shape[0]}개 문서\")\n","df_qa_test.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c5rTK4zpriJ5"},"outputs":[],"source":["# Evaluaiton 데이터셋에 RAGAS 평가 결과를 로드\n","\n","ragas_evaluation = pd.read_csv('data/ragas_evaluation_results.csv')\n","ragas_evaluation.head(2)"]},{"cell_type":"markdown","metadata":{"id":"m72AWWbbriJ5"},"source":["---\n","\n","## **평가 지표** (Evaluation Metric)\n","\n","#### 1) **검색(Retrieval) 평가**  \n","\n","- **Non-Rank Based Metrics**: Accuracy, Precision, Recall@k 등을 통해 관련성의 이진적 평가를 수행  \n","\n","- **Rank-Based Metrics**: MRR(Mean Reciprocal Rank), MAP(Mean Average Precision)를 통해 검색 결과의 순위를 고려한 평가를 수행\n","\n","- **RAG 특화 지표**: 기존 검색 평가 방식의 한계를 보완하는 LLM-as-judge 방식 도입\n","\n","- **포괄적 평가**: 정확도, 관련성, 다양성, 강건성을 통합적으로 측정\n","\n","![image.png](attachment:image.png)\n","\n","#### 2) **생성(Generation) 평가**\n","\n","- **전통적 평가**: ROUGE(요약), BLEU(번역), BertScore(의미 유사도) 지표 활용\n","\n","- **LLM 기반 평가**: 응집성, 관련성, 유창성을 종합적으로 판단하는 새로운 접근법 도입 (전통적인 참조 비교가 어려운 상황에서 유용)\n","\n","- **다차원 평가**: 품질, 일관성, 사실성, 가독성, 사용자 만족도를 포괄적 측정\n","\n","- **상세 프롬프트**와 **사용자 선호도** 기준으로 생성 텍스트 품질 평가\n","\n","\n","[출처] https://arxiv.org/abs/2405.07437"]},{"cell_type":"markdown","metadata":{"id":"VlbXLLc6riJ5"},"source":["---\n","\n","## **검색 도구 정의**"]},{"cell_type":"markdown","metadata":{"id":"iwObjqsfriJ5"},"source":["### 1) **벡터스토어** 로드\n","\n","- **Chroma DB** 설정에서 모델, 컬렉션명, 저장 경로 지정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-WEUPiosriJ5"},"outputs":[],"source":["def initialize_vector_store(embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\"), collection_name=\"hybrid_search_db\", persist_directory = \"./local_chroma_db\"):\n","    \"\"\"\n","    기존 벡터 저장소를 로드하거나 새로 생성\n","\n","    Returns:\n","        Chroma: 벡터 저장소 객체\n","    \"\"\"\n","    try:\n","\n","        # 기존 벡터 저장소 로드 시도\n","        vector_store = Chroma(\n","            collection_name=collection_name,\n","            embedding_function=embeddings,\n","            persist_directory=persist_directory,\n","        )\n","\n","        doc_count = vector_store._collection.count()\n","        if doc_count > 0:\n","            print(f\"✅ 기존 벡터 저장소 로드: {doc_count}개 문서\")\n","            return vector_store\n","        else:\n","            print(\"⚠️ 빈 벡터 저장소입니다. 데이터를 추가해주세요.\")\n","            return vector_store\n","\n","    except Exception as e:\n","        print(f\"❌ 벡터 저장소 로드 실패: {e}\")\n","        return None\n","\n","# 벡터 저장소 초기화\n","chroma_db = initialize_vector_store()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABG5YpDcriJ5"},"outputs":[],"source":["# 벡터저장소 검색기 생성\n","chroma_k = chroma_db.as_retriever(\n","    search_kwargs={'k': 4},\n",")\n","\n","# 벡터저장소 검색기를 사용하여 검색\n","query = \"테슬라의 회장은 누구인가요?\"\n","\n","retrieved_docs = chroma_k.invoke(query)\n","\n","# 검색 결과 출력\n","for doc in retrieved_docs:\n","    print(f\"- {doc.page_content} [출처: {doc.metadata['source']}]\")\n","    print(\"-\"*200)\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"MDQjBEc-riJ5"},"source":["## **RAG Chain** 정의\n","\n","- OpenAI gpt-4.1-mini 모델 활용"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x6c5ww19riJ6"},"outputs":[],"source":["# 각 쿼리에 대한 검색 결과를 한꺼번에 Context로 전달해서 답변을 생성\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","def create_rag_chain(retriever, llm):\n","\n","    template = \"\"\"Answer the following question based on this context. If the context is not relevant to the question, just answer with '답변에 필요한 근거를 찾지 못했습니다.'\n","\n","    [Context]\n","    {context}\n","\n","    [Question]\n","    {question}\n","\n","    [Answer]\n","    \"\"\"\n","\n","    prompt = ChatPromptTemplate.from_template(template)\n","\n","    def format_docs(docs):\n","        return \"\\n\\n\".join([f\"{doc.page_content}\" for doc in docs])\n","\n","    rag_chain = (\n","        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","        | prompt\n","        | llm\n","        | StrOutputParser()\n","    )\n","\n","    return rag_chain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bv8CyD33riJ6"},"outputs":[],"source":["# RAG 체인 생성 및 테스트\n","from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.5)\n","\n","openai_rag_chain = create_rag_chain(chroma_k, llm)\n","\n","question = \"테슬라의 회장은 누구인가요?\"\n","answer = openai_rag_chain.invoke(question)\n","\n","print(f\"쿼리: {question}\")\n","print(f\"답변: {answer}\")"]},{"cell_type":"markdown","metadata":{"id":"PMY7Lp99riJ6"},"source":["---\n","\n","## **RAG 답변 평가**\n","\n","- **정확성**과 **관련성**이 RAG 시스템의 핵심 평가 지표\n","\n","- **자동 평가**(ROUGE, BLEU)와 **인간 평가** 병행 필요\n","\n","- **일관성** 측정을 통한 답변의 논리적 모순 확인\n","\n","- 정량적, 정성적 평가를 종합해 RAG 시스템의 실질적 성능 측정\n","\n","    - **평가 시스템의 구조**:\n","        - 입력: 데이터셋 예시와 실행 결과를 포함\n","        - 출력: key(메트릭명), score/value(점수), comment(설명)를 포함하는 표준화된 형태\n","        - 구현: Python/TypeScript 기반의 커스텀 코드 또는 LangSmith 내장 평가기 활용\n","\n","    - **평가 방법의 유형**:\n","        - 휴리스틱 평가: 답변 길이, JSON 유효성, 코드 문법 등 기본적 규칙 검증\n","        - 자동화 메트릭: ROUGE, BLEU 등을 통한 정량적 평가\n","        - LLM-as-judge: LLM을 활용한 출력 품질 평가\n","        - Pairwise 비교: 두 시스템 출력의 직접적인 비교 평가\n","\n","\n","- **LangChain 평가도구** 활용\n","    - **다양한 평가 방식** 지원: Q&A 정확도, 맥락 이해도, 사고 과정 검증\n","    - **문자열/임베딩 거리** 및 **JSON 평가** 기능 포함\n","    - `LangChainStringEvaluator`로 **평가도구 설정** 및 **매개변수 조정** 가능\n","    - 참고: https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators"]},{"cell_type":"markdown","metadata":{"id":"JFadCsCxriJ6"},"source":["---\n","### 1) **휴리스틱 평가**\n","\n","- **휴리스틱 평가자**는 특정 규칙에 기반한 **결정론적 함수**로 작동하며, 명확한 기준에 따라 판단을 수행\n","\n","- 주로 **단순 검증**에 활용되며, 챗봇 응답의 공백 여부, 생성된 코드의 컴파일 가능성 등을 확인\n","\n","- 평가 기준이 **명확하고 객관적**이어서 정확한 분류나 검증이 필요한 경우에 효과적\n","\n","- 복잡한 상황보다는 **명확한 규칙**이 존재하는 간단한 검증 작업에 적합"]},{"cell_type":"markdown","metadata":{"id":"qspGbTatriJ6"},"source":["`(1) 답변 길이 평가`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbYcQzP0riJ6"},"outputs":[],"source":["# 길이 평가도구 정의\n","def evaluate_string_length(text, min_length=50, max_length=200):\n","    length = len(text)\n","    return {\n","        \"score\": min_length <= length <= max_length,\n","        \"length\": length\n","    }\n","\n","# 길이 평가 수행\n","print(f\"답변: {answer}\")\n","\n","# 길이 평가\n","result = evaluate_string_length(answer)\n","print(f\"길이 평가 결과: {result}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QGBJWDxJriJ6"},"outputs":[],"source":["# 토큰 길이 평가도구 정의\n","\n","def evaluate_token_length(text, tokenizer, min_tokens=10, max_tokens=100):\n","    tokens = tokenizer.tokenize(text)\n","    num_tokens = len(tokens)\n","    return {\n","        \"score\": min_tokens <= num_tokens <= max_tokens,\n","        \"num_tokens\": num_tokens\n","    }\n","\n","# 토큰 길이 평가 수행\n","from ranx_k.tokenizers import KiwiTokenizer\n","\n","kiwi_tokenizer = KiwiTokenizer(use_stopwords=False, pos_filter=[]) # POS 필터 미적용\n","result = evaluate_token_length(answer, kiwi_tokenizer)\n","print(f\"토큰 길이 평가 결과: {result}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KNFR-lyNriJ6"},"outputs":[],"source":["kiwi_tokenizer.tokenize(answer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3g_lYDqMriJ6"},"outputs":[],"source":["from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","\n","# 여러 평가 도구를 병렬로 실행하는 함수\n","def create_evaluator_chain(*evaluator_funcs):\n","    evaluator_dict = {\n","        getattr(func, '__name__', f'eval_{i}'): lambda x, func=func: func(x)\n","        for i, func in enumerate(evaluator_funcs)\n","    }\n","\n","    return RunnableParallel(evaluator_dict)\n","\n","# 길이 평가 도구 생성 함수\n","def create_length_evaluator(min_length=50, max_length=200):\n","    def evaluate_string_length(text: str):\n","        length = len(text)\n","        return {\n","            \"score\": min_length <= length <= max_length,\n","            \"length\": length\n","        }\n","    return evaluate_string_length\n","\n","# 토큰 길이 평가 도구 생성 함수\n","def create_token_length_evaluator(tokenizer, min_tokens=10, max_tokens=100):\n","    def evaluate_token_length(text: str):\n","        tokens = tokenizer.tokenize(text)\n","        num_tokens = len(tokens)\n","        return {\n","            \"score\": min_tokens <= num_tokens <= max_tokens,\n","            \"num_tokens\": num_tokens\n","        }\n","    return evaluate_token_length\n","\n","# 요약 체인 생성 함수\n","def create_summary_chain(llm, evaluators=None):\n","    prompt = ChatPromptTemplate.from_template(\n","        \"\"\"제시된 텍스트를 50자 이내로 요약하세요. (공백 포함)\n","\n","        [텍스트]\n","        {text}\n","        \"\"\"\n","    )\n","\n","    base_chain = prompt | llm | StrOutputParser()\n","\n","    if evaluators:\n","        eval_chain = create_evaluator_chain(*evaluators)\n","        chain = base_chain | RunnableParallel({\n","            \"answer\": RunnablePassthrough(),\n","            \"evaluation\": eval_chain\n","        })\n","        return chain\n","\n","    return base_chain\n","\n","\n","# 요약 체인 생성\n","llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.3, max_tokens=100)\n","evaluators = [\n","    create_length_evaluator(min_length=30, max_length=50),\n","    create_token_length_evaluator(kiwi_tokenizer, min_tokens=10, max_tokens=20),\n","]\n","\n","summary_chain = create_summary_chain(llm, evaluators)\n","\n","# 요약 체인 테스트\n","text = \"\"\"\n","테슬라는 미국의 전기 자동차 제조업체이자, 태양광 발전 및 에너지 저장장치 제조업체이다.\n","테슬라의 창업자이자 CEO인 일론 머스크는 2003년 테슬라를 설립하였다.\n","테슬라는 2008년 테슬라 로드스터를 출시하며 전기 자동차 시장에 진출하였다.\n","\"\"\"\n","\n","summary = summary_chain.invoke(text)\n","print(f\"요약: {summary['answer']}\")\n","print(f\"평가: {summary['evaluation']}\")"]},{"cell_type":"markdown","metadata":{"id":"0PXOrGgwriJ6"},"source":["`(2) JSON 유효성 평가`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jRLXlNYMriJ7"},"outputs":[],"source":["from langchain.evaluation import load_evaluator\n","\n","# Evaluator 초기화\n","json_validity_evaluator = load_evaluator(\n","    evaluator=\"json_validity\",      # JSON 유효성 검사\n","    )\n","\n","# JSON 평가\n","json_result = json_validity_evaluator.evaluate_strings(\n","    prediction='{\"name\": \"test\", \"value\": 123}',\n","    reference=None\n",")\n","print(f\"JSON 유효성 평가 결과: {json_result}\")"]},{"cell_type":"markdown","metadata":{"id":"Crw50BTLriJ7"},"source":["---\n","### 2) **정량 평가지표** 사용\n","\n","- **ROUGE**와 **BLEU**는 텍스트 생성 품질을 평가하는 대표적인 **정량 평가지표**임\n","\n","- 이러한 메트릭들은 생성된 텍스트와 참조 텍스트 간의 **단어 중첩도**를 계산하여 품질을 수치화\n","\n","- **대규모 자동화 평가**가 필요한 경우 효율적이며, 객관적인 비교가 가능한 장점이 있음\n","\n","- 하지만, 문맥이나 의미의 유사성은 완벽하게 포착하지 못하는 **한계점**이 존재\n","\n","    1. 단순 단어 비교만 가능\n","        - \"날씨가 좋다\"와 \"날씨가 훌륭하다\"는 의미는 비슷하지만 다른 점수를 받을 수 있음\n","\n","    2. 문맥 이해 불가\n","        - \"강아지가 공을 물었다\"와 \"공을 강아지가 물었다\"는 의미가 같아도 다른 점수를 받을 수 있음"]},{"cell_type":"markdown","metadata":{"id":"0FnbzVM5riJ7"},"source":["---\n","\n","- **ROUGE(Recall-Oriented Understudy for Gisting Evaluation)**\n","\n","    - 생성된 요약문의 품질을 평가\n","\n","- ROUGE-1:\n","    - 참조 문서와 생성된 요약문 사이의 단일 단어(unigram) 중복을 측정\n","    - 예시: \"고양이가 잠을 잡니다\"와 \"고양이는 낮잠을 잡니다\" 비교\n","        - 참조: [\"고양이\", \"가\", \"잠\", \"을\", \"잡니다\"]\n","        - 생성: [\"고양이\", \"는\", \"낮잠\", \"을\", \"잡니다\"]\n","        - 일치: [\"고양이\", \"을\", \"잡니다\"]\n","        - Precision = 3/5 = 0.6\n","        - Recall = 3/5 = 0.6\n","        - F1 = 0.6\n","\n","- ROUGE-2:\n","    - 연속된 두 단어(bigram) 시퀀스의 중복을 평가\n","    - 예시: \"고양이가 잠을 잡니다\"와 \"고양이는 낮잠을 잡니다\" 비교\n","        - 참조: [\"고양이 가\", \"가 잠\", \"잠 을\", \"을 잡니다\"]\n","        - 생성: [\"고양이 는\", \"는 낮잠\", \"낮잠 을\", \"을 잡니다\"]\n","        - 일치: [\"을 잡니다\"]\n","        - Precision = 1/4 = 0.25\n","        - Recall = 1/4 = 0.25\n","        - F1 = 0.25\n","\n","- ROUGE-L:\n","    - 최장 공통 부분수열(LCS)을 기반으로 텍스트의 유사도를 측정\n","    - 예시: \"고양이가 잠을 잡니다\"와 \"고양이는 낮잠을 잡니다\" 비교\n","        - 참조: \"고양이 가 잠 을 잡니다\"\n","        - 생성: \"고양이 는 낮잠 을 잡니다\"\n","        - LCS: \"고양이 을 잡니다\"\n","        - Precision = 3/5 = 0.6\n","        - Recall = 3/5 = 0.6\n","        - F1 = 0.6\n","\n","- 각 메트릭은 정밀도(Precision), 재현율(Recall), F1-score로 표현\n","- ROUGE-1은 개별 단어 일치를, ROUGE-2는 문장 구조를, ROUGE-L은 전체적인 의미 유사성을 중점적으로 평가"]},{"cell_type":"markdown","metadata":{"id":"kS6xJS1XriJ7"},"source":["---\n","\n","- **BLEU (Bilingual Evaluation Understudy)**\n","\n","    - BLEU는 기계 번역의 품질을 평가하는 대표적인 메트릭으로, 생성된 번역문과 참조 번역문 간의 n-gram 정확도를 계산\n","    - 0에서 1 사이의 값을 가지며, 1에 가까울수록 번역 품질이 좋음을 의미\n","\n","- BLEU-1 (단일 단어):\n","    - 예시: \"나는 학교에 간다\" vs \"나는 학원에 간다\"\n","        - 참조: [\"나는\", \"학교에\", \"간다\"]\n","        - 생성: [\"나는\", \"학원에\", \"간다\"]\n","        - 일치: [\"나는\", \"간다\"]\n","        - Precision = 2/3 = 0.67\n","        - BP(Brevity Penalty) = 1 (같은 길이)\n","        - BLEU-1 = 0.67\n","\n","- BLEU-2 (두 단어 시퀀스):\n","    - 예시: \"나는 학교에 간다\" vs \"나는 학원에 간다\"\n","        - 참조의 bigram: [\"나는 학교에\", \"학교에 간다\"]\n","        - 생성의 bigram: [\"나는 학원에\", \"학원에 간다\"]\n","        - 일치하는 bigram: 없음\n","        - Precision = 0/2 = 0\n","        - BP = 1\n","        - BLEU-2 = 0\n","\n","- BLEU-3 (세 단어 시퀀스):\n","    - 예시: \"나는 학교에 간다\" vs \"나는 학원에 간다\"\n","        - 참조의 trigram: [\"나는 학교에 간다\"]\n","        - 생성의 trigram: [\"나는 학원에 간다\"]\n","        - 일치하는 trigram: 없음\n","        - Precision = 0/1 = 0\n","        - BP = 1\n","        - BLEU-3 = 0\n","\n","- 주요 특징:\n","    1. Brevity Penalty(BP):\n","        - 생성문이 참조문보다 짧을 경우 페널티 부여\n","        - BP = min(1, exp(1 - 참조문길이/생성문길이))\n","\n","    2. 최종 BLEU 점수:\n","        - 일반적으로 BLEU-1부터 BLEU-4까지의 기하평균 사용\n","        - BLEU = BP × exp(∑(wn × log(pn)))\n","        - wn은 각 n-gram의 가중치 (보통 균등 가중치 0.25 사용)\n","\n","    3. ROUGE와의 차이점:\n","        - BLEU는 Precision 중심, ROUGE는 Recall 중심\n","        - BLEU는 기계 번역 평가에 주로 사용\n","        - BLEU는 항상 BP(Brevity Penalty)를 고려함\n","        - BLEU는 여러 n-gram의 기하평균을 사용"]},{"cell_type":"markdown","metadata":{"id":"Dv2tH83hriJ7"},"source":["`(1) ROUGE 스코어`\n","\n","   - **ROUGE-1**: 한 단어씩 비교 (예: \"날씨가\", \"좋습니다\" 등)\n","   - **ROUGE-2**: 두 단어(bigram)씩 연속으로 비교 (예: \"날씨가 좋습니다\")\n","   - **ROUGE-L**: 가장 긴 공통 단어열(LCS)을 찾아 비교\n","\n","  - 자동 요약과 기계 번역 평가에 사용"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"diV5s-1hriJ7"},"outputs":[],"source":["from rouge_score import rouge_scorer\n","from ranx_k.tokenizers import KiwiTokenizer\n","\n","\n","# 토크나이저 생성\n","kiwi_tokenizer = KiwiTokenizer(use_stopwords=False, pos_filter=[]) # POS 필터 미적용\n","\n","# ROUGE 스코어 계산\n","scorer = rouge_scorer.RougeScorer(\n","    [\"rouge1\", \"rouge2\", \"rougeL\"],\n","    tokenizer=kiwi_tokenizer      # tokenize 메소드를 갖는 토크나이저 사용\n",")\n","\n","# 예시 데이터\n","reference = \"오늘 날씨가 매우 좋습니다. 공원에서 산책하기 좋은 날이에요.\"   # 정답 텍스트\n","generated1 = \"오늘은 날씨가 정말 좋네요. 공원에서 산책하면 좋을 것 같아요.\" # 생성된 텍스트\n","generated2 = \"비가 많이 오고 있어요. 실내에서 쉬는 게 좋을 것 같습니다.\"   # 생성된 텍스트\n","\n","print(\"=== 유사한 텍스트 비교 ===\")\n","rouge_scores = scorer.score(reference, generated1)\n","\n","print(f\"ROUGE-1: {rouge_scores['rouge1'].fmeasure:.3f}\")\n","print(f\"ROUGE-2: {rouge_scores['rouge2'].fmeasure:.3f}\")\n","print(f\"ROUGE-L: {rouge_scores['rougeL'].fmeasure:.3f}\")\n","\n","print(\"\\n=== 다른 텍스트 비교 ===\")\n","rouge_scores = scorer.score(reference, generated2)\n","\n","print(f\"ROUGE-1: {rouge_scores['rouge1'].fmeasure:.3f}\")\n","print(f\"ROUGE-2: {rouge_scores['rouge2'].fmeasure:.3f}\")\n","print(f\"ROUGE-L: {rouge_scores['rougeL'].fmeasure:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"oFFQz7bsriJ7"},"source":["`(2) BLEU (Bilingual Evaluation Understudy)`\n","   \n","   - 생성된 텍스트가 얼마나 자연스러운지를 평가\n","   - 0부터 1 사이의 값으로 표시 (1에 가까울수록 좋음)\n","\n","  - 기계 번역 평가의 대표적 지표\n","  - n-gram 정밀도 기반\n","  - 간결성 페널티 적용\n","  - 1-4gram까지의 기하평균 사용\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"osX9xa2lriJ7"},"outputs":[],"source":["from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from typing import List, Union\n","\n","# BLEU 스코어 계산\n","def calculate_bleu_score(\n","    reference: Union[str, List[str]],\n","    hypothesis: str,\n","    weights: tuple = (0.25, 0.25, 0.25, 0.25),  # n-gram 가중치 설정 (기본값: 균등 가중치)\n","    smoother=SmoothingFunction().method1,  # Smoothing 메소드 설정 -> method1 사용 (0으로 나누는 것 방지)\n","    tokenizer=kiwi_tokenizer # Kiwi 토크나이저 사용\n",") -> float:\n","    \"\"\"\n","    BLEU 스코어 계산\n","\n","    Args:\n","        reference (Union[str, List[str]]): 참조 텍스트 또는 텍스트 리스트\n","        hypothesis (str): 비교할 생성 텍스트\n","        weights (tuple): n-gram 가중치 (기본값: 균등 가중치)\n","        tokenizer: 토크나이저 객체 (기본값: Kiwi 토크나이저)\n","\n","    Returns:\n","        float: BLEU 스코어 (0~1)\n","    \"\"\"\n","    try:\n","        # 참조 텍스트 처리\n","        if isinstance(reference, str):\n","            references = [tokenizer.tokenize(reference)]\n","        else:\n","            references = [tokenizer.tokenize(ref) for ref in reference]\n","\n","        # 생성 텍스트 토크나이징\n","        hypothesis_tokens = tokenizer.tokenize(hypothesis)\n","\n","        # BLEU 스코어 계산\n","        score = sentence_bleu(\n","            references,\n","            hypothesis_tokens,\n","            weights=weights,\n","            smoothing_function=smoother\n","        )\n","\n","        return score\n","\n","    except Exception as e:\n","        print(f\"BLEU 스코어 계산 중 오류 발생: {str(e)}\")\n","        return 0.0\n","\n","\n","\n","# 예시 데이터\n","reference = \"오늘 날씨가 매우 좋습니다. 공원에서 산책하기 좋은 날이에요.\"   # 정답 텍스트\n","generated1 = \"오늘은 날씨가 정말 좋네요. 공원에서 산책하면 좋을 것 같아요.\" # 생성된 텍스트\n","generated2 = \"비가 많이 오고 있어요. 실내에서 쉬는 게 좋을 것 같습니다.\"   # 생성된 텍스트\n","\n","print(\"=== 유사한 텍스트 비교 ===\")\n","bleu_score = calculate_bleu_score(reference, generated1, tokenizer=kiwi_tokenizer)\n","print(f\"BLEU: {bleu_score:.3f}\")\n","\n","print(\"\\n=== 다른 텍스트 비교 ===\")\n","bleu_score = calculate_bleu_score(reference, generated2, tokenizer=kiwi_tokenizer)\n","print(f\"BLEU: {bleu_score:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"VWeLyY73riJ7"},"source":["### 3) **문자열 및 임베딩 거리** 평가\n","\n","- **문자열 및 임베딩 거리**는 예측값과 참조값 간의 유사도를 **정량적으로 측정**하는 평가 방식\n","\n","- **String Distance Evaluator**는 레벤슈타인 거리 등을 활용해 문자열 간의 **편집 거리**를 계산하며, 정규화된 점수를 제공함\n","\n","- **Embedding Distance Evaluator**는 텍스트의 **의미적 유사도**를 코사인 거리 등 다양한 메트릭으로 측정함\n","\n","- 문자열과 임베딩 기반 평가 방식은 예측값의 정확도를 객관적으로 측정할 수 있으며, 유연한 커스터마이징이 가능함"]},{"cell_type":"markdown","metadata":{"id":"n4vX8ODhriJ7"},"source":["`(1) String Distance`\n","\n","- **String Distance Evaluator**는 **레벤슈타인 거리**를 사용해 두 문자열이 얼마나 다른지 측정\n","\n","- 한 문자열을 다른 문자열로 변환하는데 필요한 **최소 편집 횟수**를 계산\n","\n","- 점수는 **0에서 1 사이로 정규화**되어 제공되며, 0에 가까울수록 문자열이 유사함을 의미\n","\n","- `rapidfuzz` 라이브러리를 통해 **효율적인 계산**이 가능하며, 대규모 평가에 적합 (설치 필요)\n","    - pip install --upgrade --quiet  rapidfuzz (또는 poetry add rapidfuzz)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9r2p0L1mriJ7"},"outputs":[],"source":["from langchain.evaluation import load_evaluator\n","\n","# 문자열 거리 기반 평가기 생성\n","string_distance_evaluator = load_evaluator(\n","    evaluator=\"string_distance\",\n","    distance=\"levenshtein\"    # damerau_levenshtein, levenshtein, jaro, jaro_winkler\n",")\n","\n","# 문자열 거리 기반 평가기를 사용하여 평가 (일치하는 사례)\n","result = string_distance_evaluator.evaluate_strings(\n","    prediction= \"네, 오늘은 날씨가 맑고 좋네요\",\n","    reference=\"네, 오늘은 날씨가 맑고 좋네요\"\n",")\n","\n","# 평가 결과 출력\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-DYmM-6riJ8"},"outputs":[],"source":["# 문자열 거리 기반 평가자를 사용하여 평가 (불일치하는 사례)\n","result = string_distance_evaluator.evaluate_strings(\n","    prediction= \"해당 제품의 가격은 50,000원입니다\",\n","    reference=\"이 제품의 가격이 얼마인가요?\"\n",")\n","\n","# 평가 결과 출력\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"NT5Xs5_kriJ8"},"source":["`(2) Embedding Distance`\n","\n","- **임베딩 거리 평가도구**는 텍스트를 **고차원 벡터**로 변환하여 의미적 유사도를 계산\n","\n","- **코사인 거리**, 유클리디안 거리, 맨해튼 거리 등 **다양한 거리 메트릭**을 선택적으로 활용할 수 있음\n","\n","- OpenAI나 HuggingFace 등 다양한 **임베딩 제공자**를 설정할 수 있음\n","\n","- 단순 문자열 비교와 달리 **문맥적 의미**를 고려한 평가가 가능함"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X1S5dGAPriJ8"},"outputs":[],"source":["from langchain.evaluation import load_evaluator\n","from langchain_openai import OpenAIEmbeddings\n","\n","# Evaluator 초기화\n","embedding_evaluator = load_evaluator(\n","    evaluator='embedding_distance',               # 임베딩 거리를 기반으로 평가\n","    distance_metric='cosine',                     # 거리 측정 방법: cosine, euclidean, manhattan, chebyshev, hamming\n","    embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\")   # 기본값: OpenAI 임베딩 모델 사용\n",")\n","\n","# 의미가 다른 문장 비교\n","result1 = embedding_evaluator.evaluate_strings(\n","    prediction=\"나는 학교에 갈 것이다\",\n","    reference=\"나는 집에 있을 것이다\"\n","    )\n","\n","print(\"의미가 다른 문장 비교 결과:\", result1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"St5jwXteriJ8"},"outputs":[],"source":["# 의미가 비슷한 문장 비교\n","result2 = embedding_evaluator.evaluate_strings(\n","    prediction=\"나는 학교에 갈 것이다\",\n","    reference=\"나는 학교로 이동할 것이다\",\n","    )\n","\n","print(\"의미가 비슷한 문장 비교 결과:\", result2)"]},{"cell_type":"markdown","metadata":{"id":"wMuJ83mEriJ8"},"source":["---\n","\n","## **LangChainStringEvaluator**\n","\n","- **LangSmith**와 연동되어 다양한 **평가 도구를 제공**하는 클래스\n","\n","- **문자열 거리**, **임베딩 거리**, **LLM 기반 평가** 등 다양한 **내장 평가자**를 쉽게 이용 가능\n","\n","- 평가 결과는 **LangSmith 플랫폼**에서 체계적으로 관리되고 시각화될 수 있음\n","\n","- 사용자 정의 평가자를 만들거나 기존 평가자를 **커스터마이즈**하는 것도 가능함"]},{"cell_type":"markdown","metadata":{"id":"JqQM5ZWsriJ8"},"source":["`(1) 사전 준비`\n","\n","- **데이터셋 생성**이 평가 과정의 첫 단계로 요구됨\n","\n","- **LangSmith 클라이언트** 설정이 평가 시작 전 필수적으로 이루어져야 함"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sl6y3VfAriJ8"},"outputs":[],"source":["from langsmith import Client\n","\n","# Langsmith 클라이언트 초기화\n","client = Client()\n","\n","# 유사 문장 데이터셋 정의\n","similar_sentences = [\n","    (\"오늘 날씨가 좋네요.\", \"날씨가 정말 좋은 하루네요.\"),\n","    (\"이 책은 재미있어요.\", \"이 책이 너무 재미있네요.\"),\n","    (\"저는 커피를 좋아해요.\", \"커피를 정말 사랑합니다.\"),\n","    (\"이 영화 추천해요.\", \"이 영화 꼭 보시는 걸 추천드립니다.\"),\n","    (\"서울은 인구가 많아요.\", \"서울은 사람이 매우 많은 도시예요.\")\n","]\n","\n","# 데이터셋 생성\n","dataset_name = \"Korean_Similarity_Set_V1\"\n","dataset = client.create_dataset(dataset_name=dataset_name)\n","\n","# 입력과 출력 데이터 준비\n","inputs = [{\"input_text\": input_text} for input_text, _ in similar_sentences]\n","outputs = [{\"similar_text\": similar_text} for _, similar_text in similar_sentences]\n","\n","# 데이터셋에 예제 추가\n","client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"]},{"cell_type":"markdown","metadata":{"id":"8Z9a2UiNriJ8"},"source":["`(2) 문장 유사도 평가`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RBXlMH1WriJ8"},"outputs":[],"source":["from langsmith.evaluation import LangChainStringEvaluator, evaluate\n","from langchain_openai import OpenAIEmbeddings\n","\n","# 문자열 거리 기반 평가\n","string_distance_evaluator = LangChainStringEvaluator(\n","    \"string_distance\",\n","    config={\"distance\": \"levenshtein\", \"normalize_score\": True},\n","    prepare_data=lambda run, example: {\n","        \"prediction\": run.outputs[\"predicted_text\"],    # 실험을 통해 생성되는 예측값\n","        \"reference\": example.outputs[\"similar_text\"],   # 데이터셋에서 제공되는 참조값\n","    }\n",")\n","\n","# 임베딩 거리 기반 평가\n","embedding_distance_evaluator = LangChainStringEvaluator(\n","    \"embedding_distance\",\n","        config={\n","        \"distance_metric\": \"cosine\",\n","        \"embeddings\": OpenAIEmbeddings(model=\"text-embedding-3-small\")\n","    },\n","    prepare_data=lambda run, example: {\n","        \"prediction\": run.outputs[\"predicted_text\"],\n","        \"reference\": example.outputs[\"similar_text\"],\n","    }\n",")\n","\n","# 평가 실행\n","def similarity_test(input_data):\n","    # 여기서는 간단한 예시로 입력 문장을 그대로 반환\n","    return {\"predicted_text\": input_data[\"input_text\"]}\n","\n","evaluation_result = evaluate(\n","    similarity_test,\n","    data=dataset_name,\n","    evaluators=[\n","        string_distance_evaluator,\n","        embedding_distance_evaluator\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xyQOoq_KriJ8"},"outputs":[],"source":["evaluation_result"]},{"cell_type":"markdown","metadata":{"id":"Rvc_h2uBriJ9"},"source":["---\n","### **[실습]**\n","\n","- LangChainStringEvaluator 사용하여 테스트셋의 정답과 openai_rag_chain의 생성된 답변의 품질을 비교 평가합니다.\n","- 평가 메트릭: 문자열 거리 기반 평가, 임베딩 거리 기반 평가"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1GaNwx1riJ9"},"outputs":[],"source":["df_qa_test.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"adM1UlqsriJ9"},"outputs":[],"source":["from langsmith import Client\n","\n","# Langsmith 클라이언트 초기화\n","client = Client()\n","\n","# 데이터셋 생성\n","dataset_name = \"Livian_Tesala_RAG_Test_V1\"\n","dataset = client.create_dataset(dataset_name=dataset_name)\n","\n","# 입력 데이터 준비\n","inputs = [{\"user_input\": user_input, \"reference_contexts\": reference_contexts} for user_input, reference_contexts in zip(df_qa_test.head()['user_input'], df_qa_test.head()['reference_contexts'])]\n","outputs = [{\"reference\": reference} for reference in df_qa_test.head()['reference']]\n","\n","# 데이터셋에 예제 추가\n","client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmeBl4RtriJ9"},"outputs":[],"source":["# 여기에 평가 함수를 정의하고 실행하세요."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8AeRdgMGriJ9"},"outputs":[],"source":["from langsmith.evaluation import LangChainStringEvaluator, evaluate\n","from langchain_openai import OpenAIEmbeddings\n","\n","# 문자열 거리 기반 평가\n","string_distance_evaluator = LangChainStringEvaluator(\n","    \"string_distance\",\n","    config={\"distance\": \"levenshtein\", \"normalize_score\": True},\n","    prepare_data=lambda run, example: {\n","        \"prediction\": run.outputs[\"predicted_text\"],\n","        \"reference\": example.outputs[\"reference\"],\n","    }\n",")\n","\n","# 임베딩 거리 기반 평가\n","embedding_distance_evaluator = LangChainStringEvaluator(\n","    \"embedding_distance\",\n","        config={\n","        \"distance_metric\": \"cosine\",\n","        \"embeddings\": OpenAIEmbeddings(model=\"text-embedding-3-small\")\n","    },\n","    prepare_data=lambda run, example: {\n","        \"prediction\": run.outputs[\"predicted_text\"],\n","        \"reference\": example.outputs[\"reference\"],\n","    }\n",")\n","\n","# 평가 실행\n","def similarity_test(input_data):\n","    # RAG 체인을 사용하여 예측 생성\n","    return {\"predicted_text\": openai_rag_chain.invoke(input_data[\"user_input\"]) }\n","\n","evaluation_result = evaluate(\n","    similarity_test,\n","    data=dataset_name,\n","    evaluators=[\n","        string_distance_evaluator,\n","        embedding_distance_evaluator\n","    ]\n",")"]},{"cell_type":"code","source":["from langsmith.evaluation import LangChainStringEvaluator, evaluate\n","from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain.evaluation import load_evaluator\n","\n","# 1. 문자열 거리 기반 평가\n","string_distance_evaluator = LangChainStringEvaluator(\n","    \"string_distance\",\n","    config={\"distance\": \"levenshtein\", \"normalize_score\": True},\n","    prepare_data=lambda run, example: {\n","        \"prediction\": run.outputs[\"predicted_text\"],\n","        \"reference\": example.outputs[\"reference\"],\n","    }\n",")\n","\n","# 2. 임베딩 거리 기반 평가\n","embedding_distance_evaluator = LangChainStringEvaluator(\n","    \"embedding_distance\",\n","    config={\n","        \"distance_metric\": \"cosine\",\n","        \"embeddings\": OpenAIEmbeddings(model=\"text-embedding-3-small\")\n","    },\n","    prepare_data=lambda run, example: {\n","        \"prediction\": run.outputs[\"predicted_text\"],\n","        \"reference\": example.outputs[\"reference\"],\n","    }\n",")\n","\n","\n","# 3. Reference-free : 일관성 평가 - score_string 평가자 사용\n","# LangChainStringEvaluator로 score_criteria 평가기 구성\n","score_string_evaluator = LangChainStringEvaluator(\n","    \"score_string\",\n","    config={\n","        \"criteria\": \"coherence\",   # 일관성 평가,\n","        \"normalize_by\": 10,\n","        \"llm\": ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n","    },\n","    prepare_data=lambda run, example: {\n","        \"input\": example.inputs[\"user_input\"],\n","        \"prediction\": run.outputs[\"predicted_text\"]\n","    }\n",")\n","\n","# 4. Reference-based : Labeled Criteria 평가기 추가\n","# 사용자 정의 프롬프트 템플릿 생성\n","template = \"\"\"Respond Y or N based on how well the following response follows the specified rubric. Grade only based on the rubric and expected response:\n","\n","Grading Rubric: {criteria}\n","Ground Truth: {reference}\n","\n","DATA:\n","---------\n","Question: {input}\n","Response: {output}\n","---------\n","Write out your explanation for each criterion in 한국어, then respond with Y or N on a new line.\"\"\"\n","\n","prompt = PromptTemplate.from_template(template)\n","\n","# LangChainStringEvaluator로 labeled_criteria 평가기 구성\n","labeled_criteria_evaluator = LangChainStringEvaluator(\n","    \"labeled_criteria\",\n","    config={\n","        \"criteria\": {\n","            \"correctness\": \"Given the provided ground truth, is the response correct?\",\n","            \"relevance\": \"Does the response appropriately address the question?\",\n","        },\n","        \"llm\": ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.0),\n","        \"prompt\": prompt,\n","    },\n","    prepare_data=lambda run, example: {\n","        \"input\": example.inputs[\"user_input\"],\n","        \"prediction\": run.outputs[\"predicted_text\"],\n","        \"reference\": example.outputs[\"reference\"],\n","    }\n",")\n","\n","# 평가 실행 함수\n","def similarity_test(input_data):\n","    # RAG 체인을 사용하여 예측 생성\n","    answer = openai_rag_chain.invoke(input_data[\"user_input\"])\n","    return {\"predicted_text\": answer}\n","\n","# 모든 평가기를 함께 실행\n","evaluation_result = evaluate(\n","    similarity_test,\n","    data=dataset_name,\n","    evaluators=[\n","        string_distance_evaluator,        # 문자열 거리 평가\n","        embedding_distance_evaluator,     # 임베딩 거리 평가\n","        score_string_evaluator,           # Score Criteria 평가\n","        labeled_criteria_evaluator        # Labeled Criteria 평가\n","    ]\n",")"],"metadata":{"id":"rYXVtojirj1i"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"kt-ds","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}