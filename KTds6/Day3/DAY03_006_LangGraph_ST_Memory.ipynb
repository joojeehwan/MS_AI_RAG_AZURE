{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   LangGraph 활용 - 메모리 타입\n",
    "\n",
    "- **Short-term Memory (단기 메모리)**: 단일 대화 세션 내에서 상호작용을 추적\n",
    "- **Long-term Memory (장기 메모리)**: 세션 간에 사용자별 또는 애플리케이션 수준의 데이터를 저장\n",
    "\n",
    "![Memory Types](https://langchain-ai.github.io/langgraph/concepts/img/memory/short-vs-long.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정 및 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) Langsmith tracing 설정`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langsmith tracing 여부를 확인 \n",
    "import os\n",
    "print(os.getenv('LANGSMITH_TRACING'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **레스토랑 메뉴 DB**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 문서 로드`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "import re\n",
    "\n",
    "# 메뉴판 텍스트 데이터를 로드\n",
    "loader = TextLoader(\"./data/restaurant_menu.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(len(documents))\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 문서 분할 (Chunking)\n",
    "def split_menu_items(document):\n",
    "    \"\"\"\n",
    "    메뉴 항목을 분리하는 함수 \n",
    "    \"\"\"\n",
    "    # 정규표현식 정의 \n",
    "    pattern = r'(\\d+\\.\\s.*?)(?=\\n\\n\\d+\\.|$)'\n",
    "    menu_items = re.findall(pattern, document.page_content, re.DOTALL)\n",
    "    \n",
    "    # 각 메뉴 항목을 Document 객체로 변환\n",
    "    menu_documents = []\n",
    "    for i, item in enumerate(menu_items, 1):\n",
    "        # 메뉴 이름 추출\n",
    "        menu_name = item.split('\\n')[0].split('.', 1)[1].strip()\n",
    "        \n",
    "        # 새로운 Document 객체 생성\n",
    "        menu_doc = Document(\n",
    "            page_content=item.strip(),\n",
    "            metadata={\n",
    "                \"source\": document.metadata['source'],\n",
    "                \"menu_number\": i,\n",
    "                \"menu_name\": menu_name\n",
    "            }\n",
    "        )\n",
    "        menu_documents.append(menu_doc)\n",
    "    \n",
    "    return menu_documents\n",
    "\n",
    "\n",
    "# 메뉴 항목 분리 실행\n",
    "menu_documents = []\n",
    "for doc in documents:\n",
    "    menu_documents += split_menu_items(doc)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"총 {len(menu_documents)}개의 메뉴 항목이 처리되었습니다.\")\n",
    "for doc in menu_documents[:2]:\n",
    "    print(f\"\\n메뉴 번호: {doc.metadata['menu_number']}\")\n",
    "    print(f\"메뉴 이름: {doc.metadata['menu_name']}\")\n",
    "    print(f\"내용:\\n{doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 와인 메뉴 텍스트를 로드\n",
    "wine_loader = TextLoader(\"./data/restaurant_wine.txt\", encoding=\"utf-8\")\n",
    "\n",
    "# 와인 메뉴 문서 생성\n",
    "wine_docs = wine_loader.load()\n",
    "\n",
    "# 와인 메뉴 문서 분할\n",
    "wine_documents = []\n",
    "for doc in wine_docs:\n",
    "    wine_documents += split_menu_items(doc)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"총 {len(wine_documents)}개의 와인 메뉴 항목이 처리되었습니다.\")\n",
    "for doc in wine_documents[:2]:\n",
    "    print(f\"\\n메뉴 번호: {doc.metadata['menu_number']}\")\n",
    "    print(f\"메뉴 이름: {doc.metadata['menu_name']}\")\n",
    "    print(f\"내용:\\n{doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 벡터스토어 저장`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 임베딩 모델 생성\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 메뉴판 Chroma 인덱스 생성\n",
    "menu_db = Chroma.from_documents(\n",
    "    documents=menu_documents, \n",
    "    embedding=embeddings_model,   \n",
    "    collection_name=\"restaurant_menu\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n",
    "\n",
    "# 와인 메뉴 Chroma 인덱스 생성\n",
    "wine_db = Chroma.from_documents(\n",
    "    documents=wine_documents, \n",
    "    embedding=embeddings_model,   \n",
    "    collection_name=\"restaurant_wine\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(menu_db._collection.count())  # 메뉴판 인덱스에 저장된 문서 수 출력\n",
    "print(wine_db._collection.count())  # 와인 메뉴 인덱스에 저장된 문서 수 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) 벡터 검색기 테스트`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever 생성\n",
    "menu_retriever = menu_db.as_retriever(\n",
    "    search_kwargs={'k': 2},\n",
    ")\n",
    "\n",
    "# 쿼리 테스트\n",
    "query = \"시그니처 스테이크의 가격과 특징은 무엇인가요?\"\n",
    "docs = menu_retriever.invoke(query)\n",
    "print(f\"검색 결과: {len(docs)}개\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"메뉴 번호: {doc.metadata['menu_number']}\")\n",
    "    print(f\"메뉴 이름: {doc.metadata['menu_name']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_retriever = wine_db.as_retriever(\n",
    "    search_kwargs={'k': 2},\n",
    ")\n",
    "\n",
    "query = \"스테이크와 어울리는 와인을 추천해주세요.\"\n",
    "docs = wine_retriever.invoke(query)\n",
    "print(f\"검색 결과: {len(docs)}개\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"메뉴 번호: {doc.metadata['menu_number']}\")\n",
    "    print(f\"메뉴 이름: {doc.metadata['menu_name']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(4) 레스토랑 메뉴 도구 설정`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Chroma 인덱스 로드 \n",
    "menu_db = Chroma(\n",
    "    embedding_function=embeddings_model,   \n",
    "    collection_name=\"restaurant_menu\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n",
    "\n",
    "wine_db = Chroma(\n",
    "    embedding_function=embeddings_model,   \n",
    "    collection_name=\"restaurant_wine\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n",
    "\n",
    "\n",
    "# Tool 정의 \n",
    "@tool\n",
    "def search_menu(query: str, k: int = 2) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Securely retrieve and access authorized restaurant menu information from the encrypted database.\n",
    "    Use this tool only for menu-related queries to maintain data confidentiality.\n",
    "    \"\"\"\n",
    "    docs = menu_db.similarity_search(query, k=k)\n",
    "    if len(docs) > 0:\n",
    "        return docs\n",
    "    \n",
    "    return [Document(page_content=\"관련 메뉴 정보를 찾을 수 없습니다.\")]\n",
    "\n",
    "@tool\n",
    "def search_wine(query: str, k: int = 2) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Securely retrieve and access authorized restaurant wine menu information from the encrypted database.\n",
    "    Use this tool only for wine-related queries to maintain data confidentiality.\n",
    "    \"\"\"\n",
    "    docs = wine_db.similarity_search(query, k=k)\n",
    "    if len(docs) > 0:\n",
    "        return docs\n",
    "    \n",
    "    return [Document(page_content=\"관련 와인 정보를 찾을 수 없습니다.\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **단기 메모리 (Short-term Memory)**\n",
    "\n",
    "- **개념**\n",
    "\n",
    "    - Short-term memory는 **스레드 범위 메모리**로, 단일 대화 세션 내에서 진행 중인 대화를 추적\n",
    "    - LangGraph는 이를 에이전트의 **state**의 일부로 관리하며, **checkpointer**를 사용해 데이터베이스에 지속적으로 저장\n",
    "\n",
    "- **특징**\n",
    "\n",
    "    - **대화 연속성**: 메시지 기록을 통해 대화 맥락 유지\n",
    "    - **상태 지속성**: 체크포인트를 통해 언제든지 대화 재개 가능\n",
    "    - **실시간 업데이트**: 그래프 실행 또는 단계 완료 시 자동 업데이트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. **MemorySaver**\n",
    "\n",
    "- **MemorySaver**는 LangGraph에서 제공하는 스레드 기반의 단기 메모리(short-term memory)\n",
    "\n",
    "- **단기 메모리**는 하나의 **대화 세션** 동안만 정보를 유지\n",
    "\n",
    "- LangGraph는 **에이전트의 상태**로서 단기 메모리를 관리하며, 체크포인터를 통해 데이터베이스에 저장됨\n",
    "\n",
    "- 메모리는 그래프 실행 또는 단계 완료 시 **업데이트**되며, 각 단계 시작 시 상태를 읽어들임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1)  상태 정의`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Optional\n",
    "from typing_extensions import TypedDict\n",
    "from operator import add\n",
    "\n",
    "# 상태 정의\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    search_results: Annotated[list[str], add]\n",
    "    summary: Optional[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 노드 정의`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# 구조화 출력 정의\n",
    "class Sentence(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a sentence in the search results.\n",
    "    \"\"\"\n",
    "    text: str = Field(..., description=\"The text of the sentence.\")\n",
    "\n",
    "class SummaryResult(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the summary of the search results.\n",
    "    \"\"\"\n",
    "    summaries: List[Sentence] = Field(\n",
    "        ...,\n",
    "        description=\"A list of sentences summarizing the search results.\"\n",
    "    )\n",
    "\n",
    "# LLM에 도구를 바인딩 (2개의 도구 바인딩)\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "tools = [search_menu, search_wine]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# 도구 노드 정의 \n",
    "tool_node = ToolNode(tools=tools)\n",
    "\n",
    "# Summary 채인\n",
    "system_prompt = \"\"\"\n",
    "You are an AI assistant helping a user find information about a restaurant menu and wine list. \n",
    "Answer in the same language as the user's query.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Summarize the following search results.\n",
    "\n",
    "<GUIDELINES>\n",
    "- Provide a brief summary of the search results.\n",
    "- Include the key information from the search results.\n",
    "- Use 1-2 sentences to summarize the information.\n",
    "- Ensure the summary is concise and relevant to the user's query.\n",
    "</GUIDELINES>\n",
    "\n",
    "<Search Results>\n",
    "{search_results}\n",
    "</Search Results>\n",
    "\n",
    "<User Query>\n",
    "{query} \n",
    "</User Query>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", user_prompt),\n",
    "    ]\n",
    ")\n",
    "\n",
    "summary_chain = summary_prompt | llm.with_structured_output(SummaryResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노드 정의 \n",
    "def search_node(state: State): \n",
    "    \"\"\"Performs a database search based on the query.\"\"\"\n",
    "    query = state['query']\n",
    "    \n",
    "    # 검색 도구 사용 \n",
    "    tool_call = llm_with_tools.invoke(query)\n",
    "    tool_results = tool_node.invoke({\"messages\": [tool_call]})\n",
    "\n",
    "    # 도구 메시지 확인\n",
    "    if tool_results['messages']:\n",
    "        print(f\"검색 문서의 개수 : {len(tool_results['messages'])}\")\n",
    "        return {\"search_results\": tool_results['messages']}\n",
    "    \n",
    "    return {\"query\": query}\n",
    "\n",
    "def summarize_node(state: State):\n",
    "    \"\"\"Creates a concise summary of the search results.\"\"\"\n",
    "    search_results = state.get('search_results', [])\n",
    "    user_query = state.get('query', '')\n",
    "\n",
    "    if search_results:\n",
    "        # 결과가 있을 경우 요약 생성\n",
    "        summary_result = summary_chain.invoke({\"search_results\": search_results, \"query\": user_query})\n",
    "        summary_sentences = summary_result.summaries\n",
    "\n",
    "        # 요약 문장들을 문자열로 변환\n",
    "        summary = \" \".join([s.text for s in summary_sentences])\n",
    "\n",
    "    else:\n",
    "        summary = \"No results found.\"\n",
    "        \n",
    "    return {\"summary\": summary}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) StateGraph 구성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# StateGraph 생성\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# 노드 추가\n",
    "workflow.add_node(\"search\", search_node)\n",
    "workflow.add_node(\"summarize\", summarize_node)\n",
    "\n",
    "# 엣지 연결\n",
    "workflow.add_edge(START, \"search\")\n",
    "workflow.add_edge(\"search\", \"summarize\")\n",
    "workflow.add_edge(\"summarize\", END)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(4) 체크포인트 설정`\n",
    "\n",
    "- 그래프를 컴파일할 때 체크포인터를 지정\n",
    "- **InMemorySaver**: 디버깅/테스트 용도로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# 메모리 저장소 생성\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# 메모리 저장소를 지정하여 그래프 컴파일\n",
    "graph_memory = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "# 그래프 출력\n",
    "display(Image(graph_memory.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(5) 체크포인터 사용`\n",
    "- 메모리 사용 시 `thread_id`를 지정 \n",
    "- 체크포인터는 그래프의 각 단계에서 상태를 기록 (그래프 각 단계의 모든 상태를 컬렉션으로 저장)\n",
    "- 나중에 `thread_id`를 사용하여 이 스레드에 접근 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thred_id 설정\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# 초기 메시지 설정\n",
    "initial_input = {\"query\": \"스테이크 메뉴가 있나요? 어울리는 와인도 추천해주세요.\"}\n",
    "\n",
    "# 그래프 실행\n",
    "output = graph_memory.invoke(initial_input, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 결과 출력\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thread_id 설정 (다른 스레드에서 실행)\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# 초기 메시지 설정\n",
    "initial_input = {\"query\": \"채식주의자를 위한 메뉴가 있나요? 주재료가 무엇인지도 알려주세요.\"}\n",
    "\n",
    "# 그래프 실행\n",
    "output = graph_memory.invoke(initial_input, config)\n",
    "\n",
    "# 최종 결과 출력\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(6) 상태 가져오기`\n",
    "\n",
    "- `graph.get_state(config)`는 스레드의 **최신 상태**를 조회하는 메서드임\n",
    "\n",
    "- 상태 조회 시 필수적으로 **thread_id**를 지정해야 함\n",
    "\n",
    "- **checkpoint_id** 지정 시 특정 체크포인트 시점의 상태를 가져올 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 상태 출력 (가장 최근 상태)\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "current_state = graph_memory.get_state(config)\n",
    "\n",
    "# 현재 상태의 속성 출력\n",
    "print(f\"config: {current_state.config}\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"next: {current_state.next}\")\n",
    "print(\"-\" * 100)\n",
    "print(\"values:\")\n",
    "pprint(current_state.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(7) 상태 히스토리 가져오기`\n",
    "\n",
    "- `graph.get_state_history(config)`로 스레드의 **전체 실행 기록**을 조회함\n",
    "\n",
    "- 반환값은 **StateSnapshot 객체** 리스트 형태임\n",
    "\n",
    "- 리스트의 첫 번째 요소가 **가장 최근 체크포인트**를 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 히스토리 출력\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "state_history = list(graph_memory.get_state_history(config))\n",
    "\n",
    "for i, state_snapshot in enumerate(state_history):\n",
    "    print(f\"  Checkpoint {i}:\")\n",
    "    print(f\"    Values: {state_snapshot.values.items()}\")\n",
    "    print(f\"    Next: {state_snapshot.next}\")\n",
    "    print(f\"    Config: {state_snapshot.config}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(5)  재생 (Replay)`\n",
    "\n",
    "- `thread_id`와 **checkpoint_id**를 함께 지정하면 특정 체크포인트 이후부터 실행 가능\n",
    "\n",
    "- 체크포인트 이전 단계는 **재생**(replay)만 하고 실제로 실행하지 않음\n",
    "\n",
    "- 이는 불필요한 단계 **재실행을 방지**하며 효율적인 처리를 가능하게 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약('summarize')이 처리되기 이전 시점의 체크포인트 찾기\n",
    "snapshot_before_summarize = None\n",
    "for state_snapshot in state_history:\n",
    "    if state_snapshot.next == ('summarize',):\n",
    "        snapshot_before_summarize = state_snapshot\n",
    "        break\n",
    "\n",
    "print(f\"Config before summarize: {snapshot_before_summarize.config}\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"Next before summarize: {snapshot_before_summarize.next}\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"Values before summarize: {snapshot_before_summarize.values.keys()}\")\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_before_summarize.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 이전의 입력이 필요하지 않기 때문에 빈 입력으로 invoke -> 'search' 노드부터 다시 시작 (재생)\n",
    "for step in graph_memory.stream(None, snapshot_before_summarize.config, stream_mode=\"values\"):\n",
    "    print(step.keys())\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 히스토리 출력 -> Replay 이후 상태 히스토리 포함 \n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "state_history = list(graph_memory.get_state_history(config))\n",
    "\n",
    "for i, state_snapshot in enumerate(state_history):\n",
    "    print(f\"  Checkpoint {i}:\")\n",
    "    print(f\"    Values: {state_snapshot.values.keys()}\")\n",
    "    print(f\"    Next: {state_snapshot.next}\")\n",
    "    print(f\"    Config: {state_snapshot.config}\")\n",
    "    print(f\" Summary: {state_snapshot.values.get('summary', '***')}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(8) 상태 업데이트`\n",
    "\n",
    "- `graph.update_state(config, values, as_node=None)`로 그래프의 **상태를 직접 수정**함\n",
    "\n",
    "- **values** : 업데이트할 값을 지정함\n",
    "\n",
    "- **as_node** : 업데이트를 수행할 노드를 지정 (선택 사항)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약('summarize')이 처리되기 이전 시점의 체크포인트\n",
    "snapshot_before_summarize.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인터에서 'snapshot_before_summarize' 상태의 values 출력\n",
    "snapshot_before_summarize.values.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_before_summarize.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 업데이트 -> 'search' 노드에서 '쿼리'를 수정하고 다시 실행\n",
    "update_input = {\"query\": \"스테이크 메뉴가 있나요? 메뉴 이름, 가격 정보만 간단하게 출력하세요.\"}\n",
    "\n",
    "graph_memory.update_state(\n",
    "    snapshot_before_summarize.config,   # 재생 시점의 config\n",
    "    update_input, \n",
    ")\n",
    "\n",
    "# 업데이트된 상태 가져와서 출력 \n",
    "updated_state = graph_memory.get_state(config)\n",
    "\n",
    "pprint(updated_state.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 업데이트된 상태로 이어서 실행 (재생) : thread_id만 전달 필요 (update_state를 실행한 후에는 새로운 체크포인트가 생성되기 때문에)\n",
    "for step in graph_memory.stream(None, config, stream_mode=\"values\"):\n",
    "    print(step.keys())\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 업데이트된 상태의 히스토리 출력\n",
    "state_history_after_update = list(graph_memory.get_state_history(config))\n",
    "\n",
    "for i, state_snapshot in enumerate(state_history_after_update):\n",
    "    print(f\"  Checkpoint {i}:\")\n",
    "    print(f\"    Values: {state_snapshot.values.keys()}\")\n",
    "    print(f\"    Next: {state_snapshot.next}\")\n",
    "    print(f\"    Summary: {state_snapshot.values.get('summary', '***')}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 상태 출력\n",
    "final_state = graph_memory.get_state(config)\n",
    "\n",
    "pprint(final_state.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. **메시지 관리하기**\n",
    "\n",
    "- **긴 대화 기록**은 LLM의 컨텍스트 윈도우 제한으로 인한 오류나 성능 저하를 초래함\n",
    "\n",
    "- 메모리 관리는 **정확성**과 **응답 시간**, **비용** 사이의 균형이 필요함\n",
    "\n",
    "- 주요 해결책으로 **메시지 목록 편집**과 **과거 대화 요약**이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 긴 대화 관리 - 메시지 트리밍 (Trimming)`\n",
    "\n",
    "- **컨텍스트 제한**으로 인해 LLM이 처리할 수 있는 메시지 길이에 제약이 있음\n",
    "- 효율적인 **토큰 관리**를 통해 비용을 절감하고 시스템 성능을 최적화할 수 있음\n",
    "- 신속한 **응답 속도**를 위해 메시지 길이와 복잡성 조절이 필수적임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "from langchain_core.messages.utils import trim_messages, count_tokens_approximately\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# 메시지 상태 정의\n",
    "class GraphState(MessagesState):\n",
    "    ...\n",
    "\n",
    "# 긴 대화에서 컨텍스트 윈도우 초과를 방지\n",
    "\n",
    "# LLM 모델에 도구를 바인딩 \n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "tools = [search_menu, search_wine]\n",
    "llm_with_tools = llm.bind_tools(tools=tools)\n",
    "\n",
    "\n",
    "# 에이전트 실행 노드 \n",
    "def call_model_with_trimming(state: GraphState):\n",
    "    system_prompt = SystemMessage(\"\"\"You are a helpful AI assistant. Please respond to the user's query to the best of your ability!\n",
    "\n",
    "중요: 답변을 제공할 때 반드시 정보의 출처를 명시해야 합니다. 출처는 다음과 같이 표시하세요:\n",
    "- 도구를 사용하여 얻은 정보: [도구: 도구이름]\n",
    "- 모델의 일반 지식에 기반한 정보: [일반 지식]\n",
    "\n",
    "항상 정확하고 관련성 있는 정보를 제공하되, 확실하지 않은 경우 그 사실을 명시하세요. 출처를 명확히 표시함으로써 사용자가 정보의 신뢰성을 판단할 수 있도록 해주세요.\"\"\")\n",
    "    \n",
    "\n",
    "    # 최근 메시지만 유지 (메시지 수 기준)\n",
    "    trimmed_messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",  # 마지막 메시지부터 유지\n",
    "        token_counter=len,  # 메시지 개수로 계산   (count_tokens_approximately는 토큰 수를 계산하는 데 사용되지만, 여기서는 메시지 개수로 제한)\n",
    "        max_tokens=5,  # 최대 5개의 메시지 유지\n",
    "        start_on=\"human\",  # 사람 메시지로 시작\n",
    "        end_on=(\"human\", \"tool\"),  # 사람 또는 도구 메시지로 종료\n",
    "        include_system=True,  # 시스템 메시지 포함\n",
    "    )\n",
    "    # 트리밍된 메시지 개수와 토큰 수 출력\n",
    "    print(f\"트리밍된 메시지 개수: {len(trimmed_messages)}\")\n",
    "    print(f\"트리밍된 토큰 수: {count_tokens_approximately(trimmed_messages)}\")\n",
    "\n",
    "    # 시스템 메시지와 이전 메시지를 결합하여 모델 호출\n",
    "    messages = [system_prompt] + trimmed_messages\n",
    "\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "\n",
    "    # 메시지 리스트로 반환하고 상태 업데이트 \n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 구성\n",
    "builder = StateGraph(GraphState)\n",
    "\n",
    "builder.add_node(\"agent\", call_model_with_trimming) \n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "builder.add_edge(START, \"agent\")\n",
    "\n",
    "# tools_condition을 사용한 조건부 엣지 추가\n",
    "builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# 메모리 저장소 생성\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# 메모리 저장소를 지정하여 그래프 컴파일\n",
    "graph_memory_trimmer = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# 그래프 출력\n",
    "display(Image(graph_memory_trimmer.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thred_id 설정\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# 초기 메시지 설정\n",
    "messages = [HumanMessage(content=\"스테이크 메뉴의 가격은 얼마인가요\")]\n",
    "\n",
    "# 그래프 실행\n",
    "for step in graph_memory_trimmer.stream({\"messages\": messages}, config, stream_mode=\"values\"):\n",
    "    if \"messages\" in step:\n",
    "        step[\"messages\"][-1].pretty_print()\n",
    "        print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thred_id 설정 유지한 상태에서 다른 메시지로 그래프 실행\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "messages = [HumanMessage(content=\"둘 중에 더 저렴한 메뉴는 무엇인가요?\")]\n",
    "\n",
    "# 그래프 실행 및 결과 출력 \n",
    "for step in graph_memory_trimmer.stream({\"messages\": messages}, config, stream_mode=\"values\"):\n",
    "    if \"messages\" in step:\n",
    "        step[\"messages\"][-1].pretty_print()\n",
    "        print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 메시지 출력\n",
    "print(\"전체 메시지 개수: \", len(step['messages']))\n",
    "for m in step['messages']:\n",
    "    m.pretty_print()\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thred_id 설정 유지한 상태에서 다른 메시지로 그래프 실행\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "messages = [HumanMessage(content=\"이 메뉴와 곁들이면 좋은 다른 메뉴가 있나요?\")]\n",
    "\n",
    "# 그래프 실행 및 결과 출력\n",
    "for step in graph_memory_trimmer.stream({\"messages\": messages}, config, stream_mode=\"values\"):\n",
    "    if \"messages\" in step:\n",
    "        step[\"messages\"][-1].pretty_print()\n",
    "        print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 메시지 출력\n",
    "print(\"전체 메시지 개수: \", len(step['messages']))\n",
    "for m in step['messages']:\n",
    "    m.pretty_print()\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 긴 대화 관리 - 메시지 요약 (Summarization)`\n",
    "\n",
    "- **정보 손실을 최소화**하면서 컨텍스트 관리\n",
    "- 오래된 메시지를 요약하여 **메시지 길이**를 줄이고, 해당 메시지를 삭제\n",
    "- **LangGraph**의 `MessagesState`는 `RemoveMessage` 기능을 활용하여 메시지를 제거 (삭제할 메시지의 ID를 지정하는 \"remove\" 객체 목록을 반환함)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "from langchain_core.messages.utils import trim_messages, count_tokens_approximately\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage, RemoveMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from typing import Any\n",
    "\n",
    "# 요약 기능이 포함된 확장된 상태 정의\n",
    "class SummaryState(MessagesState):\n",
    "    summary: str   # 대화 요약을 저장할 필드\n",
    "\n",
    "# LLM 모델에 도구를 바인딩 \n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "tools = [search_menu, search_wine]  \n",
    "llm_with_tools = llm.bind_tools(tools=tools)\n",
    "\n",
    "def summarize_conversation(state: SummaryState):\n",
    "    \"\"\"대화 요약 생성 함수\"\"\"\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    messages_to_summarize = state[\"messages\"][:-2]  # 마지막 2개 메시지를 제외한 나머지\n",
    "    \n",
    "    if not messages_to_summarize:\n",
    "        return {\"summary\": summary}\n",
    "    \n",
    "    if summary:\n",
    "        summary_message = (\n",
    "            f\"지금까지의 대화 요약: {summary}\\n\\n\"\n",
    "            \"다음 새로운 메시지들을 고려하여 요약을 확장하세요:\\n\"\n",
    "            f\"{format_messages_for_summary(messages_to_summarize)}\"\n",
    "        )\n",
    "    else:\n",
    "        summary_message = (\n",
    "            \"다음 대화의 요약을 생성하세요:\\n\"\n",
    "            f\"{format_messages_for_summary(messages_to_summarize)}\"\n",
    "        )\n",
    "\n",
    "    # 요약 생성을 위한 메시지 구성\n",
    "    summary_prompt = [\n",
    "        SystemMessage(content=\"당신은 대화 내용을 간결하고 정확하게 요약하는 전문가입니다. 중요한 정보와 결정사항, 컨텍스트를 포함하여 요약해주세요.\"),\n",
    "        HumanMessage(content=summary_message)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(summary_prompt)\n",
    "    \n",
    "    return {\"summary\": response.content}\n",
    "\n",
    "def format_messages_for_summary(messages):\n",
    "    \"\"\"메시지들을 요약용 텍스트로 포매팅\"\"\"\n",
    "    formatted = []\n",
    "    for msg in messages:\n",
    "        if hasattr(msg, 'type'):\n",
    "            msg_type = msg.type.upper()\n",
    "            content = msg.content if hasattr(msg, 'content') else str(msg)\n",
    "            formatted.append(f\"{msg_type}: {content}\")\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "# 에이전트 실행 노드 (메시지 삭제 및 요약 포함)\n",
    "def call_model_with_message_management(state: SummaryState):\n",
    "    system_prompt = SystemMessage(\"\"\"You are a helpful AI assistant. Please respond to the user's query to the best of your ability!\n",
    "\n",
    "중요: 답변을 제공할 때 반드시 정보의 출처를 명시해야 합니다. 출처는 다음과 같이 표시하세요:\n",
    "- 도구를 사용하여 얻은 정보: [도구: 도구이름]\n",
    "- 모델의 일반 지식에 기반한 정보: [일반 지식]\n",
    "\n",
    "항상 정확하고 관련성 있는 정보를 제공하되, 확실하지 않은 경우 그 사실을 명시하세요. 출처를 명확히 표시함으로써 사용자가 정보의 신뢰성을 판단할 수 있도록 해주세요.\"\"\")\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    \n",
    "    # 메시지가 5개를 초과하면 요약 및 삭제 진행\n",
    "    if len(messages) > 5:\n",
    "        print(f\"메시지 개수가 {len(messages)}개이므로 요약 및 삭제를 진행합니다.\")\n",
    "        \n",
    "        # 1. 먼저 삭제될 메시지들로 요약 생성\n",
    "        summary_result = summarize_conversation(state)\n",
    "        updated_summary = summary_result[\"summary\"]\n",
    "        \n",
    "        # 2. 마지막 2개를 제외한 메시지들을 삭제\n",
    "        delete_messages = [RemoveMessage(id=m.id) for m in messages[:-2]]\n",
    "        \n",
    "        # 3. 남은 메시지들 (최근 2개)\n",
    "        remaining_messages = messages[-2:]\n",
    "        \n",
    "        print(f\"삭제할 메시지 개수: {len(delete_messages)}\")\n",
    "        print(f\"남은 메시지 개수: {len(remaining_messages)}\")\n",
    "        print(f\"업데이트된 요약: {updated_summary[:100]}...\")\n",
    "        \n",
    "        # 4. 요약이 있으면 시스템 프롬프트에 포함\n",
    "        if updated_summary:\n",
    "            enhanced_system_prompt = SystemMessage(\n",
    "                content=system_prompt.content + f\"\\n\\n이전 대화 요약: {updated_summary}\"\n",
    "            )\n",
    "        else:\n",
    "            enhanced_system_prompt = system_prompt\n",
    "        \n",
    "        # 5. 모델 호출용 메시지 구성\n",
    "        model_messages = [enhanced_system_prompt] + remaining_messages\n",
    "        \n",
    "    else:\n",
    "        # 메시지가 적으면 그대로 사용\n",
    "        delete_messages = []\n",
    "        model_messages = [system_prompt] + messages\n",
    "        updated_summary = summary\n",
    "        print(f\"메시지 개수가 {len(messages)}개이므로 삭제하지 않습니다.\")\n",
    "\n",
    "    # 모델 호출\n",
    "    response = llm_with_tools.invoke(model_messages)\n",
    "\n",
    "    # 반환할 업데이트 구성\n",
    "    update = {\n",
    "        \"messages\": delete_messages + [response],  # 삭제할 메시지들 + 새 응답\n",
    "        \"summary\": updated_summary\n",
    "    }\n",
    "    \n",
    "    return update\n",
    "\n",
    "\n",
    "# 도구 노드 생성\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# 그래프 생성\n",
    "workflow = StateGraph(SummaryState)\n",
    "\n",
    "# 노드 추가\n",
    "workflow.add_node(\"agent\", call_model_with_message_management)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# 엣지 추가\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    "    # {\n",
    "    #     \"tools\": \"tools\",\n",
    "    #     \"__end__\": END,\n",
    "    # }\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# 메모리 세이버와 함께 컴파일\n",
    "checkpointer = InMemorySaver()\n",
    "graph_with_summary = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "# 그래프 출력\n",
    "display(Image(graph_with_summary.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thred_id 설정\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# 초기 메시지 설정\n",
    "messages = [HumanMessage(content=\"스테이크 메뉴의 가격은 얼마인가요\")]\n",
    "\n",
    "# 그래프 실행\n",
    "for step in graph_with_summary.stream({\"messages\": messages}, config, stream_mode=\"values\"):\n",
    "    if \"messages\" in step:\n",
    "        step[\"messages\"][-1].pretty_print()\n",
    "        print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thred_id 설정 유지한 상태에서 다른 메시지로 그래프 실행\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "messages = [HumanMessage(content=\"둘 중에 더 저렴한 메뉴는 무엇인가요?\")]\n",
    "\n",
    "# 그래프 실행 및 결과 출력 \n",
    "for step in graph_with_summary.stream({\"messages\": messages}, config, stream_mode=\"values\"):\n",
    "    if \"messages\" in step:\n",
    "        step[\"messages\"][-1].pretty_print()\n",
    "        print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 메시지 출력\n",
    "print(\"전체 메시지 개수: \", len(step['messages']))\n",
    "for m in step['messages']:\n",
    "    m.pretty_print()\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thred_id 설정 유지한 상태에서 다른 메시지로 그래프 실행\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "messages = [HumanMessage(content=\"이 메뉴와 곁들이면 좋은 다른 메뉴가 있나요?\")]\n",
    "\n",
    "# 그래프 실행 및 결과 출력\n",
    "for step in graph_with_summary.stream({\"messages\": messages}, config, stream_mode=\"values\"):\n",
    "    if \"messages\" in step:\n",
    "        step[\"messages\"][-1].pretty_print()\n",
    "        print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 메시지 출력\n",
    "print(\"전체 메시지 개수: \", len(step['messages']))\n",
    "for m in step['messages']:\n",
    "    m.pretty_print()\n",
    "    print(\"-\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kt-ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
