{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LangChainì˜ ê°œë…ê³¼ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ì´í•´\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LangChain ì†Œê°œ\n",
    "\n",
    "### 1.1 LangChainì´ë€?\n",
    "**LangChain**ì€ ëŒ€í™”í˜• AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‰½ê²Œ ê°œë°œí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "#### ğŸ“¦ í•µì‹¬ ê°€ì¹˜\n",
    "- **ëª¨ë“ˆí™”**: ë…ë¦½ì ì¸ ì»´í¬ë„ŒíŠ¸ë¥¼ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ AI ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "- **ìƒí˜¸ìš´ìš©ì„±**: ë‹¤ì–‘í•œ AI ëª¨ë¸ê³¼ ë°ì´í„° ì†ŒìŠ¤ë¥¼ í•˜ë‚˜ì˜ ì¸í„°í˜ì´ìŠ¤ë¡œ í†µí•©\n",
    "- **í™•ì¥ì„±**: ê°„ë‹¨í•œ ì±—ë´‡ë¶€í„° ë³µì¡í•œ AI ì—ì´ì „íŠ¸ê¹Œì§€ í™•ì¥ ê°€ëŠ¥\n",
    "- **ê´€ì°°ì„±**: LangSmithë¥¼ í†µí•œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…\n",
    "\n",
    "#### ğŸ“¦ í•µì‹¬ ì•„í‚¤í…ì²˜\n",
    "```markdown\n",
    "**LangChain ìƒíƒœê³„**\n",
    "â”œâ”€â”€ langchain-core     # ê¸°ë³¸ ì¶”ìƒí™” ë° ì¸í„°í˜ì´ìŠ¤\n",
    "â”œâ”€â”€ langchain         # ì²´ì¸, ì—ì´ì „íŠ¸, ê²€ìƒ‰ ì „ëµ\n",
    "â”œâ”€â”€ langchain-openai  # OpenAI í†µí•©\n",
    "â”œâ”€â”€ langchain-anthropic # Anthropic í†µí•©\n",
    "â”œâ”€â”€ LangGraph        # ë³µì¡í•œ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°\n",
    "â””â”€â”€ LangSmith        # ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://python.langchain.com/svg/langchain_stack_112024_dark.svg\" \n",
    "        alt=\"langchain_stack\" \n",
    "        width=\"600\" \n",
    "        style=\"border: 0;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "### 2.1 ì„¤ì¹˜\n",
    "\n",
    "```bash\n",
    "# pip ì„¤ì¹˜\n",
    "pip install langchain langchain-openai langchain-google-genai\n",
    "\n",
    "# uv ì„¤ì¹˜ \n",
    "uv add langchain langchain-openai langchain-google-genai\n",
    "\n",
    "# ì¶”ê°€ ë„êµ¬ pip ì„¤ì¹˜ (ì„ íƒì‚¬í•­)\n",
    "pip install langchain-ollama langsmith\n",
    "\n",
    "# ì¶”ê°€ ë„êµ¬ uv ì„¤ì¹˜ (ì„ íƒì‚¬í•­)\n",
    "uv add langchain-ollama langsmith\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 API í‚¤ ì„¤ì •\n",
    "```python\n",
    "# .env íŒŒì¼ ìƒì„±\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "GOOGLE_API_KEY=your_google_api_key_here\n",
    "\n",
    "# LangSmith ì„¤ì • (ì„ íƒì‚¬í•­)\n",
    "LANGSMITH_API_KEY=your_langsmith_api_key\n",
    "LANGSMITH_TRACING=true\n",
    "LANGSMITH_PROJECT=your_project_name\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith ì¶”ì  í™•ì¸\n",
    "import os\n",
    "print(f\"LangSmith ì¶”ì : {os.getenv('LANGSMITH_TRACING')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. í•µì‹¬ ì»´í¬ë„ŒíŠ¸\n",
    "\n",
    "### 3.1 Chat Models (ì±„íŒ… ëª¨ë¸)\n",
    "\n",
    "- OpenAI, Anthropic, Google ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì„ ì§€ì›\n",
    "- í…ìŠ¤íŠ¸ ìƒì„±, ëŒ€í™”, ìš”ì•½ ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\", \n",
    "    temperature=0.3,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "# ê°„ë‹¨í•œ ëŒ€í™”\n",
    "response = model.invoke(\"íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\")\n",
    "print(f\"ë‹µë³€: {response.content}\")\n",
    "print(f\"ë©”íƒ€ë°ì´í„°: {response.response_metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Messages (ë©”ì‹œì§€)\n",
    "\n",
    "- ë©”ì‹œì§€ëŠ” AIì™€ì˜ ëŒ€í™”ì—ì„œ ì—­í• ì„ êµ¬ë¶„í•˜ëŠ” ê¸°ë³¸ ë‹¨ìœ„ì…ë‹ˆë‹¤.\n",
    "- ë©”ì‹œì§€ëŠ” ì‚¬ìš©ì, AI, ì‹œìŠ¤í…œ ë“± ë‹¤ì–‘í•œ ì—­í• ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# ì‹œìŠ¤í…œ ë©”ì‹œì§€: AIì˜ ì—­í•  ì •ì˜\n",
    "system_msg = SystemMessage(content=\"ë‹¹ì‹ ì€ ì¹œì ˆí•œ í™”í•™ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "# ì‚¬ìš©ì ë©”ì‹œì§€\n",
    "human_msg = HumanMessage(content=\"íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” ëª‡ ë²ˆì¸ê°€ìš”?\")\n",
    "\n",
    "# ëŒ€í™” ì‹¤í–‰\n",
    "messages = [system_msg, human_msg]\n",
    "response = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Prompt Templates (í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿)\n",
    "\n",
    "- í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì—¬ ì¼ê´€ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- ë³€ìˆ˜ ì¹˜í™˜ì„ í†µí•´ ë™ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“¦ ê¸°ë³¸ í…œí”Œë¦¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# ì „ë¬¸ê°€ í…œí”Œë¦¿\n",
    "template = \"\"\"\n",
    "ë‹¹ì‹ ì€ {topic} ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. {topic}ì— ê´€í•œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "ì§ˆë¬¸: {question}\n",
    "ë‹µë³€: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# í…œí”Œë¦¿ ì…ë ¥ ë³€ìˆ˜ í™•ì¸\n",
    "print(f\"í•„ìˆ˜ ë³€ìˆ˜: {prompt.input_variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…œí”Œë¦¿ í™•ì¸\n",
    "print(f\"í…œí”Œë¦¿: {prompt.template}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…œí”Œë¦¿ ì‚¬ìš©\n",
    "formatted_prompt = prompt.format(\n",
    "    topic=\"í™”í•™\",\n",
    "    question=\"íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    ")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“¦ ì±„íŒ… í…œí”Œë¦¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ì±„íŒ…ìš© í…œí”Œë¦¿\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë‹¹ì‹ ì€ ì „ë¬¸ {subject} ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# í…œí”Œë¦¿ ì‚¬ìš©\n",
    "prompt = chat_template.invoke({\n",
    "    \"subject\": \"ì§„ë¡œ\",\n",
    "    \"question\": \"ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ê°€ ë˜ë ¤ë©´ ì–´ë–¤ ê³µë¶€ë¥¼ í•´ì•¼ í•˜ë‚˜ìš”?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…œí”Œë¦¿ í™•ì¸\n",
    "pprint(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€ ì†ì„± í™•ì¸\n",
    "pprint(prompt.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. LCEL (LangChain Expression Language)\n",
    "\n",
    "### 4.1 LCELì´ë€?\n",
    "**LCEL**ì€ `|` ì—°ì‚°ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì—°ê²°í•˜ëŠ” ì„ ì–¸ì  ì²´ì´ë‹ì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
    "\n",
    "#### ğŸ“¦ í•µì‹¬ íŠ¹ì§•\n",
    "- **ì¬ì‚¬ìš©ì„±**: ì •ì˜ëœ ì²´ì¸ì„ ë‹¤ë¥¸ ì²´ì¸ì˜ ì»´í¬ë„ŒíŠ¸ë¡œ í™œìš©\n",
    "- **ë‹¤ì–‘í•œ ì‹¤í–‰ ë°©ì‹**: `.invoke()`, `.batch()`, `.stream()`, `.astream()`\n",
    "- **ìë™ ìµœì í™”**: ë°°ì¹˜ ì²˜ë¦¬ ì‹œ íš¨ìœ¨ì ì¸ ì‘ì—… ìˆ˜í–‰\n",
    "- **ìŠ¤í‚¤ë§ˆ ì§€ì›**: ì…ë ¥/ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ìë™ ìƒì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ê¸°ë³¸ ì²´ì¸ êµ¬ì„±\n",
    "\n",
    "#### ğŸ“¦ Prompt + LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ì»´í¬ë„ŒíŠ¸ ì •ì˜\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"ë‹¹ì‹ ì€ {topic} ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. {topic}ì— ê´€í•œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\\n\"\n",
    "    \"ì§ˆë¬¸: {question}\\në‹µë³€: \"\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.3)\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„±\n",
    "chain = prompt | llm\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰\n",
    "response = chain.invoke({\n",
    "    \"topic\": \"í™”í•™\",\n",
    "    \"question\": \"íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "})\n",
    "\n",
    "print(f\"ë‹µë³€: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“¦ Prompt + LLM + Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ì¶œë ¥ íŒŒì„œ ì¶”ê°€\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# ì™„ì „í•œ ì²´ì¸ êµ¬ì„±\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰ (ë¬¸ìì—´ ë°˜í™˜)\n",
    "response = chain.invoke({\n",
    "    \"topic\": \"í™”í•™\",\n",
    "    \"question\": \"íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "})\n",
    "\n",
    "print(f\"ë‹µë³€: {response}\")  # ì´ì œ ë¬¸ìì—´ë¡œ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. LangSmith ëª¨ë‹ˆí„°ë§\n",
    "\n",
    "### 5.1 LangSmithë€?\n",
    "**LangSmith**ëŠ” LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ê´€ì°°ì„±(Observability)ì„ ì œê³µí•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "#### ğŸ“¦ ì£¼ìš” ê¸°ëŠ¥\n",
    "- **ì²´ì¸ ì‹¤í–‰ ë¡œê¹… ë° ì¶”ì **\n",
    "- **í”„ë¡¬í”„íŠ¸ ë””ë²„ê¹…**\n",
    "- **ì„±ëŠ¥ ì¸¡ì • ë° ë¶„ì„**\n",
    "- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**\n",
    "\n",
    "### 5.2 LangSmith ì„¤ì •\n",
    "\n",
    "#### ğŸ“¦ ê³„ì • ê°€ì… ë° ì„¤ì •\n",
    "```python\n",
    "# 1. LangSmith ê³„ì • ê°€ì…: https://www.langchain.com/langsmith\n",
    "# 2. .env íŒŒì¼ ì„¤ì •\n",
    "LANGSMITH_API_KEY=your_langsmith_api_key\n",
    "LANGSMITH_TRACING=true\n",
    "LANGSMITH_PROJECT=your_project_name\n",
    "\n",
    "# 3. í™˜ê²½ í™•ì¸\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(f\"LangSmith ì¶”ì  ìƒíƒœ: {os.getenv('LANGSMITH_TRACING')}\")\n",
    "print(f\"í”„ë¡œì íŠ¸ëª…: {os.getenv('LANGSMITH_PROJECT')}\")\n",
    "```\n",
    "\n",
    "### 5.3 ìë™ ì¶”ì \n",
    "\n",
    "LangSmithê°€ ì„¤ì •ë˜ë©´ ëª¨ë“  ì²´ì¸ ì‹¤í–‰ì´ ìë™ìœ¼ë¡œ ì¶”ì ë©ë‹ˆë‹¤:\n",
    "\n",
    "```python\n",
    "# ì²´ì¸ ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ LangSmithì— ë¡œê·¸ ì „ì†¡\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"topic\": \"ì¸ê³µì§€ëŠ¥\",\n",
    "    \"question\": \"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€?\"\n",
    "})\n",
    "\n",
    "# LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ì‹¤í–‰ ê³¼ì • í™•ì¸ ê°€ëŠ¥:\n",
    "# - ê° ë‹¨ê³„ë³„ ì‹¤í–‰ ì‹œê°„\n",
    "# - ì…ë ¥/ì¶œë ¥ ë°ì´í„°\n",
    "# - í† í° ì‚¬ìš©ëŸ‰\n",
    "# - ì—ëŸ¬ ì¶”ì \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Runnable ì¸í„°í˜ì´ìŠ¤\n",
    "\n",
    "### 6.1 Runnable ê°œë…\n",
    "ëª¨ë“  LangChain ì»´í¬ë„ŒíŠ¸ëŠ” **Runnable ì¸í„°í˜ì´ìŠ¤**ë¥¼ êµ¬í˜„í•˜ì—¬ ì¼ê´€ëœ ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\n",
    "\n",
    "#### ğŸ“¦ ì£¼ìš” Runnable ìœ í˜•\n",
    "- `RunnableSequence`: ìˆœì°¨ì  ì‹¤í–‰\n",
    "- `RunnableParallel`: ë³‘ë ¬ ì‹¤í–‰\n",
    "- `RunnablePassthrough`: ì…ë ¥ ì „ë‹¬\n",
    "- `RunnableLambda`: í•¨ìˆ˜ ë˜í•‘\n",
    "\n",
    "### 6.2 RunnableSequence (ìˆœì°¨ ì‹¤í–‰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²ˆì—­ ì „ìš© ì²´ì¸ (íŒŒì´í”„ ì—°ì‚°ì ì‚¬ìš©)\n",
    "translation_prompt = PromptTemplate.from_template(\n",
    "    \"'{text}'ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ë²ˆì—­ëœ ë¬¸ì¥ë§Œì„ ì¶œë ¥í•´ì£¼ì„¸ìš”.\"\n",
    ")\n",
    "\n",
    "translation_chain = translation_prompt | llm | StrOutputParser()\n",
    "\n",
    "# ë²ˆì—­ ì‹¤í–‰\n",
    "result = translation_chain.invoke({\"text\": \"ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "# ëª…ì‹œì  RunnableSequence ìƒì„±\n",
    "translation_chain = RunnableSequence(\n",
    "    first=translation_prompt, \n",
    "    middle=[llm],\n",
    "    last=StrOutputParser()\n",
    ")\n",
    "\n",
    "# íŒŒì´í”„ ì—°ì‚°ìì™€ ë™ì¼í•œ ê²°ê³¼\n",
    "# translation_chain = translation_prompt | llm | StrOutputParser()\n",
    "\n",
    "result = translation_chain.invoke({\"text\": \"ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 RunnableParallel (ë³‘ë ¬ ì‹¤í–‰)\n",
    "\n",
    "#### ğŸ“¦ ì§ˆë¬¸ ë¶„ì„ ì‹œìŠ¤í…œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "from operator import itemgetter\n",
    "\n",
    "# 1. ì£¼ì œ ë¶„ë¥˜ ì²´ì¸\n",
    "topic_template = \"\"\"\n",
    "ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:\n",
    "- í™”í•™(Chemistry)\n",
    "- ë¬¼ë¦¬(Physics)  \n",
    "- ìƒë¬¼(Biology)\n",
    "\n",
    "ìœ„ 3ê°€ì§€ ì¹´í…Œê³ ë¦¬ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\"\"\"\n",
    "\n",
    "topic_prompt = PromptTemplate.from_template(topic_template)\n",
    "topic_chain = topic_prompt | llm | StrOutputParser()\n",
    "\n",
    "# ì‹¤í–‰\n",
    "result = topic_chain.invoke({\n",
    "    \"question\": \"íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "})\n",
    "print(f\"ì£¼ì œ ë¶„ë¥˜: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ì–¸ì–´ ê°ì§€ ì²´ì¸\n",
    "language_template = \"\"\"\n",
    "ì…ë ¥ëœ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”:\n",
    "- í•œêµ­ì–´(Korean)\n",
    "- ì˜ì–´(English)\n",
    "- ê¸°íƒ€(Others)\n",
    "\n",
    "ìœ„ 3ê°€ì§€ ì¹´í…Œê³ ë¦¬ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "\n",
    "ì…ë ¥: {question}\n",
    "\"\"\"\n",
    "\n",
    "language_prompt = PromptTemplate.from_template(language_template)\n",
    "language_chain = language_prompt | llm | StrOutputParser()\n",
    "\n",
    "# ì‹¤í–‰\n",
    "result = language_chain.invoke({\n",
    "    \"question\": \"What is the atomic number of Carbon?\"\n",
    "})\n",
    "print(f\"ì–¸ì–´ ë¶„ë¥˜: {result}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ë‹µë³€ ìƒì„± ì²´ì¸\n",
    "answer_template = \"\"\"\n",
    "ë‹¹ì‹ ì€ {topic} ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \n",
    "{language}ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "ë‹µë³€: \"\"\"\n",
    "\n",
    "answer_prompt = PromptTemplate.from_template(answer_template)\n",
    "\n",
    "# 4. ë³‘ë ¬ ì²˜ë¦¬ ì²´ì¸ êµ¬ì„±\n",
    "analysis_chain = RunnableParallel({\n",
    "    \"topic\": topic_chain,\n",
    "    \"language\": language_chain,\n",
    "    \"question\": itemgetter(\"question\")\n",
    "})\n",
    "\n",
    "# 5. ì „ì²´ ì²´ì¸ ì—°ê²°\n",
    "complete_chain = analysis_chain | answer_prompt | llm | StrOutputParser()\n",
    "\n",
    "# ì‹¤í–‰\n",
    "result = complete_chain.invoke({\n",
    "    \"question\": \"íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "})\n",
    "print(f\"ìµœì¢… ë‹µë³€: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ (ì˜ˆì œ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "# ìš”ì•½ í”„ë¡¬í”„íŠ¸\n",
    "summarize_prompt = PromptTemplate.from_template(\n",
    "    \"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”: {text}\"\n",
    ")\n",
    "\n",
    "# ê°ì • ë¶„ì„ í”„ë¡¬í”„íŠ¸\n",
    "sentiment_prompt = PromptTemplate.from_template(\"\"\"\n",
    "ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "í…ìŠ¤íŠ¸: {summary}\n",
    "\n",
    "ê·œì¹™:\n",
    "1. ë°˜ë“œì‹œ 'ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½' ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”\n",
    "2. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë¶€ê°€ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”\n",
    "\n",
    "ë‹µë³€:\n",
    "\"\"\")\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„±\n",
    "summarize_chain = summarize_prompt | llm\n",
    "sentiment_chain = sentiment_prompt | llm | StrOutputParser()\n",
    "\n",
    "# ì „ì²´ íŒŒì´í”„ë¼ì¸\n",
    "analysis_pipeline = (\n",
    "    summarize_chain \n",
    "    | RunnableParallel(\n",
    "        summary=lambda x: x.content,\n",
    "        sentiment=lambda x: sentiment_chain.invoke({\"summary\": x.content}),\n",
    "    )\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸\n",
    "text = \"\"\"ì˜¤ëŠ˜ ì‹œí—˜ì„ ë´¤ëŠ”ë° ì •ë§ ì˜ ë³¸ ê²ƒ ê°™ì•„ìš”. \n",
    "ëª‡ ì£¼ ë™ì•ˆ ì—´ì‹¬íˆ ê³µë¶€í•œ ë³´ëŒì´ ìˆë„¤ìš”. \n",
    "ê²°ê³¼ê°€ ë‚˜ì˜¤ë©´ ì¢‹ì€ ì ìˆ˜ë¥¼ ë°›ì„ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ì„œ ê¸°ëŒ€ë©ë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "result = analysis_pipeline.invoke({\"text\": text})\n",
    "print(f\"ìš”ì•½: {result['summary']}\")\n",
    "print(f\"ê°ì • ë¶„ì„: {result['sentiment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. ì‹¤ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ğŸŒŸ ë¬¸ì œ 1: LCEL ê¸°ë³¸ ì²´ì¸ ë§Œë“¤ê¸°\n",
    "- ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì£¼ì œì— ëŒ€í•´ 3ì¤„ ìš”ì•½ ì œê³µ\n",
    "- ì˜¨ë„ëŠ” 0.5ë¡œ ì„¤ì •\n",
    "- ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸŒŸ ë¬¸ì œ 2: RunnableParallel í™œìš©\n",
    "- í•˜ë‚˜ì˜ ì…ë ¥ì— ëŒ€í•´ ë²ˆì—­ê³¼ ìš”ì•½ì„ ë™ì‹œì— ìˆ˜í–‰í•˜ëŠ” ì²´ì¸ì„ ë§Œë“œì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
    "from langchain_core.runnables import RunnableParallel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— ìœ ìš©í•œ ë§í¬\n",
    "\n",
    "- [LangChain ê³µì‹ ë¬¸ì„œ](https://python.langchain.com/docs/introduction/)\n",
    "- [LangSmith ê°€ì´ë“œ](https://docs.smith.langchain.com/)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kt-ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
